[{"title":"INBOX-学习备忘","date":"2020-12-10T10:07:40.000Z","path":"2020/12/10/INBOX-学习备忘/","text":"mybatis generator写一个web工程最佳实践 技术体系整理 知识回顾 大数据技术体系","tags":[{"name":"INBOX","slug":"INBOX","permalink":"https://northernw.github.io/tags/INBOX/"}]},{"title":"Netty学习笔记","date":"2020-11-18T11:13:20.000Z","path":"2020/11/18/Netty学习笔记/","text":"Java NIOhttps://www.javadoop.com/post/java-nio 三大组件 Buffer 本质是内存中的一块，将数据写入这块内存，从这块内存获取数据 核心是ByteBuffer，也最常使用 Channel 通道，数据来源或数据写入的目的地 与Buffer打交道，读操作时将Channel的数据填充到Buffer中，写操作将Buffer的数据写入Channel。 读操作：channel.read(buffer) 写操作：channel.write(buffer) Selector 多路复用，一个线程管理多个Channel 将Channel注册到Selector上（带上需要监听的事件类型s） 四种事件： SelectionKey.OP_READ 对应 00000001，通道中有数据可以进行读取 SelectionKey.OP_WRITE 对应 00000100，可以往通道中写入数据 SelectionKey.OP_CONNECT 对应 00001000，成功建立 TCP 连接 SelectionKey.OP_ACCEPT 对应 00010000，接受 TCP 连接 ServerSocketChannel不和Buffer交互，不处理实际数据，一旦接收到请求后，实例化SocketChannel，之后在这个连接通道上的数据传递它就不管了，继续监听端口，等待下一个连接。 12345ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();serverSocketChannel.socket().bind(new InetSocketAddress(8080));while(true)&#123; SocketChannel socketChannel = serverSocketChannel.accept();&#125; Java的非阻塞IO和异步IOhttps://www.javadoop.com/post/nio-and-aio 简单地说，ServerSocketChannel、Selector这种多路复用的，是非阻塞，non-blocking。 AsynchronousServerSocketChannel是异步的，有Future和回调函数两种方式。 NettyChannelFuture和PromisePipeline和Inbound、OutboundEventLoopGroup和EventLoopNetty线程池指NioEventLoopGroup，线程池中的单个线程是NioEventLoop。","tags":[{"name":"NOTE","slug":"NOTE","permalink":"https://northernw.github.io/tags/NOTE/"},{"name":"Netty","slug":"Netty","permalink":"https://northernw.github.io/tags/Netty/"},{"name":"学习笔记","slug":"学习笔记","permalink":"https://northernw.github.io/tags/学习笔记/"}]},{"title":"国际&国内行业分析统计","date":"2020-11-09T19:37:24.000Z","path":"2020/11/10/国际-国内行业分析统计/","text":"世界500强财富世界500强（英语：Fortune Global 500，簡稱：Global 500），是指美国《财富)》杂志从1995年起每年评选的全球最大500家公司的排行榜，以公司的营业额为排名，2018年的财富世界500强入围门槛为年营业额235亿美元。该榜单并无表示“强大”之意，其入围的唯一标准是全年营业额，因此上榜企业中也有不少亏损企业。 2020年榜单2020年财富世界500强前10公司 排名 公司 国家 产业 收入 (百万美元) 利润 (百万美元) 雇员数 1 沃尔玛 美国 零售业 $523,964 $14,881 2,200,000 2 中国石油化工集团 中国 石油 $407,009 $6,793.2 582,648 3 国家电网 中国 能源 $383,906 $7,970 907,677 4 中国石油天然气集团 中国 石油 $379,130 $4,443.2 1,344,410 5 荷兰皇家壳牌 荷兰 石油 $352,106 $15,842 83,000 6 沙特阿拉伯国家石油公司 沙特阿拉伯 石油 $329,784 $88,210.9 79,000 7 大众集团 德国 汽车 $282,760 $15,542 671,205 8 BP 英国 石油 $282,616 $4,026 72,500 9 亚马逊公司 美国 互联网服务和零售 $280,522 $11,588 798,000 10 丰田汽车 日本 汽车 $275,288 $19,096.2 359,542 2019互联网7家互联网相关公司，美国的亚马逊、Alphabet公司、Facebook公司，中国的京东集团（102）、阿里巴巴集团（132）、腾讯控股有限公司（197）和小米集团。 2019行业子榜 排名 行业名称 上榜公司总数 1 银行：商业储蓄 50 2 车辆与零部件 34 3 炼油 29 4 人寿与健康保险（股份） 26 5 采矿、原油生产 23 6 食品店和杂货店 19 7 贸易 19 8 金属产品 18 9 财产与意外保险（股份） 17 10 电信 16 11 公用设施 15 12 电子、电气设备 15 13 航天与防务 13 14 工程与建筑 13 15 制药 13 16 人寿与健康保险（互助） 11 17 多元化金融 10 18 能源 10 19 专业零售 10 20 计算机、办公设备 9 21 化学品 8 22 食品生产 7 23 工业机械 7 24 互联网服务和零售 7 25 邮件、包裹及货物包装运输 7 26 航空 6 27 批发：保健 6 28 建材、玻璃 5 29 房地产 5 30 饮料 4 31 食品：消费产品 4 32 保健：保险和管理医保 4 33 信息技术服务 4 34 管道运输 4 35 计算机软件 3 36 娱乐 3 37 综合商业 3 38 家居、个人用品 3 39 网络、通讯设备 3 40 铁路运输 3 41 批发：食品 3 42 船务 3 43 建筑和农业机械 3 44 服装 3 45 食品：饮食服务业 2 46 保健：药品和其他服务 2 47 财产与意外保险（互助） 2 48 半导体、电子元件 2 49 烟草 2 50 批发：电子、办公设备 2 51 保健：医疗设施 2 52 医疗器材和设备 2 53 多元化外包服务 2 54 纺织 2 55 油气设备与服务 1 56 科学、摄影和控制设备 1 市值2020年中国百亿市值互联网公司排名-20201106单位：亿美元 排名 公司 市值 周涨幅 前值 市场 业务领域 成立时间 1 阿里巴巴 8115.56 -1.56% 8243.8 美股 综合 1999年 2 腾讯 7595.68 3.98% 7304.92 港股 综合 1998年 3 美团点评 2478.64 13.40% 2185.79 港股 本地服务 2010年 4 京东 1447.37 13.46% 1275.7 美股 电子商务 2006年 5 拼多多 1371.64 27.28% 1077.62 美股 电子商务 2015年 6 贝壳 864.97 8.46% 797.51 美股 居住服务 2018年 7 小米集团 805.65 14.10% 706.12 港股 硬件 2010年 8 网易 636.02 6.05% 599.74 美股 综合 1999年 9 蔚来 561.55 36.13% 412.5 美股 智能驾驶 2014年 10 百度 491.15 8.23% 453.8 美股 综合 2000年 11 好未来 427.66 7.18% 399.02 美股 教育服务 2008年 12 阿里健康 378.21 7.66% 351.3 港股 医疗健康 2014年 13 东方财富 328.23 9.07% 300.93 A股 金融服务 2005年 14 腾讯音乐 256.16 2.62% 249.62 美股 娱乐 2012年 15 小鹏汽车 255.75 79.61% 142.39 美股 智能驾驶 2018年 16 金山办公 236.33 4.75% 225.62 A股 办公软件 2011年 17 用友网络 229.21 8.70% 210.87 A股 商业软件 1995年 18 分众传媒 225.07 9.87% 204.86 A股 营销传播 2003年 19 理想汽车 221.3 31.19% 168.69 美股 智能驾驶 2017年 20 芒果超媒 213.12 8.25% 196.88 A股 传媒 2005年 21 携程 190.49 11.68% 170.57 美股 酒旅服务 1999年 22 爱奇艺 189.28 4.45% 181.21 美股 娱乐 2009年 23 万国数据 182.1 32.21% 137.74 美股 数据处理 2006年 24 跟谁学 173.35 9.51% 158.29 美股 教育服务 2014年 25 哔哩哔哩 169.79 9.40% 155.2 美股 娱乐 2009年 26 三六零 157.52 -1.43% 159.81 A股 安全 2005年 27 平安好医生 153.29 3.61% 147.95 港股 医疗健康 2015年 28 唯品会 153.25 5.94% 144.66 美股 电子商务 2010年 29 苏宁易购 136.85 1.51% 134.81 A股 电商零售 1996年 30 科大讯飞 136.57 9.31% 124.94 A股 人工智能 1999年 31 汽车之家 122.78 8.05% 113.63 美股 汽车资讯 2005年 32 寒武纪 114.17 2.07% 111.85 A股 AI芯片 2016年 市值接近百亿公司（50-100亿美元）： 排名 公司 市值 周涨幅 前值 市场 业务领域 成立时间 33 奇安信 111.09 21.88% 91.15 A股 网络安全 2014年 34 金蝶国际 106.38 16.97% 90.95 港股 商业软件 1999年 35 微博 100.03 6.31% 94.09 美股 社交媒体 2009年 36 三七互娱 90.81 -2.80% 93.43 A股 网络游戏 1995年 37 完美世界 84.45 3.80% 81.36 A股 网络游戏 1999年 38 金山云 84.26 33.05% 63.33 美股 云计算 2012年 39 达达 81.51 9.94% 74.14 美股 即时零售 2014年 40 欢聚时代 81.47 10.33% 73.84 美股 网络直播 2005年 41 金山软件 80.28 9.54% 73.29 港股 商业软件 1988年 42 阅文集团 78.6 -5.51% 83.18 港股 网络文学 2015年 43 九号公司 73.04 45.85% 50.08 A股 智能出行 2014年 44 巨人网络 55.1 -4.04% 57.42 A股 网络游戏 2004年 2020胡润中国10强金融科技企业 公司 公司英文名称 价值（亿元人民币） 主要领域 董事长 总部 1 蚂蚁集团 Ant Group 21,000 支付、理财、小额贷款 井贤栋 杭州 2 陆金所 Lufax 2,700 理财 计葵生 上海 3 东方财富 East Money 2,215 互联网金融信息服务 其实 上海 4 京东数科 JD Digits 2,000 支付、理财、小额贷款 刘强东 北京 4 微众银行 WeBank 2,000 小额贷款、中小企业融资 顾敏 深圳 6 恒生电子 Hundsun 1,010 金融科技解决方案 彭政纲 杭州 7 同花顺 Hithink RoyalFlush 860 互联网金融信息服务 易峥 杭州 8 苏宁金服 Suning Finance 560 支付、理财、小额贷款 黄金老 南京 9 众安在线 ZhongAn 500 保险 欧亚平 上海 10 金融壹账通 One Connect 490 金融科技解决方案 叶望春 深圳 来源：胡润研究院 全球市值Top102020.09.06 公司 市值（亿美元） 行业 产品 1 沙特阿美（Saudi Aramco） 1.685万 石油与天然气开采与加工 石油、天然气及其他石化产品 2 Microsoft 1.359万 软件开发 Microsoft Office、Microsoft Windows、Xbox 3 Apple Inc. 1.286万 电子、信息技术 个人电脑和平板电脑、手机、音频播放器等 4 Amazon Inc. 1.233万 零售业 5 Alphabet Inc. 9190 互联网 6 Facebook 5840 互联网 7 Alibaba Group 5450 互联网 电子商务、在线拍卖托管、网上转账、移动商务 8 Tencent 5100 互联网 社交网络、即时通讯、大众传媒、门户网站等 9 Berkshire Hathaway Inc. 4550 保险、金融、铁路运输、公用事业、食品和非食品领域 10 Johnson&amp;Johnson 3950 医药 药品和医疗器械","tags":[{"name":"INBOX","slug":"INBOX","permalink":"https://northernw.github.io/tags/INBOX/"}]},{"title":"Java-Lambda的实现原理","date":"2020-11-09T18:27:57.000Z","path":"2020/11/10/Java-Lambda的实现原理/","text":"函数接口（Functional Interface） 并不能在代码的任何地方任意写Lambda表达式。 Lambda的类型，是对应函数接口的类型。 省略了接口名、方法名、参数类型（基于类型推断）。 将功能视为方法参数。 匿名内部类与Lambda实现对比匿名内部类会产生额外的class文件 Lambda被封装成主类的一个私有方法，并通过invokedynamic指令进行调用。 自定义函数接口编写一个只有一个抽象方法的接口。 @FunctionalInterface是可选的，加上注解编译器会帮忙检查是否符合函数接口的规范。（类似@Override会帮助检查是否重载） 函数式数据处理使用流 筛选和切片 filter 谓词筛选 distinct 筛选各异的元素，去重 limit 截断流，返回前n个元素 skip 跳过元素，跳过前n个元素 映射 map flatMap 扁平化流 查找和匹配 anyMatch 至少一个匹配 allMatch 完全匹配 noneMatch 都不匹配 findAny 返回任意元素 findFirst 返回第一个元素 规约 reduce (折叠，fold) 最大值 .reduce(Integer::max) 最小值 .reduce(Integer::min) 原始类型流特化 IntStream、DoubleStream、LongStream，分别将流中的元素转化为int、long、double，避免暗含的装箱成本 mapToInt.. 将Stream转换为数值流 boxed() 装箱，将数值流转为Stream 12IntStream intstream = menu.stream().mapToInt(Dish::getCalories);// 将Stream转换为数值流Stream&lt;Integer&gt; stream = intStream.boxed(); // 将数值流转为Stream 常用数值规约 sum max min OptionalInt 数值范围 rangeClosed 闭区间，起始值，结束值 range 开区间，起始值，结束值（开区间） 从多个源创建流 集合、值、数组、文件，以及5里的迭代和生成 无限流，没有固定的大小 迭代 1Stream.iterate(0,n-&gt;n+2).limit(10).forEach(System.out::println); 生成 1Stream.generate(Math::random).limit(5).forEach(System.out::println); 收集数据 counting 统计个数 maxBy minBy 最大值、最小值 汇总 summingInt/summingLong/summingDouble 平均值 averagingInt/… 连接字符串 joining 广义规约 reducing，有三个参数 起始值 提供数值的函数 BinaryOperator 进行规约的二元操作 groupingBy 分组 按子组收集数据 Map&lt;Dish.Type, Long&gt; typesCount = ...(groupingBy(Dish::getTyype, counting())) Optional isPresent() Optional存在值时返回true，否则false ifPresent(Consumer&lt;T&gt; block) 会在值存在时执行给定代码块 T get() 会在值存在时返回值，否则抛出一个NoSuchElement异常 T orElse(T other) 会在值存在时返回值，否则返回一个默认值","tags":[{"name":"INBOX","slug":"INBOX","permalink":"https://northernw.github.io/tags/INBOX/"},{"name":"Java","slug":"Java","permalink":"https://northernw.github.io/tags/Java/"},{"name":"Lambda","slug":"Lambda","permalink":"https://northernw.github.io/tags/Lambda/"}]},{"title":"Java动态代理","date":"2020-11-09T16:15:26.000Z","path":"2020/11/10/Java动态代理/","text":"小结 实现一个调用处理器InvocationHandler h，并以某种方式持有被代理实例，在invoke方法中，触发被代理实例的方法 Proxy.newProxyInstance(..)需要传入接口的类加载器、接口类以及调用处理器，生成一个代理实例，所以呢 代理实例持有h，h持有被代理实例 调用代理实例的x方法，会触发h的invoke方法，再触发被代理实例的x方法 Spring中对动态代理的应用JdkDynamicAopProxy.java 它是一个InvocationHandler实例 持有一个AdvisedSupport，AdvisedSupport中有被代理实例、被代理实例的接口们、Advisors增强or切面 invoke方法中，会判断当前方法是否有命中切面，执行有切面（责任链模式）or无切面的逻辑，触发被代理实例的方法 Java动态代理的使用接口类 123public interface UserService &#123; String getName();&#125; 实现类 1234567public class UserServiceImpl implements UserService &#123; @Override public String getName() &#123; System.out.println(\"in method: getName\"); return \"KK\"; &#125;&#125; InvocationHadler 123456789101112131415public class LogInvocationHandler&lt;T&gt; implements InvocationHandler &#123; T target; public LogInvocationHandler(T target) &#123; this.target = target; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println(\"before proxy...\"); Object result = method.invoke(target, args); System.out.println(\"after proxy...\"); return result; &#125;&#125; 客户端 12345678public class ProxyMain &#123; public static void main(String[] args) &#123; UserService userService = new UserServiceImpl(); LogInvocationHandler handler = new LogInvocationHandler&lt;&gt;(userService); UserService userServiceProxy = (UserService) Proxy.newProxyInstance(UserService.class.getClassLoader(), new Class&lt;?&gt;[]&#123;UserService.class&#125;, handler); System.out.println(userServiceProxy.getName()); &#125;&#125; 原理说明关键点在于Proxy.newProxyInstance(UserService.class.getClassLoader(), new Class&lt;?&gt;[]{UserService.class}, handler);，生成了动态代理类。 Proxy的源码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) throws IllegalArgumentException &#123; Objects.requireNonNull(h); final Class&lt;?&gt;[] intfs = interfaces.clone(); final SecurityManager sm = System.getSecurityManager(); if (sm != null) &#123; checkProxyAccess(Reflection.getCallerClass(), loader, intfs); &#125; /* * Look up or generate the designated proxy class. */ // 这里生成了代理类的字节码，并加载进虚拟机 Class&lt;?&gt; cl = getProxyClass0(loader, intfs); /* * Invoke its constructor with the designated invocation handler. */ try &#123; if (sm != null) &#123; checkNewProxyPermission(Reflection.getCallerClass(), cl); &#125; // 代理类的构造器 final Constructor&lt;?&gt; cons = cl.getConstructor(constructorParams); final InvocationHandler ih = h; if (!Modifier.isPublic(cl.getModifiers())) &#123; AccessController.doPrivileged(new PrivilegedAction&lt;Void&gt;() &#123; public Void run() &#123; cons.setAccessible(true); return null; &#125; &#125;); &#125; // 创建代理类的实例 return cons.newInstance(new Object[]&#123;h&#125;); &#125; catch (IllegalAccessException|InstantiationException e) &#123; throw new InternalError(e.toString(), e); &#125; catch (InvocationTargetException e) &#123; Throwable t = e.getCause(); if (t instanceof RuntimeException) &#123; throw (RuntimeException) t; &#125; else &#123; throw new InternalError(t.toString(), t); &#125; &#125; catch (NoSuchMethodException e) &#123; throw new InternalError(e.toString(), e); &#125; &#125; 如何生成代理类的字节码呢？ 再看下生成的代理类字节码什么样子。 123456789byte[] classFile = ProxyGenerator.generateProxyClass(\"$Proxy0\", new Class&lt;?&gt;[]&#123;UserService.class&#125;);String filename = \"UserServiceProxy.class\";try (FileOutputStream fos = new FileOutputStream(filename)) &#123; fos.write(classFile); fos.flush(); System.out.println(\"done\");&#125; catch (Exception e) &#123; e.printStackTrace();&#125; UserServiceProxy.class反编译后： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public final class $Proxy0 extends Proxy implements UserService &#123; //第二步, 生成静态域 private static Method m1; private static Method m3; private static Method m2; private static Method m0; //第一步, 生成构造器 public $Proxy0(InvocationHandler var1) throws &#123; super(var1); &#125; //第三步, 生成代理方法 public final boolean equals(Object var1) throws &#123; try &#123; return (Boolean)super.h.invoke(this, m1, new Object[]&#123;var1&#125;); &#125; catch (RuntimeException | Error var3) &#123; throw var3; &#125; catch (Throwable var4) &#123; throw new UndeclaredThrowableException(var4); &#125; &#125; public final String getName() throws &#123; try &#123; return (String)super.h.invoke(this, m3, (Object[])null); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; public final String toString() throws &#123; try &#123; return (String)super.h.invoke(this, m2, (Object[])null); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; public final int hashCode() throws &#123; try &#123; return (Integer)super.h.invoke(this, m0, (Object[])null); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; //第四步, 生成静态初始化方法 static &#123; try &#123; m1 = Class.forName(\"java.lang.Object\").getMethod(\"equals\", Class.forName(\"java.lang.Object\")); m3 = Class.forName(\"wyq.learning.quickstart.proxy.UserService\").getMethod(\"getName\"); m2 = Class.forName(\"java.lang.Object\").getMethod(\"toString\"); m0 = Class.forName(\"java.lang.Object\").getMethod(\"hashCode\"); &#125; catch (NoSuchMethodException var2) &#123; throw new NoSuchMethodError(var2.getMessage()); &#125; catch (ClassNotFoundException var3) &#123; throw new NoClassDefFoundError(var3.getMessage()); &#125; &#125;&#125;","tags":[{"name":"Java","slug":"Java","permalink":"https://northernw.github.io/tags/Java/"},{"name":"KNOWLEDGE","slug":"KNOWLEDGE","permalink":"https://northernw.github.io/tags/KNOWLEDGE/"},{"name":"动态代理","slug":"动态代理","permalink":"https://northernw.github.io/tags/动态代理/"}]},{"title":"关于中台","date":"2020-11-06T11:08:42.000Z","path":"2020/11/06/关于中台/","text":"【pass 是数据】数据中台：让数据用起来 付登坡等 机械工业出版社 2020.01 经济理财榜-经济管理 7位作者，【数澜科技】，主要来自阿里数据中台团队。 从建设、管理、运营、安全等角度讲解了数据中台。 【pass】中台产品经理宝典：从业务建模到中台设计全攻略 刘天 电子工业出版社 2020.06 科学科技榜-计算机 用最低的成本完成最多的产出 中台的概念与功能 如何设计中台产品 产品理论，没有技术 【感觉是1.0的中台思想】企业IT架构转型之道：阿里巴巴中台战略思想与架构实战 钟华 机械工业出版社 2017.04 科学科技榜-计算机 共享服务理念 企业级互联网架构建设 技术框架的选择，哪些技术平台支撑了共享服务体系 【pass数据】数据中台实战：手把手教你搭建数据中台 董超华 电子工业出版社 2020.09 科学科技榜-计算机 偏实战，数据的采集、存储、打通和应用 【pass不是我要的B-PaaS】算法与数据中台：基于Google、Facebook与微博实践 詹盈 电子工业出版社 2020.08 科学科技榜-计算机 看目录，各种基础、算法、数据、业务的中台的…实践or实战？ 【pass不值得看】中台战略：中台建设与数字商业 陈新宇等 机械工业出版社 2019.08 个人成长榜-职场 理论性，好像很全局的样子，期待一下 【pass不值得看】中台实践：数字化转型方法论与解决方案 陈新宇 机械工业出版社 2020.09 个人成长榜-职场 咋还写两本呢… 【pass不值得看】业务中台产品搭建指南：电商业务平台全流程设计与实战 高晖 人民邮电出版社 2020.05 经济理财榜-理财 em….感觉中台信息不多，更多是电商业务 【pass不值得看】数据中台架构：企业数据化最佳实践 张旭等 电子工业出版社 2020.04 个人成长榜-职场 看着不太行 【提供了业务中台、数据中台和技术中台的架构图】中台架构与实现：基于DDD和微服务 欧创新 邓頔 机械工业出版社 2020.10 23w 科学科技榜-计算机 主讲DDD和微服务 提供了业务中台、数据中台和技术中台的架构图，这点比较靠谱，其他没看 【pass不值得看】企业中台，成就智慧品牌 百胜智库 中国经济出版社 2018.12 11w 经济理财榜-经济学著作 好像是从运营角度讲的，并不是技术书 #####企业IT架构转型之道：阿里巴巴中台战略思想与架构实战 钟华 机械工业出版社 2017.04 科学科技榜-计算机 共享服务理念 企业级互联网架构建设 技术框架的选择，哪些技术平台支撑了共享服务体系 名词解释 去IOE：阿里提出的概念。去掉IBM的小型机、Oracle数据库、EMC存储设备，代之以开源为基础的自研系统。 业务中台架构 数据中台架构 技术中台架构","tags":[{"name":"INBOX","slug":"INBOX","permalink":"https://northernw.github.io/tags/INBOX/"}]},{"title":"国内外大厂发展史","date":"2020-11-04T17:57:03.000Z","path":"2020/11/05/国内外大厂发展史/","text":"https://weread.qq.com/web/reader/a003296071e04a38a0087c4kc7432af0210c74d97b01b1c 这里提到头条的发展，十分有趣 生生开辟了两大战场，占足先机","tags":[{"name":"INBOX","slug":"INBOX","permalink":"https://northernw.github.io/tags/INBOX/"}]},{"title":"如何构建笔记系统","date":"2020-11-03T16:48:13.000Z","path":"2020/11/04/如何构建笔记系统/","text":"收藏自：如何构建自己的笔记系统？ - Lachel的回答 - 知乎 https://www.zhihu.com/question/23427617/answer/28206585 Iphone用于记录路上随时产生的想法、灵感，以及拍照；ipad用于阅读，做读书笔记；PC用于对笔记进行整理。 笔记体系： Inbox（收集） Note（储存） Knowledge（主题） 详述： Inbox：收集零碎咨询，用关键词记下——关键词笔记 Note：存放inbox中经过整理的笔记，有完整的时间、标题即脉络——参考笔记 Knowledge：将Note储存的笔记主题化——主题笔记 具体流程： 收集：任意时刻看到觉得有价值的咨询，记下关键字，文字、画图、拍照等，放入inbox 完善：定期进行，一般1-3天一次。将inbox的关键字扩充成详细笔记，放入Note。示例：比如关键字长尾效应，可扩充的：长尾效应是什么意思？谁提出的？得到了怎样的印证？业界对其态度如何？存在怎样的问题？等等。 整理：在Knowledge进行思维整理，哪些Note是相互关联的知识 应用：即做输出，写文章or告诉别人等 归档：对于完全物尽其用的笔记，或已经记得滚瓜烂熟的笔记，删掉，为今后的输入腾出空间","tags":[{"name":"KNOWLEDGE","slug":"KNOWLEDGE","permalink":"https://northernw.github.io/tags/KNOWLEDGE/"}]},{"title":"如何阅读一本书","date":"2020-11-03T16:47:39.000Z","path":"2020/11/04/如何阅读一本书/","text":"《如何阅读一本书》 莫提默·艾德勒 查尔斯·范多伦 出版社：商务印书馆有限公司 2014.10 24w 教育榜-其他 ——【学以致用：本书在谈如何阅读一本书；四个阅读层次，比较重要的是检视阅读和分析阅读，也是主题阅读的基础；主题阅读是阅读的最终目标。】 阅读的层次： 基础阅读：简单说来，基本的读写能力，初中毕业 检视阅读 分析阅读 主题阅读 主题阅读：同一个主题的多本书 分析阅读： 阶段一，找出本书在谈些什么 书本分类：种类与主题 用最简短的句子说出本书在谈些什么 列出书本重要的部分 找出作者的问题or作者想要解决的问题（作者的意图） 阶段二，诠释一本书的内容——这一部分比较抽象.. 诠释作者的关键字，与作者达成共识 找最重要的句子，抓出重要主旨 找论述，明白作者的主张 确定作者已解决哪些问题，哪些未解决——不懂… 检视阅读：（一个小时左右） 略读或粗读 书名，序，副标题（对书的主题有概念） 目录（概括性地理解书的基本架构） 索引——国内书本一般没什么用 出版者的介绍——说不定有书的主旨，也是为了对书有基本概念 挑几个和主题息息相关的篇章来看，或者读章节开头或结尾的摘要 翻一翻书，念个一两段，有时候连续读几页，但不要太多，把全书翻一遍，留意主题的基本脉动 粗浅的阅读 碰到难懂的不要停下来查询或思索，先从头到尾读完一遍 阅读速度快一点，不只是要能读得快，还要能用不同的速度读不同的读物，不论多么难读的书： 无关紧要的读快 难的读慢 要回答的问题，做笔记把答案写下来（结构笔记） 这是一本什么样的书？ 整本书在谈的是什么？ 作者借着怎样的整体架构来发展他的观点或陈述他对这个主题的理解 基础阅读的四个阶段： 准备阶段，从出生到六七岁 读简单的读物，认字 快速建立词汇的能力 精练与增进前面所学的技巧","tags":[{"name":"KNOWLEDGE","slug":"KNOWLEDGE","permalink":"https://northernw.github.io/tags/KNOWLEDGE/"}]},{"title":"广告行业入门","date":"2020-11-03T11:20:55.000Z","path":"2020/11/03/广告行业入门/","text":"Questions: 交易模式——RTB …. 等看差不多了再整理 书籍【按出版时间降序】 信息流广告实战 明学海 清华大学出版社 2020年3月 信息流广告的生态现状、理论基础、代表性平台 在线广告：互联网广告系统的架构及算法 张亚东 清华大学出版社 2019.08 14万 抖音电商运营：广告+引流+卖货+IP变现 刘东明 中国铁道出版社 2019.07 程序化广告：个性化精准投放实用手册 梁丽丽 人民邮电出版社 2017.10 12万 程序化广告实战 吴俊 机械工业出版社 2017.08 广告人手记——真·广告 叶茂中 北京联合出版公司 2016.03 计算广告：互联网商业变现的市场与技术 刘鹏 王超 人民邮电出版社 2015年9月 20万 信息流广告实战明学海 清华大学出版社 2020年3月 信息流广告的生态现状、理论基础、代表性平台 中国信息流广告市场生态图谱 RTBChina 中国程序化广告科技资讯网 rtbchina.com 中国程序化广告技术生态圈 (Last Update: June, 2020/ 最近更新于2020年6月) 名词解释 DAU：Daily Active User，日活跃用户数量。反映网站、应用或游戏的运营情况。通常统计一日之内，登录或使用了某个产品的用户数。 流量： OTV：Online TV，在线视频 IAB：Interactive Advertising Bureau，（美国）互动广告局，程序化购买相关技术规范的制定者 IDFA：Identifier For Advertising，广告标识符，iOS设备描述用户信息的设备ID。 IMEI：International Mobile Equipment Identity，国际移动设备识别码，安卓设备上描述用户信息的设备ID。 KOL：Key Opinion Leader，关键意见领袖。 KA：Key Account，重点客户。在数字营销行业一般指年度预算比较大的广告主。KA对应SMB客户 SMB：Small and Medium-sized Business，中小企业客户 LA：Leading Agency，首选代理公司 LBS：Location Based Services，位置服务，在数字营销中一般指区域或位置的定向 MD5：Message-Digest Algorithm，MD5消息摘要算法 PDB：Programmatic Direct Buying，程序化直接购买 RTB：Real Time Bidding，实时竞价 TA：Target Audience，目标受众，指广告希望覆盖的消费者群体 计费方式 CPT： GD： CPC： CPM： CPV：Cost Per View，单次播放成本。注重视频创意的播放效果，播放超过10秒即为有效播放。 VTR衡量标准：View Through Rate，播完率，VTR = 视频播放量/广告展现量*100% 信息流广告的定义信息流（Feeds）广告，是在移动设备上穿插在内容流中的广告。 因其原生性和用户沟通的友好性，可以很好平衡广告主、媒体和用户的利益。 对于广告主：用更少的预算覆盖目标用户 用户：减少硬广的打扰 媒体：穿插在内容中释放大量的广告库存 更适合精准营销。大流量入口：如开屏视频，采用大曝光覆盖模式；垂直化流量入口：如头条的“推荐”页、QQ空间的“沉浸式”视频，基于大数据采用TA人群覆盖的策略。 营销效果：品牌广告和效果广告。 ps原生广告的六大属性：媒介适配性、内容创意适用性、用户体验打扰度低、用户选择自由、内容价值、数据管理能力 信息流广告发展2006年，鼻祖Facebook。到2014年，Facebook 50%的广告收入来自信息流广告，同年Twitter信息流广告收入占比超过70%。 2012年，国内今日头条推出内容推荐功能，开辟信息流类型内容智能分发的领域。随后腾讯、百度、阿里加入战场。 2014年，今日头条推出信息流广告形式，其他媒体追随。 2014年，Facebook和Twitter引入视频信息流（自2016年多家国内媒体推出后开始蓬勃发展）。 2015年，微信朋友圈广告上线。 2016年，视频信息流成为新蓝海。 2016年Q4，百度上线信息流广告，起步较晚。 2017年，短视频应用四足鼎立。短视频应用：抖音、快手；社交媒体：微信、微博、QQ；咨询媒体：今日头条、腾讯新闻、网易新闻；OTV：爱奇艺、优酷、腾讯视频。 2018年，抖音火爆全国。 2018年Q4，大数据和机器学习深入信息流营销流程，即用户洞察、决策支撑、创意内容、智能投放和效果分析。 据艾瑞数据，2019年国内信息流广告市场规模将达到1800亿元，到2020年，环比增长率依然高达45%以上。（2017年的预测）（2018年互联网市场规模3509亿元，同比增长16.6%） 信息流广告的运作模式营销诉求：促成用户转化路径的发生。 转化路径：知道Awareness &gt;&gt; 了解Research &gt;&gt; 考虑Consider &gt;&gt; 购买Purchase。 营销转化漏斗：转化路径的另一种量化表述方式，展示 &gt;&gt; 点击 &gt;&gt; 到站/下载-激活 &gt;&gt; 咨询/注册 &gt;&gt; ROI 信息流生态媒体流量方 广告需求方 代理公司 第三方数据公司 第三方广告监测公司 第三方创意公司 在线广告：互联网广告系统的架构及算法张亚东 清华大学出版社 2019.08 14万 广告投放引擎架构 面向广告主的客户系统。创建广告投放计划。【营销目标：品牌认知、购买意向和行动转化】 供运营与研发使用的相关工具平台，包括用户管理、订单管理、权限控制、财务管理、效果监控、订单审核和黑白名单模块。 基础架构及相关模块。【需要随时保持较高的系统稳定性和快速响应能力，响应时间通常是几毫秒到几百毫秒。（如果限定时间内广告系统没有返回广告，那么这次请求将不展现任何广告，系统也不会有收益）】包括在线投放系统（即一个请求到达时，快速处理完所有逻辑，返回合适的广告结果）和大数据处理模块（日志存储、广告数据的实时或离线分析、计费等） 内部模块，广告位模块、排序模块… 工具和测试平台","tags":[{"name":"INBOX","slug":"INBOX","permalink":"https://northernw.github.io/tags/INBOX/"},{"name":"广告行业","slug":"广告行业","permalink":"https://northernw.github.io/tags/广告行业/"}]},{"title":"技术栈Notes","date":"2020-10-26T14:36:11.000Z","path":"2020/10/26/技术栈Notes/","text":"第一阶段一、MyBatis1. Mybatis框架分析（1）mybatis框架架构图 （2）整体执行流程图 （3）sqlSession执行流程图 2. MyBatis源码分析（1）Config文件加载流程 （2）mapper文件加载流程 （3）SQLSource创建流程 （4）获取BoundSql流程 （5）参数映射流程 （6）结果集映射流程 3. 设计模式构造者模式 简单工厂模式 工厂方法模式 抽象工厂模式 单例模式 4. 手写Mybatis实现配置文件加载流程 实现封装jdbc的执行流程 二、Spring1. 核心模块介绍（1）核心容器模块 core模块 beans模块 context模块 expression模块 （2）AOP和设备模块 （3）数据访问及集成模块 （4）web模块 （5）报文消息模块 （6）test测试模块 2. 核心接口讲解（1）BeanFactory接口体系 （2）BeanDefinition接口体系 （3）ApplicationContext接口体系 3. 源码分析（1）IOC源码解析 IOC初始化流程 BeanDefinition加载注册流程 DI依赖注入流程 （2）aop流程 aop标签解析流程 AspectJAwareAdvisorAutoProxyCreator类的作用 AspectJExpressionPointcut类的作用 AspectJPointcutAdvisor类的作用 动态代理对象创建流程 AspectJAwareAdvisorAutoProxyCreator类的实现 AspectJExpressionPointcut类的实现 （3）tx流程 spring事务的实现原理 PlatformTransactionManager的接口体系 TransactionStatus接口 4. 设计模式（1）创建型 简单工厂模式 工厂方法模式 抽象工厂模式 单例模式 原型模式 （2）结构型 代理模式 jdk动态代理模式 cglib动态代理模式 装饰模式 组合模式 （3）行为型 5. 手写框架（1）IOC模块 实现IOC初始化流程 实现BeanDefinition的加载注册 实现DI依赖注入 （2）AOP模块 6. 常见分析 BeanFactory和FactoryBean的区别 BeanFactoryPostProcessor和BeanPostProcessor的区别 循环依赖和循环构造问题如何解决 三、SpringMVC1. 架构分析（1）11步执行流程图 （2）六大组件介绍 DispatcherServlet HandlerMapping HandlerAdapter Handler ViewResolver View 2. 源码分析（1）DispatcherServlet初始化流程 HandlerMapping初始化流程 HandlerAdapter初始化流程 （2）DispatcherServlet执行流程 （3）HandlerMapping执行流程 解析@Controller和@RequestMapping流程 （4）HandlerAdapter执行流程 参数设置流程 结果映射流程 3. 手写springmvc源码分析里的几个组件和流程的实现 四、MySQL1. SQL语法顺序和解析顺序的理解2. MySQL架构分析和执行流程分析（1）逻辑架构图连接器 服务管理 连接池 SQL接口 解析器 查询优化器 查询缓存 可插拔存储引擎：MyISAM、InnoDB、Memory （2）执行流程图 简单执行流程 详细执行流程 （3）物理存储结构 日志文件 错误日志 二进制日志 通用查询日志 重做redo日志 回滚undo日志 中继日志 数据文件 InnoDB数据文件 MyISAM数据文件 3. 索引篇（1）索引基础 索引介绍 什么是索引 索引优势 索引劣势 索引分类 单列索引 组合索引 全文索引 空间索引 索引使用 索引创建 索引删除 索引查看 （2）索引原理 索引存储结构 B树与B+树 非聚集索引（MyISAMEMStore索引） 聚集索引（InnoDB） 多用组合索引 组合索引的优势 最左前缀原则 索引使用场景 使用索引情况 不使用索引情况 索引失效分析 查看执行计划 explain查看执行计划 参数说明 select_type simple primary subquery union type const eq_ref ref range index all extra using index using where using index condition ICP的理解 index filter的理解 using filesort 多个索引使用案例分析 4. 锁和事务篇（1）锁的介绍 表级锁 行级锁 行锁 MDL元数据锁 （2）行锁原理分析 简单SQL的加锁分析 RC级别下的主键索引、唯一索引、非唯一索引、无索引分析 RR级别下的主键索引、唯一索引、无唯一索引、无索引分析 复杂SQL的加锁分析 where条件如何拆分 index key index filter table filter 死锁原理分析 两个session的两条SQL产生死锁分析 两个session的一条SQL产生死锁分析 （3）事务流程分析 回滚流程undo 重做流程redo （4）InnoDB架构分析 架构图分析 内存结构分析 Buffer Pool data page和index page insert buffer adaptive hash index lock info data dictionary Redo log buffer double write 磁盘文件分析 系统表空间和用户表空间文件 重做日志文件和归纳文件（？） 重做日志的落盘机制 （5）InnoDB一致性非锁定读 一致性非锁定读的机制 MVCC（多版本并发控制）原理 InnoDB的MVCC实现 （6）InnoDB事务分析 原子性、一致性、持久性原理分析 隔离性原理分析 事务并发问题理解 当前读和快照读 一致性非锁定读理解 InnoDB的MVCC实现 5. 性能分析篇（1）性能分析思路 （2）慢查询日志分析 何时开启慢查询日志 设置慢查询日志超时时间 分析慢查询日志的工具 （3）查询计划分析 （4）profile性能分析 6. 性能优化篇（1）服务器层面优化 innodb_buffer_pool_size设置 内存预热 innodb_log_file_size设置 选择SSD磁盘提高读写能力 （2）SQL设计层面优化 中间表的设计 冗余字段的设计 拆表字段 拆表数据（分库分表） （3）SQL语句优化 limit优化 索引优化 如何创建索引并正确使用组合索引 order by group by与索引设计的关联 其他优化项 7. 主从复制和读写分离集群（1）主从复制集群 主从复制原理 binlog和relay日志 主从复制实践 （2）读写分离集群 原理分析 读写分离实践 8. 分库分表篇（1）分库分表策略 数据切分方案 数据切分规则 收切分原则 分库分表要解决的问题 分布式事务问题 分布式主键ID问题 跨库join问题 跨库count、order by、group by问题 （2）MyCat集群 架构介绍 核心概念介绍 十种常见分片规则 MyCat集群搭建与分库分表应用 MyCat读写分离方式设置 用户购物下单实践 五、Redis1. Redis五种数据类型及使用场景分析2. Redis事务（1）Redis事务分析 （2）事务失败的处理 3. 持久化原理及性能分析（1）rdb方式 快照触发时机 设置快照规则 快照实现原理 优缺点分析 （2）aof方式 同步磁盘数据分析 aof重写原理分析 文件损坏如何恢复 （3）如何选择rdb和aof 4. 主从复制原理分析 主从配置的实现原理 全量同步 增量同步 5. 哨兵机制哨兵进程的作用分析 故障判断原理分析 ODOWN SDOWM 法定人数理解 6. cluster集群Redis集群策略 架构分析 容错机制 redis cluster集群搭建 数据迁移 7. 与lua整合如何编写包含redis api的lua脚本 Redis整合lua脚本 8. Redis消息模式队列模式 发布订阅者模式 9. 分布式锁分布式锁的实现方式 注意事项 分布式锁实战与高并发测试 10. 常见缓存问题与解决方案带来的问题都是：加大数据库压力 缓存穿透现象：数据库和缓存中都不存在的数据，查询这种不存在的数据的现象 解决方案： 布隆过滤器 BloomFilter（有比较小的误判的概率，会认为一个不存在的数据存在，即允许这样的请求执行） 缓存数据的ID 缓存击穿现象：大量请求查询同一个key，而这个key正好失效了，就会导致大量请求打到数据库上 解决方案：在查询数据库的逻辑前加排他锁，获取锁的线程可以查询，并将结果放入缓存（释放锁后，其他线程在获取锁前再读一次缓存，double check） 缓存雪崩现象：大量的key在同一时间失效，或者缓存宕机了 解决方案： 使用缓存集群，降低服务宕机的概率 本地缓存+限流&amp;降级 热点数据集中失效现象：一般会给key设置失效时间，热点数据请求比较多，会都打到数据库上 解决方案： 热点数据和非热点数据设置不同的失效时间 采用缓存击穿的方案，查询数据库加排他锁 设置为永不失效，采用定时任务对快失效的热点数据进行更新失效时间，进行续租 缓存双写一致性六、MongoDB第二阶段一、Zookeeper1. 简介重要概念讲解 Paxos算法 ZAB协议 CAP原则 2. 源码解析Watcher核心机制 Leader选举 3. 应用场景注册中心 分布式锁 分布式队列 负载均衡 配置维护 命名服务 DNS服务 分布式同步 集群管理 二、网络通信RPC原理与本质 RPC理论 RPC基于序列化信息通信 RPC解决什么问题 解决SOA编程模式问题……？ RPC实例实战 RPC基于socket相连 RPC基于动态代理调用透明化 RPC模块化演进 通过设计模式实现RPC模块单一职责 NettyIO模型原理 NIO事件驱动流性能优化 BIO和OIO堵塞流问题 AIO异步流使用场景 线程模型 NioEventLoop线程模型 驱动模式 ServerBootstrap启动原理 Bootstrap启动原理 Codec框架 tcp黏包、拆包 encode编码器 decode解码器 通道 channelHandler channelInboundHandler ChannelInitializer ByteToMessageDecoder channelOutboundHandler MessageToMessageEncoder LineEncoder 责任链模式 channelPipeline 上下文 channelHandlerContext 配置构建模式 channelConfig 内部类 unsafe 主从模型 BossGroup&amp;WorkerGroup 零拷贝 zero-copy 字节容器 ByteBuf原理 Netty实战 websocket聊天 三、Dubbo入门Dubbo分布式服务模块划分 高可用容错机制 服务降级 服务限流 服务暴露延迟 结果缓存应用 多版本控制 多注册中心 高级应用Dubbo负载均衡策略与自定义实现 仅订阅与仅注册 提供者的异步调用、异步执行 Dubbo源码解析 Dubbo的SPI Dubbo对spring配置文件的加载与解析 provider的服务暴露 Consumer的服务消费 四、Nginx五、消息中间件消息中间件在分布式架构中的应用场景：业务解耦/最终一致性/广播/错峰流控 JMS：Java Message Service，Java消息服务。试图通过提供公共Java API，隐藏单独MQ产品供应商提供的实际接口，跨越壁垒，解决互通问题。 AMQP：Advanced Message Queue Protocol，高级消息队列，2006年提出。是应用层协议的一个开放标准，基于此协议的客户端与消息中间件可传递消息，不受产品、开发语言等条件的限制。 四个项目都是开源的 产品 启动时间&amp;开发者 ，开源时间 语言 吞吐量 时效性 可用性 消息可靠性 说明 ActiveMQ 2004LogicBlaze，2007Apache Java 万级 毫秒级 高，基于主从架构 有较低的概率丢失数据 早期活跃、成熟产品，现在用的少。主要用于解耦和异步，较少在大规模吞吐的场景中使用。现在社区不活跃 RabbitMQ 2007RabbitMQ Technologies Ltd erlang 万级 微秒级 高，基于主从架构 erlang开发，并发能力强，性能极好，延时低。社区活跃 RocketMQ 2012阿里开源，2016Apache Java 十万级 毫秒级 非常高，分布式架构 可以做到0丢失 大规模吞入，性能好，分布式扩展方便。社区活跃一般 Kafka 2011LinkedIn，2012Apache Scala+Java 十万级 毫秒级 非常高 可以做到0丢失 超高吞吐量，极高的可用性和可靠性，分布式任意扩展。大数据领域实时计算、日志采集的事实标准 RabbitMQ概念 生产者和消费者 队列 交换器、路由键、绑定 Kafka 基于zookeeper实现高可用 消息处理过程剖析 副本机制与选举原理 六、SpringBoot七、SpringCloud反应式Web开发框架Webflux第三阶段一、FastDFS二、ElasticSearch互联网电商项目","tags":[]},{"title":"Learning English","date":"2020-10-21T19:50:18.000Z","path":"2020/10/22/Learning-English/","text":"如何记忆单词 阅读记忆 生词圈出，反复研读句子。勾画词组，了解替换（同义词替换），熟词僻（辟）意 www.economist.com 经纪学人 杂志 www.times.com 时代 www.gardian.co.uk 英国卫报 词根、词缀、词源、字母学规律 例子：un touch able 前缀 prefix —— a.肯定、否定 b.方向 c.数字 后缀 suffix —— 词性 词根 root —— 意义 Etymology 词源 联想 前缀 re again 再 back 向后 against 相反","tags":[]},{"title":"Raft学习笔记","date":"2020-09-23T15:32:53.000Z","path":"2020/09/23/Raft学习笔记/","text":"https://ongardie.net/static/raft/userstudy/ 保证： 一个任期内只有一个leader 三个状态： leader follower candidate 选举： 任期加一 切换为candidate状态 为自己投票 向其他所有节点发出投票请求（RequestVote RPCs），重试 从大多数节点收到投票 成为leader 向其他所有节点发送AppendEntries心跳 收到合法leader的RPC（=AppendEntries） 回退到follower状态 没有节点赢得选举（选举超时结束后）：【比如有多个节点进入candidate状态，选票分裂了，没有哪个节点收到大多数majority投票】 任期加一，开始新一轮选举 cont’d: continued的缩写，连接上文，续上 性质 安全性：每个任期允许至多一个leader 每个节点每个任期内只会发出一个投票（磁盘持久化） 同一任期内，不同candidate不会累加选票（可以理解为，要么不存在大多数，要么至多有一个大多数，不会有两个节点都获得大多数选票） 活性：终有一个candidate会胜出 随机选择选举超时[T,2T] 一个节点通常在其他节点醒来前超时并赢得选举 在T &gt;&gt; broadcast time时非常有效 normal operation 常规操作，正常运行 客户端向leader发送指令command leader将指令追加到日志log leader向followers发送AppendEntries RPCs 一旦新entry提交了committed： leader将指令发给状态机，向客户端返回结果 leader在随后的AppendEntries RPCs中通知followers提交entry followers将指令发给各自的状态机 对于宕机或者运行慢的followers： leader一直重试RPCs直到成功 并不影响leader对客户端的响应，不是必须等待 通常情况下，表现理想 一次RPC能触及大多数节点 Log Consistency 日志一致性 日志间的高度一致性： 如果不同节点的日志entry有相同的index和term，那么： 它们存储相同的指令 在这之前的日志项entry也都相同 如果一个entry是提交的，那么在它之前的entry也是提交的 AppendEntries一致性检查 每个AppendEntries RPC包含前一个entry的index和term follower包含前一个entry（index和term相同），才接受请求，否则拒绝这个请求 Safety Requirement 安全需求 已提交-&gt;保证majority大多数的节点上有最大的index，即使重新选举，那些缺失最大index的节点会选举失败-&gt;保证log出现在后续的leader节点上 Election Rules 如果投票节点的日志比candidate的”完整”，则拒绝candidate的选票 主要看term和index 任期和索引 V：投票节点 C：candidate 候选节点 (lastTerm V &gt; lastTerm C) || (lastTerm V == lastTerm C) &amp;&amp; (lastIndex V &gt; lastIndex C) Commitment Rules leader判断一个entry是否已提交，需要考虑 entry必须存在大多数节点上 至少有一个当前leader任期的新entry也存在大多数节点上 第二点不是特别理解.. Client Protocol 将指令发送给leader 直到指令追加进日志、已提交、并且被leader的状态机执行，leader才会响应客户端 如果请求超时（比如leader宕机）： Client（随机）重发指令给其他节点 被定向到一个新leader 向新leader重试请求 如果leader在执行完指令、响应之前宕机了，为了不执行同一条指令两次，解决方案： Client的指令请求里包含unique id 节点的log里存储每个请求的id leader在接收指令前，检查日志中是否已存在指令中的ID 如果存在，忽略新指令，直接返回旧指令对应的响应 达到exactly-once semantics 仅执行一次语义，线性一致性","tags":[]},{"title":"学习笔记-大数据技术体系详解","date":"2020-09-11T10:42:36.000Z","path":"2020/09/11/学习笔记-大数据技术体系详解/","text":"广度上认识大数据体系 大数据体系逻辑图 数据收集 &gt; 数据存储层 &gt; 资源管理与服务调度 &gt; 计算引擎 &gt; 数据分析 Part2 数据收集篇CH1 概述1.2 企业级大数据技术框架 Google的大数据技术栈实现 开源的大数据技术栈实现 CH4 分布式消息队列Kafka设计动机降低数据生产者与消费者之间的耦合性，使系统更易扩展 特点 高性能：对比其他消息队列有更高的性能和吞吐率（优秀的设计实现） 良好的扩展性：采用分布式设计架构，数据经分片（分区+副本）后写入多个节点，既可以突破单节点数据存储和处理的瓶颈，也可以实现容错等功能 数据持久化：数据消息均会持久化到磁盘上，并通过副本策略避免数据丢失 采用顺序写、顺序读和批量写等机制，提升磁盘操作的效率。 概念topic：kafka中的消息以主题为单位进行归类，生产者将消息发到特定的主题，消费者订阅主题并进行消费。 partition：主题可细分为多个分区，一个分区只属于单个主题。分区在存储层面可以看作一个可追加的日志Log文件 offset：消息在分区中的唯一标识，在消息被追加到分区日志文件的时候分配的一个特定的偏移量 Consumer：生产者，发送消息的一方 Producer：消费者，接收消息的一方 Broker：服务代理节点，kafka服务节点or服务实例 基本架构Producer+Broker+Consumer Producer将数据写入Broker，Consumer从Broker读取数据进行处理 多个Broker构成一个可靠的分布式消息存储系统，避免数据丢失 push-pull架构，Consumer从Broker pull数据的优势： Consumer可根据自己的实际负载和需求获取数据，避免push方式给Consumer带来较大压力 Consumer自己维护已读数据的offset，而不是Broker维护，大大缓解Broker的压力，使它更加轻量 关键技术点 可控的可靠性级别：支持三种消息应答方式，通过request.required.acks控制 0：无需对消息进行确认，Producer向Broker发送消息后马上返回，无需等待对方写成功。写入性能高，容错低。 1：Producer向Broker发送消息，需等待leader副本写成功后才返回，对应得follower副本不一定写成功。折中。 -1：…需等待leader+follower副本都写成功才返回。写入性能低，容错高。 数据多副本：一个分区都多个副本，leader+follower 高效的持久化机制：直接将数据持久化到磁盘上，而不是内存中。 数据传输优化：批处理与zero copy 批处理：降低单条消息传输带来的网络开销，Producer发送数据时将多条消息组装在一起ProducerBatch，存储和发送采用统一的数据格式，Broker发送给Consumer也是批量的 零拷贝：四次数据拷贝变成三次，少了内核态和用户态的两次拷贝，直接由内核态read buffer到内核socket buffer 可控的消息传递语义 at most once：至多一次，消息发送后立即返回，不关心对方时候成功接收。消息可能成功接收，也可能丢失。 at least once：至少一次，消息发送后需要等待确认，如果未收到确认，则会重发消息。保证能收到消息，但可能收到多次。 exactly once：会且只会收到一次同一条消息。常用技术手段： 两阶段提交协议：分布式系统中常用一致性协议 在支持幂等操作（多次处理一条消息和只处理一次是等效的）的前提下，使用at least once。 应用场景 消息队列 流式计算框架的数据源 Part3 数据存储篇 数据序列化：将内存对象转化为字节流，决定了数据解析效率以及模式演化能力（数据格式发生变化，比如增删字段，能否保持兼容性）。 文件存储格式：数据在磁盘上的组织方式，决定了数据存取效率，以及被上层分布式计算集成的容易程度。 存储系统：针对不同类型的数据，可采用不同的存储系统。 CH5 数据序列化与文件存储格式数据序列化的演化阶段 转化为字符串，以文本形式保存或传输，面临的问题： 难以表达嵌套数据 无法表达二进制数据：图片视频等 难以应对数据模式变化 语言内置的序列化机制，如Java的Serialization，Python的pickle。问题：和语言绑定在一起，难跨语言 JSON和XML等。问题：性能问题，解析速度慢，同时数据冗余较大，比如JSON重复存储每个属性的名称 带有schema描述的数据表示格式，如Thrift、Protocol buffers、Avro，称为“Language of Data”。具备特征： 提供IDL（Interface Description language）用以描述数据schema，容易描述任意结构化数据和非结构化数据 支持跨语言读写——指可以生成目标语言的代码 数据编码存储（整数可采用变长编码，字符串可采用压缩编码等），尽量避免不必要的存储浪费 支持schema演化 性能对比 时间从小到大：Protobuf、Thrift、Avro 大小从小到大：Avro、Protobuf、Thrift compact、Thrift binary 文件存储格式行式存储：文本格式text file、key value二进制存储格式sequence file 列式存储：ORC、Parquet、Carbon Data CH6 分布式文件系统 角色NameNode：集群管理者，管理文件系统元信息（文件系统目录树）和所有DataNode（DataNode向NameNode汇报心跳，若DataNode故障，则在其他存活DataNode上重构丢失的数据块）。 DataNode：存储实际的数据块 Client：客户端，文件的分块在客户端完成，从NameNode领取多个DataNode地址，与DataNode建立数据流水线，将数据块写入DataNode。 HDFS一般不支持编辑修改。可以写入、删除、查询。 CH7 分布式结构化存储系统HBase构建在分布式文件系统HDFS之上，支持随机插入和删除，列式存储系统。可以理解为，具有持久化能力的分布式多维有序映射表。 HBASE随机读写性能较高，但数据扫描比较慢，难以适用于OLAP场景。 Cloudera提出Kudu项目，很好地兼顾吞吐率和延迟。 特点极好的扩展性：随着数据量的增加，支持自动水平扩展，满足存储要求 弱化ACID需求：不少大数据应用场景中，对事物的要求比较低，可选择性支持 良好的容错性：大数据存储应用倾向于选择成本较低的横向扩展方案，要求数据存储软件具有良好的故障自动处理能力 逻辑数据模型 rowkey：类似于主键，表内全局有序 column family：schema一部分，预先定义。每行相同。同一column family的数据在屋里上存储在一个文件中。 column qualifier：column family内部列标识，可动态制定，每行数据可有不同column qualifier cell：通过rowkey, column family和column qualifier可唯一定位一个cell，内部保存多个版本的数值 timestamp：cell数据的版本，默认写入时间为版本号，可自定义，数据类型为long 表示成多维映射表： 物理数据模型HBASE是列簇式存储引擎 以column family为单位存储数据，每个column family内部数据以key value格式保存： [row key, column family, column qualifier, timestamp] =&gt; value rowkey升序，column family升序，版本号降序 HBASE不是列式存储（列式存储以列为单位，压缩比高、读IO少）。 同一列簇中的数据会单独存储，但列簇内数据是行式存储的。 为了将HBASE改造成列式存储，进一步提高读写新能，出现了Kudu。 列簇的优点：同一family的同时读取，比较快？理解为捆绑的几个列？如果这些列分散在列式里，读写性能没有列簇好？ 列式的优点：…？ 基本架构 HMaster：协调RegionServer（为RegionServer分配region，均衡RegionServer的负载，发现失效RegionServer并重新分配其上的region），元信息管理（提供table表的增删改查） RegionServer：负责各个Region的存储和管理，与Client交互，处理读写请求 Zookeeper：存储元信息和状态信息，担任协调角色 Client：与RegionServer交互读写数据，维护缓存 BlockCache：读缓存 MemStore：写缓存，未写入磁盘的数据——在内存中 HFile：支持多级索引的数据存储格式，保存HBASE表中实际的数据。所有HFile均保存在HDFS中。 WAL：write ahead log，预写日志，保存未持久化到HDFS的HBASE数据，以便RegionServer宕机恢复数据。 写流程 RegionServer收到请求，以追加的形式写入HDFS上的日志文件，即WAL RegionServer将数据写入MemStore，通知客户端写成功 MemStore达到一定阈值后，将数据顺序刷入HDFS中，保存成HFile格式 读流程 扫描读缓存BlockCache，缓存了最近读取 扫描写缓存MemStore，缓存了最近写入 如果两个缓存中没有命中，读取HFile 每个column family有一个MemStore，上面提到的物理数据格式，key value形式 HFile是Google Sorted String Table（BigTable用到的存储格式）的开源实现，一种有序key value磁盘存储格式，带有多级索引，方便定位数据，多级索引类似于B+树。不太懂，先跳过。 Part4 分布式协调与资源管理CH8 分布式协调服务ZooKeeper数据模型 层级命名空间，命名方式类似于文件系统，以多叉树形式组织在一起。每个节点称为znode，包含以下属性： data：数据域 type：znode类型 persistent 持久 ephemeral 临时 sequencial 顺序 version znode中数据的版本号，每次更新版本号加一 children znode的子节点，临时节点不能有子节点 ACL 访问控制列表，可单独设置每个znode的可访问用户列表 zookeeper能保证数据访问的原子性，即znode的数据要么写成功、要么写失败 Watcher 发布订阅机制，在znode上注册watcher以监听变化 watcher一旦触发后便会被删除，除非用户再次注册该watcher。 session 客户端与zookeeper服务端之间的通信通道，同一个session中的消息是有序的。 Session具有容错性：如果客户端连接的ZooKeeper服务器宕机，客户端会自动连接到其他活着的服务器上。 CH9 资源管理与调度系统YARN分离资源管理和任务调度/监控 split up the functionalities of resource management and job scheduling/monitoring into separate daemons 基本架构 工作流程 调度系统的架构演化Google：Omega: flexible, scalable schedulers forlarge compute clusters 中央式调度器架构，类似于Hadoop JobTracker 资源的调度和应用程序的管理功能放在一个进程中 扩展性差：集群规模受限，难以融入新的调度策略 双层调度器架构，类似于Mesos和YARN 保留一个简化的集中式资源调度器，分配集群中的资源给引用程序 具体任务相关的调度策略下放到应用程序调度器中，应用程序将资源分配给各个任务 缺点：各个应用程序无法知道整个集群的实时资源使用情况；使用悲观锁，任意时刻一个资源只会推送给一个框架/应用程序，并发粒度小 共享状态架构，Omega 将集中式资源调度器简化为一些持久化的共享数据和针对这些数据的验证代码，共享数据=整个集群的实时资源使用信息 应用程序自己控制资源分组、资源使用量、用户的资源使用量 多个应用程序申请同一份资源时，优先级高的应用程序获得 引入多版本并发控制…——具体控制什么的多版本并发，不太懂，跳过 Part5 计算引擎篇CH10 批处理引擎MapReduce组成：编程模型+运行时环境 易用的编程接口 节点间的通信、节点失效、数据切分 产生背景 扩展学习：Nutch，2002年由Doug Cutting创建，是一个开源的网络搜索引擎，目标是构建一个大型的全网搜索引擎，包括网页抓取、索引和查询等功能。随着抓取的网页数量的增加，遇到了可扩展问题：不能解决十亿网页的存储和索引问题。 基于Google论文分布式文件系统GFS（2003）和分布式计算框架MapReduce（2004）完成了开源实现Hadoop。 约2006年，Doug加入雅虎，组装专门团队继续发展Hadoop。 2008年，Hadoop称为Apache顶级项目。 设计目标 编程模型 基本架构 看的头大，不总结了 CH11 DAG计算引擎Spark特点 性能高效 简单易用 与Hadoop完好集成 核心概念RDD：Resilient Distributed Datasets 弹性分布式数据集，只读的、带分区的数据集合 DAG：Directed Acyclic Graph CH12 交互式计算引擎CH13 流式实时计算引擎Part6 数据分析篇CH14 数据分析语言HQL与SQLCH15 大数据统一编程模型CH16 大数据机器学习库","tags":[]},{"title":"学习笔记-Linux内核设计与实现","date":"2020-09-10T17:44:56.000Z","path":"2020/09/11/学习笔记-Linux内核设计与实现/","text":"高级编程语言-&gt;汇编语言-&gt;机器指令 CH3 进程管理3.1 进程fork() exec() exit() 3.2 进程描述符及任务结构进程的列表：任务列表 task list 进程描述符 task_struct / process descriptor：打开的文件，进程的地址空间，挂起的信号，进程的状态，等 3.4 线程在Linux中的实现Linux把所有的线程都当做进程来实现，一个可与其他进程共享某些资源的进行。 CH4 进程调度程序在可运行态进程之间分配有限的处理器时间资源的内核子系统 进程主动挂起自己的操作称为让步yielding CH7 中断和中断处理中断本质上是一种特殊的电信号，由硬件设备发向处理器。处理器接受到中断后，会马上向操作系统反应此信号的到来，由操作系统负责处理这些新到来的数据。 内核随时可能因为新到来的中断而被打断。","tags":[]},{"title":"HashMap红黑树源码分析学习笔记","date":"2020-08-10T18:27:00.000Z","path":"2020/08/11/HashMap红黑树源码分析学习笔记/","text":"把根节点移动到槽位置 123456789101112131415161718192021222324252627/** * Ensures that the given root is the first node of its bin. */static &lt;K,V&gt; void moveRootToFront(Node&lt;K,V&gt;[] tab, TreeNode&lt;K,V&gt; root) &#123; int n; if (root != null &amp;&amp; tab != null &amp;&amp; (n = tab.length) &gt; 0) &#123; int index = (n - 1) &amp; root.hash; TreeNode&lt;K,V&gt; first = (TreeNode&lt;K,V&gt;)tab[index]; // root与fist不相同 // 把root从链表中取出来，放到槽位置，root的next指向原来的first // ps: 这样改变了原链表的顺序 if (root != first) &#123; Node&lt;K,V&gt; rn; // root next tab[index] = root; TreeNode&lt;K,V&gt; rp = root.prev; if ((rn = root.next) != null) ((TreeNode&lt;K,V&gt;)rn).prev = rp; if (rp != null) rp.next = rn; if (first != null) first.prev = root; root.next = first; root.prev = null; &#125; assert checkInvariants(root); &#125;&#125;","tags":[]},{"title":"网络与操作系统","date":"2020-07-27T16:29:48.000Z","path":"2020/07/28/technology/05网络与操作系统/","text":"网络计算机网络TCP/UDP的区别 UDP：用户数据报协议 UDP(User Datagram Protocol)是无连接的，尽最大可能交付，没有拥塞控制，面向报文 (对于应用程序传下来的报文不合并也不拆分，只是添加 UDP 首部)，支持一对一、一对多、多对一和多对多 的交互通信。 TCP：传输控制协议 TCP(Transmission Control Protocol)是面向连接的，提供可靠交付，有流量控制，拥塞控 制，提供全双工通信，面向字节流(把应用层传下来的报文看成字节流，把字节流组织成大小不等的数据 块)，每一条 TCP 连接只能是点对点的(一对一)。 UDP首部格式 首部字段只有 8 个字节，包括源端口、目的端口、长度、检验和。12 字节的伪首部是为了计算检验和临时添加的。 TCP首部格式 序号 :用于对字节流进行编号，例如序号为 301，表示第一个字节的编号为 301，如果携带的数据长度为 100 字节，那么下一个报文段的序号应为 401。 确认号 :期望收到的下一个报文段的序号。例如 B 正确收到 A 发送来的一个报文段，序号为 501，携带的数据 长度为 200 字节，因此 B 期望下一个报文段的序号为 701，B 发送给 A 的确认报文段中确认号就为 701。 数据偏移 :指的是数据部分距离报文段起始处的偏移量，实际上指的是首部的长度。 确认 ACK :当 ACK=1 时确认号字段有效，否则无效。TCP 规定，在连接建立后所有传送的报文段都必须把 ACK 置 1。 同步 SYN :在连接建立时用来同步序号。当 SYN=1，ACK=0 时表示这是一个连接请求报文段。若对方同意建 立连接，则响应报文中 SYN=1，ACK=1。 终止 FIN :用来释放一个连接，当 FIN=1 时，表示此报文段的发送方的数据已发送完毕，并要求释放连接。 窗口 :窗口值作为接收方让发送方设置其发送窗口的依据。之所以要有这个限制，是因为接收方的数据缓存空 间是有限的。 TCP如何保证传输的可靠性。使用超时重传来实现可靠传输：如果一个已经发送的报文段在超时时间内没有收到确认，那么就重传这个报文段。 TCP滑动窗口 暂时存放字节流。发送方和接收方各有一个窗口，接收方通过TCP报文段中的窗口字段告诉发送方自己的窗口大小，发送方根据这个值和其他信息设置自己的窗口大小。 发送窗口内的字节都允许被发送，接收窗口内的字节都允许被接收。如果发送窗口左部的字节已经发送并且收到了确认，那么就将发送窗口向右滑动一定距离，直到左部第一个字节不是已发送并且已确认的状态;接收窗口的滑动类似，接收窗口左部字节已经发送确认并交付主机，就向右滑动接收窗口。 接收窗口只会对窗口内最后一个按序到达的字节进行确认，例如接收窗口已经收到的字节为 {31, 34, 35}，其中 {31} 按序到达，而 {34, 35} 就不是，因此只对字节 31 进行确认。发送方得到一个字节的确认之后，就知道这个字节之前 的所有字节都已经被接收。 TCP的拥塞控制 与流量控制的区别： 流量控制是上一题里窗口，接收方发送窗口值来控制发送方的窗口大小，从而影响发送方的发送速率。将窗口值设置为0，则发送方不能发送数据。 控制发送方的发送速率，保证接收方来得及接收。 拥塞控制 是为了降低整个网络的拥塞程度 主要通过四个算法进行拥塞控制：慢开始、拥塞避免、快重传、快恢复。 发送方需要维护一个叫做拥塞窗口(cwnd)的状态变量（只是一个状态变量，不是发送方窗口。再区别一下，拥塞窗口讨论的是报文段数量，发送窗口讨论的是字节数量） 慢开始与拥塞避免 发送的最初是慢开始，cwnd=1，发送方只能发送一个报文段；接收到确认后，将cwnd加倍，之后能发送的报文段数量是2、4、8.. ssthresh是慢开始门限（初始值自己定），当cwnd &gt;= ssthresh 时，进入拥塞避免，每个轮 次只将 cwnd 加 1。 如果出现超时，则另ssthresh = cwnd / 2，并重新执行慢开始。 见图1、2、3 快重传与快恢复 【在接收方，要求每次接收到报文段都应该对最后一个已收到的有序报文段进行确认。例如已经接收到 M1 和 M2，此时收到 M4，应当发送对 M2 的确认。】 在发送方，如果收到三个重复确认，那么可以知道下一个报文段丢失，此时执行快重传，立即重传下一个报文段。【例如收到三个 M2，则 M3 丢失，立即重传 M3。】 同时执行快恢复，令 ssthresh = cwnd / 2 ，cwnd = ssthresh，并直接进入拥塞避免。 见上图4、5 TCP建立连接的三次握手假设A为客户端，B为服务端 首先B处于监听（listen）状态，等待客户的连接请求 A向B发送连接（SYN，同步）请求报文，SYN=1，ACK=0，seq=x（选择一个初始的序号x） B收到连接请求报文，如果同意建立连接，则向A发送连接确认报文，SYN=1，ACK=1，ack=x+1（确认号为x+1），seq=y（同时也选择一个初始的序号y） A收到B的连接确认报文后，还要向B发出确认，seq=x+1（序号为x+1），ack=y+1（确认号为y+1） 为什么要三次握手？ 三次握手是为了防止失效的连接请求到达服务器，让服务器错误打开连接。 客户端发送的连接请求如果在网络中滞留，那么隔很长时间才能收到服务器发回的连接确认，在这段时间内，客户端等待一个超时重传时间后，就会重新发送连接请求。同时滞留的连接请求最后还是会到达服务器，如果只是两次握手，那么服务器会打开两个连接。如果有第三次握手，客户端会忽略服务器之后发送的对滞留连接请求的连接确认，不进行第三次握手，因此就不会再次打开连接。 TCP四次挥手断开连接ack都为1. A 发送连接释放报文，FIN=1。 B 收到之后发出确认，此时 TCP 属于半关闭状态，B 能向 A 发送数据但是 A 不能向 B 发送数据。 当 B 不再需要连接时，发送连接释放报文，FIN=1。 A 收到后发出确认，进入 TIME-WAIT 状态，等待 2 MSL(最大报文存活时间)后释放连接。 B 收到 A 的确认后释放连接。 四次挥手的原因 客户端发送FIN连接释放报文后，服务器收到这个报文就进入CLOSE_WAIT状态，这个状态是为了让服务器端发送未传送完毕的数据，发完后服务器就会发送FIN连接释放报文。 TIME_WAIT 客户端收到服务端的FIN报文后进入此状态，并不是直接进入CLOSED状态，还需要等待一个时间计时器设置的时间2MSL。有两个理由： 确保最后一个确认报文能够到达。如果 B 没收到 A 发送来的确认报文，那么就会重新发送连接释放请求报文， A 等待一段时间就是为了处理这种情况的发生。 等待一段时间是为了让本次连接持续时间内所产生的所有报文都从网络中消失，使得下一个新的连接不会出现旧的连接请求报文。 哪些典型的应用用的是udpdns: Domain Name System，域名系统 域名解析 TFTP: Trivial File Transfer Protocol,简单文件传输协议 1.包总量较少的通信（DNS、SNMP等） 2.视频、音频等多媒体通信（即时通信） 3.限定于 LAN 等特定网络中的应用通信 4.广播通信（广播、多播） HTTPhttps和http区别，有没有用过其他安全传输手段？区别： http明文传输，安全性低；HTTPS数据加密传输，安全性高 使用https协议需要到CA（Certificate Authority，数字证书认证机构）申请证书 http的响应速度比HTTPS快，因为HTTPS除了http三次握手的包，还要加上ssl的交互–具体是？ 端口不同，http80端口，https443端口 https本质是构建在ssl/tls之上的http协议 HTTP 与 HTTPS 的区别 Http协议 基础概念 URI：uniform resource identifier 统一资源标识符 URL：uniform resource locator 统一资源定位符 URN：uniform resource name 统一资源名称 URI包括URL和URN 请求报文的格式 request line 请求行：请求方法，URL，协议 request headers 请求头：各种header 请求行和请求头合称为请求消息头 空行分隔开请求头和请求消息体 request message body 请求消息体：key-value形式或者raw格式等等 响应报文的格式 status line 状态行：协议，状态码 response headers 响应头 状态行和响应头合称为响应消息头 空行分隔开消息头和消息体 response message body 响应消息体 HTTP方法 get 主要用来获取资源 head 获取报文首部，主要用于确认 URL 的有效性以及资源更新的日期时间等。 post 主要用来传输数据 put 上传文件，不带验证机制存在安全问题，一般不使用 patch 对资源进行部分修改 – 也不常用 delete 删除文件，与put功能相反，同样不带验证机制 options 查询支持的方法，会返回Allow: GET, POST, HEAD, OPTIONS这样的内容 connect 要求在与代理服务器通信时建立隧道。使用 SSL(Secure Sockets Layer，安全套接层)和 TLS(Transport Layer Security，传输层安全)协议把通信内容 加密后经网络隧道传输。 trace 追踪路径，一般也不用… HTTP状态码 简要记一下 1XX 信息性状态码，接收的请求正在处理 2XX 请求正常处理完毕 3XX 重定向 4XX 客户端错误 5XX 服务端错误 再关注下前面的http和HTTPS的比较 其他安全传输手段：SSH SSH 协议原理、组成、认证方式和过程 延伸 https的特性：加密保证安全性防窃听、认证防伪装、完整性防篡改 加密方式：混合加密，用非对称加密传输对称秘钥，用对称秘钥进行要传输的数据的加解密 认证：使用证书来对通信双方认证。 完整性：ssl提供报文摘要功能来进行完整性保护。 http也可以通过md5验证完整性，但数据篡改后也可重新生成md5，因为是明文的。https是通过ssl的报文摘要来保证完整性的，结合了加密与认证，即使加密后数据被篡改，也很难再生成报文摘要，因为不知道明文是什么。 cookie session介绍一下 cookie 是服务器发送到用户浏览器并保持在本地的一小块数据，会在浏览器向同一服务器再次发起请求时被带上。 用途： 会话状态管理（比如用户登录状态、购物车等） 个性化设置（比如用户自定义设置、主题等） 浏览器行为分析 生成方式 服务器发送Set-Cookie: yummy_cookie=choco这样的header，客户端得到响应报文后把cookie存在浏览器 浏览器通过document.cookie属性可创建新的cookie HttpOnly 标记为 HttpOnly 的 Cookie 不能被 JavaScript 脚本调用。 Secure 标记为 Secure 的 Cookie 只能通过被 HTTPS 协议加密过的请求发送给服务端。但即便设置了 Secure 标记，敏感信 息也不应该通过 Cookie 传输，因为 Cookie 有其固有的不安全性，Secure 标记也无法提供确实的安全保障。 session 存储在服务端，可以存储在服务器上的文件、数据库或者内存中。也可以将 Session 存储在 Redis 这种内存型数据库中 使用 Session 维护用户登录状态的过程如下: 用户进行登录时，用户提交包含用户名和密码的表单，放入 HTTP 请求报文中; 服务器验证该用户名和密码，如果正确则把用户信息存储到 Redis 中，它在 Redis 中的 Key 称为 Session ID; 服务器返回的响应报文的 Set-Cookie 首部字段包含了这个 Session ID，客户端收到响应报文之后将该 Cookie 值存入浏览器中; 客户端之后对同一个服务器进行请求时会包含该 Cookie 值，服务器收到之后提取出 Session ID，从 Redis 中取 出用户信息，继续之前的业务操作。 cookie和session的选择 cookie只能存储ASCII码字符串，session可以存储任何类型的数据 cookie存储在浏览器中，安全性较低 对于大型网址，如果所有用户信息都存储在session中，开销比较大 – 【感觉不是个问题…】 session表结构怎么设计，储存在哪里？ 我们项目里没有直接使用session，用的是商城统一单点登录 如果我设计 首先一个用户请求过来，如果没有带session id，先重定向到登录页 收到登录请求，身份验证通过后，生成一个session，key为唯一ID，即session id，value为需要存储的信息，比如用户名、生成时间等，将session id作为cookie响应发回浏览器 众多的session是key-value结构，session本身也是key-value结构 存储在Redis 你们的session cookie在项目里运用到哪里？ session是SSO用的，cookie也主要是SSO用的 偶尔用的cookie是虚拟登录这样的场景 比如超级账号：员工的erp账号以只读的形式登录到用户账号，主要用于排查问题 比如账号管家：系统中，账号体系中的主账号可以登录到子账号上，一般也只读 再如虚拟登录，业务范畴上，两个账号建立授权关系，B账号可以虚拟登录到A账号上，代为操作系统 实现：被登录人一般是sso中的session对应的用户，属于资源所属者；操作者是erp账号、主账号、虚拟登录账号等，会有登录类型区分，这些信息会先加密，再存入cookie中（还会有不同的拦截器，进行身份和权限验证） 单点登录的实现 CAS TGT：Ticket Granted Ticket（俗称大令牌，或者说票根，他可以签发ST）。【类似session】 TGC：Ticket Granted Cookie（cookie中的value），存在Cookie中，根据他可以找到TGT。【类似session id】 ST：Service Ticket （小令牌），是TGT生成的，默认是用一次就生效了。也就是上面的ticket值。 ps: 未登录状态下，访问app1时，展示登录页，浏览器会写入cas服务器的TGC；第二次访问app2，（因为app2本身校验当前请求未登录）重定向到cas服务器时，会带上TGC，cas服务器根据TGC判断用户已登录，签发新的ST再重定向到app2，这时候app2用ST校验通过，记录下自己的session cookie，提供请求内容。 OAuth 【不看了不看了！】 https://juejin.im/post/5cc81d5451882524f72cd32c https://juejin.im/post/5b3b3b61f265da0f955ca780 操作系统冯诺依曼计算机的结构运算器（算术逻辑单元，处理寄存器） 控制器（指令寄存器，程序计数器） 存储器（存储数据和指令） 输入设备 输出设备 Linux怎么查看系统负载情况？ uptime w top 查看linux系统负载情况 线上服务器cpu飙高，如何处理这个问题 定位进程：top 查看cpu占用情况 定位线程：如果是Java应用，top -Hp pid 定位代码` printf %x tid 打印出线程ID对应的16进制数 0xtid jstack pid |grep -A 200 0xtid 内核态 和 用户态、cas 和 sout 哪个用到了内核态和用户态的切换sout用到了切换 进程的调度进程间的通讯方式线程间的同步方式进程和线程的区别","tags":[]},{"title":"Java","date":"2020-07-27T16:29:48.000Z","path":"2020/07/28/technology/02Java/","text":"ConcurrentHashMaphttps://bbs.huaweicloud.com/blogs/151782 transfer讲的比较好 JavaJava基础Java反射原理， 注解原理？反射原理：在运行状态下，对于任何一个类，能够知道这个类的所有属性和方法；对于任意一个对象，都能调用它的任意方法，并能改变它的属性。总结来说，反射把Java类中的各个成分映射成为一个个Java对象，并且可以进行操作。 注解原理：注解的本质是一个继承了Annotation接口的接口。 解析一个类或者方法的注解有两种形式，一是编译期扫描，如@Override，编译器会检查方法是否真的重写了父类的某个方法；二是运行期反射，虚拟机规范定义了一系列和注解相关的属性表，字段、属性或类上有注解时（被注解修饰了），会写相应信息进字节码文件，Class类中提供了一些接口用于获取注解或判断是否被某个注解修饰。 延伸阅读：JAVA 注解的基本原理 ps: Java类执行的过程/类加载过程（2-6）/类的生命周期（2-8） – tbc 更准确的说法 编译：Java文件编译成.class字节码文件 加载：类加载器通过全限定名，将字节码加载进JVM，存储在方法区，将其转换为一个与目标类型对应的Class对象实例 验证：格式（.class文件规范）验证和语义（final不能继承等）验证？ 准备：静态变量赋初值与内存空间，final修饰的内存空间直接赋原值（？），不是开发人员赋的初值 解析：符号引用转换为直接引用，分配地址（?） 初始化：先初始化父类，再初始化自身；静态变量赋值，静态代码块执行。 使用 卸载 Java中==、equals与hashCode的区别和联系https://juejin.im/entry/59b3897b5188257e733c24eb – 后面写的比较乱 https://juejin.im/post/5a4379d4f265da432003874c – equals与hashCode Java数据类型 8种基本数据类型 （整型）数值类型 byte short int long 1.2.4.8 （浮点）数值类型 float double 4.8 字符型 char 2 存储 Unicode 码，用单引号赋值。 布尔类型 boolean 1 3种引用类型：类、接口、数组 == 比较两个数据是否相等，基本类型比较数值是否相等，引用类型比较地址是否相等。 equals()方法 Object类型定义的，比较二者== 1234//object的equals方法public boolean equals(Object obj) &#123; return (this == obj);&#125; 想自定义对象逻辑“相等”（值相等、或内容相等）的含义时，重写equals方法。 重写equals准则： 自反性：对于任何非空引用值 x，x.equals(x) 都应返回 true。 对称性：对于任何非空引用值 x 和 y，当且仅当 y.equals(x) 返回 true 时，x.equals(y) 才应返回 true。 传递性：对于任何非空引用值 x、y 和 z，如果 x.equals(y) 返回 true， 并且 y.equals(z) 返回 true，那么 x.equals(z) 应返回 true。 一致性：对于任何非空引用值 x 和 y，多次调用 x.equals(y) 始终返回 true 或始终返回 false， 前提是对象上 equals 比较中所用的信息没有被修改。 非空性：对于任何非空引用值 x，x.equals(null) 都应返回 false。 一般只判断同类型的对象 主要不要违反了对称性、传递性 hashCode()方法 1public native int hashCode(); equals的对象hashcode一定相等，hashcode相同的对象不一定equals 为什么对象的hashcode会相同？ hashcode的实现取决于jvm，比较典型的一种是基于内存地址进行哈希计算，也有基于伪随机的实现。 哈希计算会存在哈希碰撞。 https://juejin.im/entry/597937cdf265da3e114cd300 谈谈final、finally、finalize的区别 – 放一起有点奇怪final 修饰类、方法或变量 修饰类：表明类不能被继承 方法：禁止在子类中被覆盖（private方法会隐式被指定为final） 变量： 基本数据类型的变量：数值在初始化后不能更改 引用类型的变量：初始化后不能再指向另一个对象（指向的地址不可变） finally： 一般与try catch一起使用，无论程序抛出异常或正常执行，finally块的内容一定会被执行。 最常用的地方：通过try-catch-finally来进行类似资源释放、保证解锁等动作。 finalize Object的protected方法，子类可以覆盖该方法以实现资源清理工作，GC在回收对象之前调用该方法。 日常开发中基本不用，也不推荐使用。Java9中被标记为deprecated! – 不想多说 https://juejin.im/post/5b9bb81ef265da0ac2565a0f java如何实现序列化的，Serialization底层如何实现的简单说来，是将类信息和数据信息递归写成字节信息 序列化定义：将对象的状态信息转化为可存储或传输的形式（的过程）。 讲一讲AtomicInteger，为什么要用CAS而不是synchronized？整形的原子操作类，可在并发场景中使用。 主要方法是getAndIncrement自增等。 因为这里临界区域的操作非常简单，只需要改变一个基本类型 变量的值，cas配合volatile即可实现原子性 synchronized加锁的成本比CAS大 java中的反射反射定义：指程序在运行时可以访问、检测和修改它本身状态或行为的一种能力。 field的赋值底层实现 以UnsafeBooleanFieldAccessorImpl为例，也是利用unsafe 偏移 ps: Unsafe工具类 static final Unsafe unsafe = Unsafe.getUnsafe(); 1234567891011121314151617181920212223242526272829// set public void set(Object obj, Object value) throws IllegalArgumentException, IllegalAccessException &#123; ensureObj(obj); if (isFinal) &#123; throwFinalFieldIllegalAccessException(value); &#125; if (value == null) &#123; throwSetIllegalArgumentException(value); &#125; if (value instanceof Boolean) &#123; // 这里 unsafe.putBoolean(obj, fieldOffset, ((Boolean) value).booleanValue()); return; &#125; throwSetIllegalArgumentException(value); &#125;// get public Object get(Object obj) throws IllegalArgumentException &#123; return Boolean.valueOf(getBoolean(obj)); &#125; public boolean getBoolean(Object obj) throws IllegalArgumentException &#123; ensureObj(obj); // 这里 return unsafe.getBoolean(obj, fieldOffset); &#125; Java容器1. Java容器有哪些？哪些是同步容器,哪些是并发容器？容器分两个大类，Collection和Map。Collection又分List、Set、Queue、Vector几个大类，Map有HashMap、TreeMap、LinkedHashMap、HashTable，其中，Vector、HashTable是同步容器。 并发容器一般在juc包下，有ConcurrentHashMap、CopyOnWriteArrayList等。 ps: List: ArrayList、LinkedList Set: HashSet、LinkedHashSet、TreeSet Queue: LinkedList、PriorityQueue 引申：几个容器的主要方法的操作流程，容器体系结构 2. ArrayList和LinkedList的插入和访问的时间复杂度？ArrayList：插入O(n) 访问O(1) LinkedList：插入O(1) 访问O(n) HashMap在什么情况下会扩容，或者有哪些操作会导致扩容？java8中 放入新值（putValue–put/putMapEntries）后，元素个数size大于阈值threshold，会触发扩容。 链表树化时，如果表长table.length小于64，会用扩容代替树化。 put值前，如果表长为0，会触发扩容 HashMap put方法的执行过程？ 如果table为空，或长度为0，初始化。默认loadFactor为0.75，默认capacity为16（capacity是table的长度），threshold一般为capacity*loadFactor。 通过hash定位槽，如果槽为空，构造新节点赋值给槽 若槽不为空，则在槽的链表或树中找到key相同的节点，替换节点值为新值；或是没有key相同的节点，就在树中或链表尾部加入新节点；若链表加入新节点后长度达到8（槽不算，aka槽下原有7个节点），则进行红黑树转化 如果是新加入节点，modCount、元素个数size自增1，如果元素个数超过阈值，则进行扩容 Java8扩容的执行过程？ 计算新容量newCap和新阈值newThr（ps: 当容量已到最大值时，不再扩容；2倍扩容；） 创建新的数组，赋值给table 将键值对重新映射到新数组上 如果无链表，根据hash&amp;(newCap-1)定位 如果是树节点，委托红黑树来拆分和重新映射 为链表，根据hash&amp;oldCap的值分成0、非0两组，映射到j和j+oldCap（0低位，非0高位）（链表顺序不变） HashMap概述 查找 根据hash定位槽 在槽中查找给定key（hash相等、key相等），找到直接返回，否则最后返回null 若槽节点key相等，返回槽节点 若槽节点为树节点，委托给树查找 遍历链表查找 遍历 从index = 0, table[index]开始，找到一个不为null的槽，遍历链表 插入 如果table为空，或长度为0，初始化。（默认loadFactor为0.75，默认capacity为16（capacity是table的长度），threshold一般为capacity*loadFactor。） 通过hash定位槽，如果槽为空，构造新节点赋值给槽 若槽不为空，则在槽的链表或树中找到key相同的节点，替换节点值为新值；或是没有key相同的节点，就在树中或链表尾部加入新节点；若链表加入新节点后长度达到8（槽不算，aka槽下原有7个节点），则进行红黑树转化 如果是新加入节点，modCount、元素个数size自增1，如果元素个数超过阈值，则进行扩容 扩容 计算新容量newCap和新阈值newThr（ps: 当容量已到最大值时，不再扩容；2倍扩容；） 创建新的数组，赋值给table 将键值对重新映射到新数组上 如果无链表，根据hash&amp;(newCap-1)定位 如果是树节点，委托红黑树来拆分和重新映射 为链表，根据hash&amp;oldCap的值分成0、非0两组，映射到j和j+oldCap（0低位，非0高位）（链表顺序不变） 删除 定位到槽 找到删除节点 删除节点，并修复链表或红黑树 链表树化 链表树化有两个条件，不满足采用扩容，满足再扩容 树化时，将Node节点替换为TreeNode，保留next信息 替换后，再从head开始，进行红黑树化（标记红黑节点、父子节点，如果root节点不是first节点，再修正next和prev？）【链表转成红黑树后，原链表的顺序仍然会被引用仍被保留了（红黑树的根节点会被移动到链表的第一位）】 在扩容过程中，树化要满足两个条件： 链表长度大于等于 TREEIFY_THRESHOLD 8 桶数组容量大于等于 MIN_TREEIFY_CAPACITY 64 红黑树拆分（扩容时候） 红黑树中保留了next引用，拆分原理和链表相似 根据hash拆分成两组（这时候会生成新的next关系） 各组内根据情况，链化或者重新红黑树化 红黑树链化 将TreeNode替换为Node ConcurrentHashMap概述相比较HashMap，主要是增加了写操作时候的同步处理。扩容迁移时，多个线程帮助迁移。 为什么要用synchronized代替ReentrantLock？ 优化后的synchronized性能与ReentrantLock差不多，基于JVM也保证synchronized在各平台上的使用一致。 锁粒度降低了；在大量数据操作下，基于api的ReentrantLock会有更大的内存开销。 sizeCtl 默认为0 当table为null时，持有一个initial table size用于初始化 当sizeCtl&lt;0时 -1表示正在初始化 非-1的负数 123（sizeCtl的低16位-1）表示有多少个线程参与扩容迁移 sizeCtl的高16位-(1 + the number of active resizing threads) sizeCtl&gt;0时，(n &lt;&lt; 1) - (n &gt;&gt;&gt; 1) = 0.75n （表示阈值，超过阈值需要扩容） 插入 计算hash 循环执行 如果数组为空，初始化initTable 如果hash定位到的槽为空，CAS替换为新节点，退出循环 如果槽不为空，节点hash为-1，说明正在迁移，helpTransfer 槽不为空，且不在迁移，那么，对头节点加锁，链表或红黑树形式插入或更新节点 addCount 迁移 transfer的第二个参数为空的时候，触发扩容，创建nextTable，在addCount和tryPresize中有这样的调用。 addCount是size不精确情况下，可能触发扩容；tryPresize是已知精确size的情况下做扩容。 计算步长stride 如果nextTab未创建，则创建之，并赋给nextTable 循环迁移 分配迁移区间i和bound（i从前往后，bound = i - stride + 1`，总之就是stride） 如果区间已达边界，将sc减1，表示本线程退出迁移。如果是最后一个迁移线程，标记finish和advance为true，进入下一循环recheck；非最后线程，直接退出方法。 如果finish为true，table = nextTab; sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1);，退出 若未达边界，且槽为空，CAS槽为fwd，进入下一循环 槽不为空，且槽已经是fwd，进入下一循环 最后一种情形，进行迁移 为链表，根据节点hash二进制第k位为0或1分成两组（n=2^k），1连接到高位槽上 为红黑树，分组同链表，分好的组根据节点个数判断是否链化或新生成红黑树 HashMap检测到hash冲突后，将元素插入在链表的末尾还是开头？Java8是加载链表末尾 Java7是开头 头插法会改变链表中元素原本的顺序，在并发情况下可能会产生链表成环的问题。 Java7到Java8的改变HashMap为何从头插入改为尾插入 java7的问题老生常谈，HashMap的死循环 1.8还采用了红黑树，讲讲红黑树的特性，为什么人家一定要用红黑树而不是AVL、B树之类的？插入、删除、查找的最坏时间复杂度都为 O(logn)。 红黑树特性： 每个节点要么是黑色，要么是红色 根节点是黑色的 每个叶节点是黑色的（Java实现中，叶子节点是null，遍历时看不到黑色的叶子节点，反而每个叶子节点是红色的） 如果一个节点是红色的，那么它的两个子节点是黑色的（意味着可以有连续的黑色节点，但不能有连续的红色节点。若给定N个黑色节点，最短路径情况是连续N个黑色，树高为N-1；最长路径情况是红黑相间，树高为2N-2） 对任一节点，从节点到它每个叶子节点的路径包含相同数量的黑色节点（最主要特性，插入、删除要调整以遵守这个规则） 面试旧敌之红黑树（直白介绍深入理解） 为什么用红黑树？ 红黑树的统计性能（理解为增删查平均性能）优于AVL树。 AVL：名字来源发明者G. M. Adelson-Velsky和E. M. Landis。本质是平衡二叉搜索树（查找树），任何节点的左右子树高度差不超过1，是高度平衡的二叉查找树。 B树：重温数据结构：理解 B 树、B+ 树特点及使用场景 平衡二叉树节点最多有两个子树，而 B 树每个节点可以有多个子树，M 阶 B 树表示该树每个节点最多有 M 个子树 AVL树高度平衡，查找效率高，但维护这个平衡的成本比较大，插入、删除要做的调整比较耗时。 红黑树的插入、删除、查找各种操作的性能比较平衡。 B树和B+树多用于数据存储在磁盘上的场景，比较矮胖，一次读取较多数据，减少IO。节点内是有序列表。列表的插入、删除成本比较高，如果是链表形式，则查找效率比较低（不能用二分查找提高查询效率）。 【自己的理解：B树节点内是有序列表，通过二分查找提高效率】 为什么STL和linux都使用红黑树作为平衡树的实现？ - Acjx的回答 - 知乎 https://www.zhihu.com/question/20545708/answer/58717264 谈谈Java容器ArrayList、LinkedList、HashMap、HashSet的理解，以及应用场景 ArrayList LinkedList HashMap HashSet 数据结构 （可变）数组 （双向）链表 数组+红黑树 底层实现是HashMap 插入时间复杂度 o(n) o(1) 删除时间复杂度 o(n) o(1) 访问时间复杂度 o(1) 支持随机访问 o(n) 不支持随机.. 应用场景 经常访问 经常修改 映射..？ 去重 sortset底层，原理，怎么保证有序TreeSet具体实现是TreeMap，底层是红黑树 containsKey、get、put、remove 时间复杂度log(n) 红黑树 通过对任何一条（根到叶子的）路径上的各个节点的着色方式的限制，确保没有一条路径会比其他路径长出2倍，因而近乎是平衡的 性质： 每个节点是红色的，或是黑色的 根节点是黑色的 每个叶子节点（Nil）是黑色的 如果一个节点是红色的，则它的两个子节点是黑色的 对每个节点，从该节点到其子孙节点的所有路径上包含相同个数的黑色节点。（红节点不能有红孩子）（从该节点出发的所有下降路径，有相同的黑节点个数） 黑高度：从一个节点到达一个叶子节点的任意一条路径上黑色节点的个数 红黑树的黑高度定义为根节点的黑高度 优先级队列的底层原理？堆，默认是小顶堆 入队 123456789101112131415161718192021222324public boolean offer(E e) &#123; if (e == null) throw new NullPointerException(); modCount++; int i = size; if (i &gt;= queue.length) grow(i + 1); siftUp(i, e); size = i + 1; return true;&#125;private static &lt;T&gt; void siftUpComparable(int k, T x, Object[] es) &#123; Comparable&lt;? super T&gt; key = (Comparable&lt;? super T&gt;) x; while (k &gt; 0) &#123; // 如果父节点比自己大 int parent = (k - 1) &gt;&gt;&gt; 1; Object e = es[parent]; if (key.compareTo((T) e) &gt;= 0) break; es[k] = e; k = parent; &#125; es[k] = key;&#125; 出队 1234567891011121314151617181920212223242526272829303132333435363738public E poll() &#123; final Object[] es; final E result; if ((result = (E) ((es = queue)[0])) != null) &#123; modCount++; final int n; final E x = (E) es[(n = --size)]; es[n] = null; if (n &gt; 0) &#123; final Comparator&lt;? super E&gt; cmp; if ((cmp = comparator) == null) siftDownComparable(0, x, es, n); else siftDownUsingComparator(0, x, es, n, cmp); &#125; &#125; return result;&#125;private static &lt;T&gt; void siftDownComparable(int k, T x, Object[] es, int n) &#123; // assert n &gt; 0; Comparable&lt;? super T&gt; key = (Comparable&lt;? super T&gt;)x; int half = n &gt;&gt;&gt; 1; // loop while a non-leaf while (k &lt; half) &#123; // 从孩子中选一个小的 int child = (k &lt;&lt; 1) + 1; // assume left child is least Object c = es[child]; int right = child + 1; if (right &lt; n &amp;&amp; ((Comparable&lt;? super T&gt;) c).compareTo((T) es[right]) &gt; 0) c = es[child = right]; if (key.compareTo((T) c) &lt;= 0) break; es[k] = c; k = child; &#125; es[k] = key;&#125; DelayQueuehttps://www.cnblogs.com/jobs/archive/2007/04/27/730255.html DelayQueue = BlockingQueue + PriorityQueue + Delayed Java并发线程池的工作原理，几个重要参数，然后给了具体几个参数分析线程池会怎么做，最后问阻塞队列的作用是什么？线程池解决两个问题： 由于减少了每个任务的调度开销，通常在执行大量异步任务时提供优秀的性能。 提供了管理、调控资源的方式 Executors工厂方法： newFixedThreadPool 固定size的线程池。为了满足资源管理的需求，需要限制当前线程数量的场景。适用于负载比较重的服务器。 corePoolSize == maximumPoolSize keepAliveTimes = 0 LinkedBlockingQueue 队列大小Integer.MAX_VALUE，等价于无界 当线程池中线程数达到corePoolSize后，新任务将在队列中等待 由于使用无界队列，运行中的线程池不会拒绝任务 newSingleThreadExecotor 单个线程的线程池。需要保证顺序执行任务的场景，并且在任意时间点不会有多个线程是活动的。 corePoolSize = maximumPoolSize = 1 keepAliveTimes = 0 LinkedBlockingQueue 如果当前线程池无线程，就创建一个线程来运行任务 当线程数达到1后，新的任务都加入到队列中 newCachedThreadPool 大小无界的线程池（自动资源回收？），适用于有很多短期异步执行任务的小程序，或者是负载比较轻的服务器。 corePoolSize = 0, maximumPoolSize = Integer.MAX_VALUE keepAliveTimes = 60s SynchronousQueue 是一个没有容量的阻塞队列，一个插入操作必须等待另一个线程对应的移除操作 提交任务时如果有空闲线程，就空闲线程取到这个任务执行；否则创建一个线程来执行任务 适用于将主线程的任务传递给空闲线程执行 重要参数： core and maximum pool sizes corePoolSize 核心最大线程：新任务加入时，如果运行线程个数小于核心线程数，即使有其他工作线程是空闲的，也会创建新线程 – 线程池预热 maximumPoolSize 线程池最大线程：阻塞队列满时，如果运行线程数小于maximumPoolSize，才可创建新线程运行任务 corePoolSize=maximumPoolSize时，等价于newFixedThreadPool maximumPoolSize=本质上无限的数（比如Integer.MAX_VALUE），等价于newCachedThreadPool ？ 一般只在构造时设置这两个参数，但也可以通过两个set方法改变 这两个参数会自动调整么？ On-demand construction 默认情况下，只有任务提交时才会创建线程（包括核心线程） 也可以通过prestartCoreThread或者prestartAllCoreTheads来预先创建线程。比如构建了一个阻塞队列不为空的线程池时，会想要这么做（预先创建线程）。 Creating new threads 默认使用defaultThreadFactory来创建线程，相同的线程组ThreadGroup、优先级priority和非守护线程状态non-daemon status. 也可以使用自定义的threadFactory，自定义线程名称、线程组、优先级等。 threadFactory创建线程失败的什么东西没看懂 Keep-alive times keepAliveTime 如果线程数多于核心线程数，超过这个时间的空闲线程将会被停掉（指销毁掉？） queuing 入队规则 rejected tasks 四个拒绝策略 RejectedExecutionHandler ThreadPoolExecutor.AbortPolicy 抛出RejectedExecutionException CallerRunsPolicy 调用者自身来执行 DiscardPolicy 丢弃任务，任务不会被执行 DiscardOldestPolicy work queue的首个任务将会被丢弃，重试添加当前任务（可能再次失败，自旋执行） hook methods beforeExecute afterExecute 可用来设置运行环境，重新初始化本地线程，获取统计数据，添加日志。 terminated executor终止时提供的钩子方法 queue maintenance getQueue可用于监控和调试当前work queue，其他用途不建议。remove和purge可用于大量任务取消时候的存储清理。 reclamation （清除？）一个在程序中无引用、并且无剩余线程的线程池，即使无显式shutdown关闭，也可以被清除回收。可以通过这些方式设置线程池的线程在无使用时（最终）销毁：设置keep-alivet times；使用小的核心线程数比如0，或者设置allowCoreThreadTimeOut。 ScheduledThreadPoolExe7Zcutor 延迟运行命令，或周期执行命令 LinkedBlockingQueue和DelayQueue的实现原理 LinkedBlockingQueue 就是生产者消费者的实现 应用了ReentrantLock（putLock &amp; tackLock）和lock的Condition（notEmpty &amp; notFull） DelayQueue 应用了PriorityQueue，时间小的在队头 ReentrantLock（lock）和Condition（available） FutureTask是用AQS实现的 get=acquireShared，run/cancel后=release ThreadPoolExecutor-&gt;AbstractExecutorService-&gt;ExecutorService-&gt;Executor Runnable、Callable、Future、FutureTask的关系 Runnable表明它的一个实现要在Thread线程上运行，没有返回值 Callable在Runnable基础上，有返回值 Future表示一个异步计算的结果，Callable返回给调用方的句柄，用于对异步计算结果的查询、取消、获取执行结果 前三个是接口，FutureTask是Future的具体实现 https://juejin.im/post/6844904033673560077 Future是一个句柄，即Callable任务返回给调用方这么一个句柄，通过这个句柄我们可以跟这个异步任务联系起来，我们可以通过future来对任务查询、取消、执行结果的获取，是调用方与异步执行方之间沟通的桥梁 谈谈Java线程的基本状态，其中的wait() sleep() yield()方法的区别。线程的基本状态 新建、运行（运行中、就绪）、等待、超时等待、阻塞、终止 wait() Object的方法，在某个对象上等待，等待这个对象将它唤醒，释放锁。运行-&gt;等待/超时等待 sleep() Thread的静态方法，当前线程睡眠，不释放锁。运行-&gt;超时等待 yield() Thread的方法，让出当前cpu。还是运行这个大状态，从运行中变成就绪状态。不释放锁。 简单谈谈JVM内存模型，以及volatile关键字运行时数据区域包括堆、方法区（包括运行时常量池）、Java虚拟机栈、本地方法栈、程序计数器、直接内存。 堆：所有对象在这里分配内存【所有线程共享】 方法区：存放已被加载的类信息、常量、静态变量、即时编译器编译后的代码等信息【所有线程共享】 Java虚拟机栈：生命周期与线程相同，描述的是Java方法执行时候的内存模型，每个方法被执行的时候都会创建一个栈帧，存储局部变量表、操作数栈、常量池引用（动态链接）、方法出口等信息。【线程私有】 本地方法栈：与虚拟机栈类似，只不过方法是本地方法【线程私有】 程序计数器：记录正在执行的虚拟机字节码指令的地址（如果是本地方法则为空）【线程私有】 直接内存：JDK1.4引入NIO，可以使用native函数库分配堆外内存，然后通过堆内的DirectByteBuffer作为这部分内存的引用、进行操作。可以提高性能，避免堆外内存和堆内内存的来回拷贝。 Java内存模型 JMM Java memory model 用来屏蔽不同硬件和操作系统的内存访问差异，实现Java在各平台上一致的内存访问效果。 JMM规定，所有变量都存在主内存中（类似于操作系统的普通内存）；每个线程有自己的工作内存（=CPU的寄存器或高速缓存），保存了该线程使用的变量的主内存副本拷贝。线程只能操作工作内存。 存在缓存不一致问题。 主内存与工作内存交互操作 内存模型三大特性 原子性：上述8个操作是原子的（double&amp;long等64位变量的操作，JVM允许非原子），一系列操作合起来其实是非原子的 可见性：指一个线程修改了共享变量的值，其他线程可以立即得知这个修改。JMM是通过变量修改后将新值同步到主内存（并使其他工作内存中的这个变量副本无效）、在变量读取前从主内存刷新变量值来实现的。 有序性：从本线程来看，所有操作都是有序的；从线程外看，操作是无序的，因为发生了指令重排。JMM允许编译器和处理器进程指令重排，重排不会影响到单线程的执行结果，但会影响多线程的执行正确性。 volatile关键字通过添加内存屏障的方式来禁止指令重排（重排序时不能把屏障后的指令重排到屏障前） 先行发生原则 单一线程原则：在一个线程内，在程序前面的操作先行发生于后面的操作。 管程锁定原则：一个 unlock 操作先行发生于后面对同一个锁的 lock 操作。 volatile变量规则：对一个 volatile 变量的写操作先行发生于后面对这个变量的读操作。 线程启动规则：Thread 对象的 start() 方法调用先行发生于此线程的每一个动作。 线程加入规则：Thread 对象的结束先行发生于 join() 方法返回。 线程中断规则：对线程 interrupt() 方法的调用先行发生于被中断线程的代码检测到中断事件的发生。 对象终结规则：一个对象的初始化完成(构造函数执行结束)先行发生于它的 finalize() 方法的开始。 传递性：如果操作 A 先行发生于操作 B，操作 B 先行发生于操作 C，那么操作 A 先行发生于操作 C。 volatile关键字 volatile关键字 保证了不同线程对该变量操作的内存可见性 禁止指令重排序，保证（volatile读写）有序性 volatile的底层如何实现，怎么就能保住可见性了？具体在👆 在缓存行和主内存之间，利用的是缓存一致性协议。 在写入缓存和缓存行之间，利用的是内存屏障。 从规范上讲是内存屏障，x86实现上是lock前缀指令，既有原子性的效果，也有StoreLoad内存屏障的效果。 内存屏障的保守插入方式，为了使写操作一定刷新到缓存行，（因为缓存一致性和禁止重排序）读操作一定读到最新值： 在每个volatile读后面，插入LoadLoad和LoadStore 在每个volatile写前面插入StoreStore，写后面插入StoreLoad 123456789101112131415161718192021public class VolatileExample &#123; int a = 0; volatile boolean flag = false; public void writer() &#123; a = 1; // StoreStore 确保a的值已写入 flag = true; // StoreLoad 确保flag的值在后面的Load之前已写入 &#125; public void reader() &#123; if (flag) &#123; // volatile读后 后面的写和读不能重排序到读flag前，确保是基于最新flag值做操作 // LoadStore // LoadLoad int i = a; System.out.println(i); &#125; &#125;&#125; 线程之间的交互方式有哪些？有没有线程交互的封装类？ 线程之间的协作 join() 在线程中调用另一个线程的join()方法，会将本线程挂起，直到目标线程结束 wait() notify() notifyAll() 调用 wait() 使得线程等待某个条件满足，线程在等待时会被挂起，当其他线程的运行使得这个条件满足时，其它线程会调用 notify() 或者 notifyAll() 来唤醒挂起的线程。 属于Object（不是Thread） await() signal() signalAll() java.util.concurrent 类库中提供了 Condition 类来实现线程之间的协调，可以在 Condition 上调用 await() 方法使线程等待，其它线程调用Condition的 signal() 或 signalAll() 方法唤醒等待的线程。 线程交互的封装类 CountDownLatch 用来控制一个线程等待多个线程。 维护了一个计数器 cnt，每次调用 countDown() 方法会让计数器的值减 1，减到 0 的时候，那些因为调用 await() 方 法而在等待的线程就会被唤醒。 12345678910111213141516public class CountdownLatchExample &#123; public static void main(String[] args) throws InterruptedException &#123; final int totalThread = 10; CountDownLatch countDownLatch = new CountDownLatch(totalThread); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; totalThread; i++) &#123; executorService.execute(() -&gt; &#123; System.out.print(\"run..\"); countDownLatch.countDown();&#125;); &#125; countDownLatch.await(); System.out.println(\"end\"); executorService.shutdown();&#125; &#125;run..run..run..run..run..run..run..run..run..run..end 等待所有run结束 CyclicBarrier 用来控制多个线程互相等待，只有当多个线程都到达时，这些线程才会继续执行。 和 CountdownLatch 相似，都是通过维护计数器来实现的。线程执行 await() 方法之后计数器会减 1，并进行等待，直到计数器为 0，所有调用 await() 方法而在等待的线程才能继续执行。 CyclicBarrier 和 CountdownLatch 的一个区别是，CyclicBarrier 的计数器通过调用 reset() 方法可以循环使用，所以它才叫做循环屏障。 123456789101112131415161718192021public class CyclicBarrierExample &#123; public static void main(String[] args) &#123; final int totalThread = 10; CyclicBarrier cyclicBarrier = new CyclicBarrier(totalThread); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; totalThread; i++) &#123; executorService.execute(() -&gt; &#123; System.out.print(\"before..\"); try &#123; cyclicBarrier.await(); &#125; catch (InterruptedException | BrokenBarrierException e) &#123; e.printStackTrace(); &#125; System.out.print(\"after..\"); &#125;);&#125; executorService.shutdown(); &#125;&#125;before..before..before..before..before..before..before..before..before..before..after..after..after..after..after..after..after..after..after..after.. 等待所有before结束 Semaphore Semaphore 类似于操作系统中的信号量，可以控制对互斥资源的访问线程数。 123456789101112131415161718192021public class SemaphoreExample &#123; public static void main(String[] args) &#123; final int clientCount = 3; final int totalRequestCount = 10; Semaphore semaphore = new Semaphore(clientCount); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; totalRequestCount; i++) &#123; executorService.execute(()-&gt;&#123; try &#123; semaphore.acquire(); System.out.print(semaphore.availablePermits() + \" \"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; semaphore.release(); &#125;&#125;); &#125; executorService.shutdown(); &#125;&#125;1 0 1 1 1 2 2 2 0 1 有限个资源 Java的锁机制 – 内容巨多Java锁机制 AQS机制 背景知识 指令流水线：现代处理器的体系结构中，采用流水线的方式对指令进行处理。每个指令的工作可分为5个阶段：取指令、指令译码、执行指令、访存取数和结果写回。 CPU多级缓存：计算机系统中，存在CPU高速缓存，用于减少处理器访问内存所需平均时间。当处理器发出内存访问请求时，会先查看缓存中是否有请求数据，若命中则直接返回该数据；若不存在，则先从内存中将数据载入缓存，再将其返回处理器。 问题引入 原子性：即一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。（比如i++，如果对实例变量i的操作不做额外的控制，那么多个线程同时调用，就会出现覆盖现象，丢失部分更新。） – 因为指令流水线 可见性：是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值（存在可见性问题的根本原因是由于缓存的存在）– 因为存在缓存 顺序性：即程序执行的顺序按照代码的先后顺序执行 – 因为存在指令重排 JMM内存模型 主要目标是定义程序中各个变量的访问规则，即在虚拟机中将变量存储到内存和从内存中取出变量这样的底层细节。这里的变量指共享变量（存在竞争问题的变量），如实例字段、静态字段、数组对象元素等。不包括线程私有的局部变量、方法参数等。 内存划分：分为主内存和工作内存，【每个线程都有自己的工作内存，它们共享主内存。】【线程对共享变量的所有读写操作都在自己的工作内存中进行，不能直接读写主内存中的变量。】【不同线程间也无法直接访问对方工作内存中的变量，线程间变量值的传递必须通过主内存完成。】 主内存（Main Memory）存储所有共享变量的值。 工作内存（Working Memory）存储该线程使用到的共享变量在主内存的的值的副本拷贝。 内存间交互规则【一个变量如何从主内存拷贝到工作内存，如何从工作内存同步到主内存中】 8种原子操作 lock: 将一个变量标识为被一个线程独占状态 unclock: 将一个变量从独占状态释放出来，释放后的变量才可以被其他线程锁定 read: 将一个变量的值从主内存传输到工作内存中，以便随后的load操作 load: 把read操作从主内存中得到的变量值放入工作内存的变量的副本中 use: 把工作内存中的一个变量的值传给执行引擎，每当虚拟机遇到一个使用到变量的指令时都会使用该指令 assign: 把一个从执行引擎接收到的值赋给工作内存中的变量，每当虚拟机遇到一个给变量赋值的指令时，都要使用该操作 store: 把工作内存中的一个变量的值传递给主内存，以便随后的write操作 write: 把store操作从工作内存中得到的变量的值写到主内存中的变量 原子操作的使用规则 read、load、use必须成对顺序出现，但不要求连续出现。assign、store、write同之； 变量诞生和初始化：变量只能从主内存“诞生”，且须先初始化后才能使用，即在use/store前须先load/assign； lock一个变量后会清空工作内存中该变量的值，使用前须先初始化；unlock前须将变量同步回主内存； 一个变量同一时刻只能被一线程lock，lock几次就须unlock几次；未被lock的变量不允许被执行unlock，一个线程不能去unlock其他线程lock的变量。 对于double和long，虽然内存模型允许对非volatile修饰的64位数据的读写操作分为两次32位操作来进行，但商用虚拟机几乎把64位数据的读写实现为了原子操作，可以忽略这个问题。 先行发生原则 【Java内存模型具备一些先天的“有序性”，即不需要通过任何同步手段（volatile、synchronized等）就能够得到保证的有序性，这个通常也称为happens-before原则。】 如果两个操作的执行次序不符合先行原则且无法从happens-before原则推导出来，那么它们就不能保证它们的有序性，虚拟机可以随意地对它们进行重排序。 程序次序规则（Program Order Rule）：一个线程内，逻辑上书写在前面的操作先行发生于书写在后面的操作。 监视器锁规则（Monitor Lock Rule）：一个unLock操作先行发生于后面对同一个锁的lock操作。“后面”指时间上的先后顺序。 volatile变量规则（Volatile Variable Rule）：对一个volatile变量的写操作先行发生于后面对这个变量的读操作。“后面”指时间上的先后顺序。 传递规则（Transitivity）：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C。 线程启动规则（Thread Start Rule）：Thread对象的start()方法先行发生于此线程的每个一个动作。 线程中断规则（Thread Interruption Rule）：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生（通过Thread.interrupted()检测）。 线程终止规则（Thread Termination Rule）：线程中所有的操作都先行发生于线程的终止检测，我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值手段检测到线程已经终止执行。 对象终结规则（Finaizer Rule）：一个对象的初始化完成（构造函数执行结束）先行发生于他的finalize()方法的开始。 问题解决 原子性 由JMM保证的原子性变量操作 基本数据类型的读写（工作内存）是原子的 JMM的lock和unlock指令可以实现更大范围的原子性保证，虚拟机提供synchronized关键字和Lock锁来保证原子性。 可见性 volatile关键字修饰的变量，被线程修改后会立即同步回主内存，其他线程要读取这个变量会从主内存刷新值到工作内存。（因为缓存一致性协议会让其他工作内存中的该变量拷贝无效，必须得从主内存再读取）即read、load、use三者连续顺序执行，assign、store、write连续顺序执行。 synchronized/Lock 由lock和unlock的使用规则保证【这里有疑问啊，synchronized有lock和unlock，但是Lock没有吧…Lock怎么保证可见性？还是说Lock保证不了可见性。可见性只能由volatile保证？–参见ConcurrentHashMap，有synchronized，还配合volatile使用—ConcurrentHashMap有些是不加锁的操作，比如get，所以还是用volatile保证可见性。synchronized 锁的是某个node节点，对这个node节点的】 synchronized有语义规定，说是通过内存屏障实现的 线程解锁前，必须把共享变量的最新值刷新到主内存中线程加锁前，将清空工作内存中共享变量的值，从而使用共享变量时需要从主内存中重新读取最新的值 Lock用了cas，有lock cmpxchg，lock前缀指令保证了可见性，同时有内存屏障的作用 同时，这俩还能保证临界区操作的所有变量的可见性因为内存屏障 LOCK前缀的指令具有如下效果： 把写缓冲区中所有的数据刷新到内存中 注意，是所有的数据，可不仅仅是对state的修改 ReentrantLock对可见性的支持 All threads will see the most recent write to a volatile field, along with any writes which preceded that volatile read/write. Reentrantlock的lock和unlock方法实际上会cas一个state的变量，state是volatile的，因此夹在两次state之间的操作都能保证可见性。这应该算是happen before的传递性… 顺序性 volatile 禁止指令重排序 synchronized/Lock “一个变量在同一个时刻只允许一条线程对其执行lock操作” – 感觉这个也没用，不然双重检查的单例怎么还用volatile关键字来防止重排序 – 最多保证原子性，被加锁内容按照顺序被多个线程执行 锁机制 volatile： 保证可见性和顺序性【实现方式：lock前缀指令+依赖MESI缓存一致性协议】 volatile修饰的变量，在进行写操作的时候会多一行汇编代码，lock指令，做两件事： 将当前处理器缓存行的数据写回系统内存 引起其他处理器里缓存了该内存地址的数据无效。【实现缓存一致性协议，处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了（处理器发现自己缓存行对应的内存地址被修改，就会将自己的缓存设置成无效状态）】 final：有两个重排序规则 – 不甚了解 写final域的重排序规则：在构造函数内对一个final域的写入，与随后把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序。 读final域的重排序规则：初次读一个包含final域的对象的引用，与随后初次读这个final域，这两个操作之间不能重排序。 synchronized关键字 使用哪个对象的监视器： 修饰对象方法时，使用当前对象的监视器 修饰静态方法时，使用类类型（Class 的对象）监视器 修饰代码块时，使用括号中的对象的监视器 必须为 Object 类或其子类的对象 无锁 -&gt; 偏向锁 -&gt; 轻量级锁 -&gt; 重量级锁 简单理解，只有一个线程CAS时，如果CAS成功，表示没有锁竞争，保持偏向锁状态，如果CAS失败，说明有竞争，（先撤销偏向锁，将对象头设置成无锁状态，并设置为不可偏向）升级为轻量级锁。 几种锁的适用场景 偏向锁：锁不仅不存在线程竞争，而且总是由同一个线程多次获得，这时候偏向锁的代价最低。适用只有一个线程访问同步块的场景。（如果有别的线程来获取锁，发现） 轻量级锁：同步块执行时间非常快的，执行完就替换回mark word，别的线程要加锁也很快，CAS。（如果同步块执行很久，竞争线程自旋cas非常久，就很耗cpu，所以会升级到重量级锁，竞争线程阻塞挂起） 重量级锁：同步块执行时间比较长的，原因如2 锁升级机制 1. 偏向锁：线程检查锁对象的状态是否是可偏向的，是的话，检查mark word中的线程ID是不是自己，是的话进入代码块，不是的话，将线程ID cas进mark word。cas失败的话，说明之前是别的线程（假设A）取到的了，等待全局安全点，JVM暂停线程A，检查线程A的状态：如果A不在活动中，将锁对象的mark word中的线程ID置空，再cas成自己的线程ID；如果A在活动中（未退出代码块），升级为轻量级锁：JVM在线程A中分配锁记录，拷贝锁对象mark word，并将锁对象mark word指向这个锁记录；在线程B中分配锁记录，拷贝锁对象mark word，并持续自旋cas（如果自旋n次还失败，就要再次升级成重量级锁了..）... 2. 轻量级锁：如果不止一个线程尝试获取锁，就会升级到轻量级锁。**通过自适应自旋CAS的方式获取锁。**如果获取失败，说明存在竞争，膨胀为重量级锁，线程阻塞。默认自旋10次。**将对象头中的Mark Word复制到栈帧（一块空间，称为锁记录）中，然后用CAS将对象头中的Mark Word替换为指向栈帧中锁记录的指针。** 3. 重量级锁：通过系统的线程互斥锁来实现的，未获取到锁的线程会阻塞挂起 大佬的图，来源见水印 右下角的轻量级锁释放的补充说明： 在某个线程A正持有轻量级锁的时候（还在代码块内运行，时间比较长），某个线程B自旋cas竞争锁（肯定是cas失败了）失败了，这时候就会升级成重量级锁了，mark word指向了互斥量的指针，这和线程A中锁记录的值不同，线程A后续释放锁就失败了（意识到已经升级成重量级锁，唤醒其他挂起的线程） ![img](../../../image/172a2f26935d33c8.png) AQS 【内存屏障和”lock”前缀指令】理解volatile通过编译器，既会增加”lock”前缀指令，也会加上内存屏障（mfence等） 内存屏障是抽象概念，各个硬件、处理器实现不同 lock前缀指令和mfence等是具体实现 mesi协议保证缓存和主存间的一致性 有了msei协议，为什么汇编层面还需要lock(volatile)来实现可见性？ - Rob Zhang的回答 - 知乎 https://www.zhihu.com/question/334662600/answer/747038084 内存屏障能保证从storebuffer到缓存再到主存的一致性，在多线程运行中可以作为mesi的补充（因为mesi管不到那么多），但内存屏障 lock前缀主要是为了提供原子操作，虽然它也包含了内存屏障功能（强制将寄存器、缓存（、storebuffer/invalid queue或类似的东西）等强制同步到主存） 关于内存屏障的几个问题？ - cao的回答 - 知乎 https://www.zhihu.com/question/47990356/answer/108650501 x86在Windows下的内存屏障是用lock前缀指令来达到效果的 简单理解： 内存屏障保证了寄存器和缓存之间的一致性 lock前缀保证操作原子性 二者都能保证可见性 x86架构的内存屏障 sfence: Store Barrier = StoreStore Barriers 写屏障 所有sfence之前的store指令都在sfence之前被执行，并刷出到CPU的L1 Cache中； 所有在sfence之后的store指令都在sfence之后执行，禁止重排序到sfence之前。 所以，所有Store Barrier之前发生的内存更新都是可见的。 lfence: Load Barrier = LoadLoad Barriers 读屏障 所有在lfence之后的load指令，都在lfence之后执行，并且一直等到load buffer被该CPU读完才能执行之后的load指令（即要刷新失效的缓存）。配合sfence，使所有sfence之前发生的内存更新，对lfence之后的load操作都可见。 mfence: Full Barrier = StoreLoad Barriers 全屏障 综合了sfence和lfence的作用，强制所有在mfence之前的store/load指令都在mfence之前被执行，之后的store/load指令都在之后执行，禁止跨越mfence重排序。并且都刷新到缓存&amp;重新载入无效缓存。 Mark Word 对象头【见JMM】 todo主要有锁标志位，根据不同的锁状态其他位上存有不同的值，比如 偏向锁：拥有锁的线程ID，偏向状态 轻量级锁：拥有锁的锁记录地址 重量级锁：监视器锁的地址 synchronized底层实现加在方法上和加在同步代码块中编译后的区别、类锁、对象锁 编译时候加入监视器锁 1234567891011public class SyncTest &#123; public void syncBlock() &#123; synchronized (this) &#123; System.out.println(\"hello block\"); &#125; &#125; public synchronized void syncMethod() &#123; System.out.println(\"hello method\"); &#125;&#125; 加在方法上：方法上有synchronized关键字，flags里有ACC_SYNCHRONIZED https://blog.csdn.net/hosaos/java/article/details/100990954 ACC_SYNCHRONIZED是获取监视器锁的一种隐式实现(没有显示的调用monitorenter，monitorexit指令) 如果字节码方法区中的ACC_SYNCHRONIZED标志被设置，那么线程在执行方法前会先去获取对象的monitor对象，如果获取成功则执行方法代码，执行完毕后释放monitor对象 123456789101112131415public synchronized void syncMethod(); descriptor: ()V flags: ACC_PUBLIC, ACC_SYNCHRONIZED Code: stack=2, locals=1, args_size=1 0: getstatic #2 // Field java/lang/System.out:Ljava/io/PrintStream; 3: ldc #5 // String hello method 5: invokevirtual #4 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 8: return LineNumberTable: line 15: 0 line 16: 8 LocalVariableTable: Start Length Slot Name Signature 0 9 0 this Lwyq/learning/quickstart/juc/SyncTest; 加在同步块上：monitorenter / monitorexit 关键字 12345678910111213141516171819202122232425262728293031323334353637383940public void syncBlock(); descriptor: ()V flags: ACC_PUBLIC Code: stack=2, locals=3, args_size=1 0: aload_0 1: dup 2: astore_1 3: monitorenter 4: getstatic #2 // Field java/lang/System.out:Ljava/io/PrintStream; 7: ldc #3 // String hello block 9: invokevirtual #4 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 12: aload_1 13: monitorexit 14: goto 22 17: astore_2 18: aload_1 19: monitorexit 20: aload_2 21: athrow 22: return Exception table: from to target type 4 14 17 any 17 20 17 any LineNumberTable: line 9: 0 line 10: 4 line 11: 12 line 12: 22 LocalVariableTable: Start Length Slot Name Signature 0 23 0 this Lwyq/learning/quickstart/juc/SyncTest; StackMapTable: number_of_entries = 2 frame_type = 255 /* full_frame */ offset_delta = 17 locals = [ class wyq/learning/quickstart/juc/SyncTest, class java/lang/Object ] stack = [ class java/lang/Throwable ] frame_type = 250 /* chop */ offset_delta = 4 volatile在编译上的体现1234567public class VolatileTest &#123; private volatile int i; public void plus() &#123; i = 2; &#125;&#125; 字节码网上查到的是变量上flags有ACC_VOLATILE标识，自己编译出来没看到… 123456789101112131415public void plus(); descriptor: ()V flags: ACC_PUBLIC Code: stack=2, locals=1, args_size=1 0: aload_0 1: iconst_2 2: putfield #2 // Field i:I 5: return LineNumberTable: line 11: 0 line 12: 5 LocalVariableTable: Start Length Slot Name Signature 0 6 0 this Lwyq/learning/quickstart/juc/VolatileTest; 看文章说还是lock前缀指令 http://gee.cs.oswego.edu/dl/jmm/cookbook.html – x86架构下，实现是lock前缀指令，支持”SSE2”扩展 (Pentium4 and later)的版本支持mfence指令（比lock前缀更推荐），cas的cmpxchg的实现需要lock前缀 ​ https://www.cnblogs.com/xrq730/p/7048693.html 锁总线，其它CPU对内存的读写请求都会被阻塞，直到锁释放，不过实际后来的处理器都采用锁缓存替代锁总线，因为锁总线的开销比较大，锁总线期间其他CPU没法访问内存 lock后的写操作会回写已修改的数据，同时让其它CPU相关缓存行失效，从而重新从主存中加载最新的数据 不是内存屏障却能完成类似内存屏障的功能，阻止屏障两边的指令重排序 整理一下最终的实现： lock前缀指令会引起处理器缓存回写到内存 一个处理器的缓存回写到内存会导致其他处理器的缓存无效，这是MESI实现的（缓存一致性协议） 另外，lock前缀指令能完成内存屏障的功能，阻止屏障前后的指令重排序 ​ 这篇文章https://juejin.im/post/5ea938426fb9a043856f2f6a提到，x86下使用`lock`来实现`StoreLoad`，并且只有 StoreLoad 有效果。x86 上怎么使用 Barrier 的说明可以在 openjdk 的代码中看到，在这里src/hotspot/cpu/x86/assembler_x86.hpp。 ​ 3种重排序类型1是编译器重排序，2和3是处理器重排序。会导致多线程程序出现内存可见性问题。 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-LevelParallelism，ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。 aqs，countDownLatch如何实现 todo计算密集型/IO密集型 任务 分别如何设置线程池的核心线程数和最大线程数，为什么这么设置https://blog.csdn.net/weixin_40151613/java/article/details/81835974 计算密集型： CPU使用率比较高，（也就是一些复杂运算，逻辑处理） 线程数设置为CPU核数 IO密集型： cpu使用率较低，程序中会存在大量I/O操作占据时间，导致线程空余出来 一般设置线程数为CPU核数的2倍 最佳线程数目 = （（线程等待时间+线程CPU时间）/线程CPU时间 ）* CPU数目 线程等待时间越长，需要越多的线程 补充 高并发、任务执行时间短的业务：线程池线程数可以设置为CPU核数+1，减少线程上下文的切换 并发不高、任务执行时间长的业务： 假如是业务时间长集中在IO操作上，也就是IO密集型的任务，因为IO操作并不占用CPU，所以不要让所有的CPU闲下来，可以适当加大线程池中的线程数目，让CPU处理更多的业务 假如是业务时间长集中在计算操作上，也就是计算密集型任务，和（1）一样，线程池中的线程数设置得少一些，减少线程上下文的切换 并发高、业务执行时间长，解决这种类型任务的关键不在于线程池而在于整体架构的设计 数据能否做缓存 增加服务器 业务执行时间长的问题，也可能需要分析一下，看看能不能使用中间件（任务时间过长的可以考虑拆分逻辑放入队列等操作）对任务进行拆分和解耦。 死锁死锁定义：多个进程循环等待它方占有的资源而无限期地僵持下去的局面。 产生死锁的必要条件： 互斥（mutualexclusion），一个资源每次只能被一个进程使用 不可抢占（nopreemption），进程已获得的资源，在未使用完之前，不能强行剥夺 占有并等待（hold andwait），一个进程因请求资源而阻塞时，对已获得的资源保持不放 环形等待（circularwait），若干进程之间形成一种首尾相接的循环等待资源关系。 对待死锁的策略主要有： 死锁预防：破坏导致死锁必要条件中的任意一个就可以预防死锁。例如，要求用户申请资源时一次性申请所需要的全部资源，这就破坏了保持和等待条件；将资源分层，得到上一层资源后，才能够申请下一层资源，它破坏了环路等待条件。预防通常会降低系统的效率。 死锁避免：避免是指进程在每次申请资源时判断这些操作是否安全，例如，使用银行家算法。死锁避免算法的执行会增加系统的开销。 死锁检测：死锁预防和避免都是事前措施，而死锁的检测则是判断系统是否处于死锁状态，如果是，则执行死锁解除策略。 死锁解除：这是与死锁检测结合使用的，它使用的方式就是剥夺。即将某进程所拥有的资源强行收回，分配给其他的进程。 避免死锁的几个常见方法 避免一个线程同时获取多个锁 避免一个线程在锁内同时占用多个资源，尽量保证每个锁只占用一个资源。 尝试使用定时锁，使用lock.tryLock(timeout)来代替使用内部锁机制。 对于数据库锁，加锁和解锁必须在一个数据库连接里，否则会出现解锁失败的情况。 Java线程池的设计和实现https://www.javadoop.com/post/java-thread-pool Executor &lt;- ExecutorService &lt;- AbstractExecutorService &lt;- ThreadPoolExecutor Future + Runnable &lt;- RunnableFuture &lt;- FutureTask 需要获取结果（FutureTask），用 submit 方法，不需要获取结果，可以用 execute 方法。 核心参数： corePoolSize 核心线程数 maximumPoolSize 最大线程数 keepAliveTime 空闲线程的保活时间 workQueue 任务队列 BlockingQueue接口的某个实现 threadFactory 线程工厂 handler 拒绝策略 用一个32位的整数来存放线程池的状态（前3位），和线程池中的线程数（后29位） java 线程池有哪些关键属性？ corePoolSize，maximumPoolSize，workQueue，keepAliveTime，rejectedExecutionHandler corePoolSize 到 maximumPoolSize 之间的线程会被回收，当然 corePoolSize 的线程也可以通过设置而得到回收（allowCoreThreadTimeOut(true)）。 workQueue 用于存放任务，添加任务的时候，如果当前线程数超过了 corePoolSize，那么往该队列中插入任务，线程池中的线程会负责到队列中拉取任务。 keepAliveTime 用于设置空闲时间，如果线程数超出了 corePoolSize，并且有些线程的空闲时间超过了这个值，会执行关闭这些线程的操作 rejectedExecutionHandler 用于处理当线程池不能执行此任务时的情况，默认有抛出 RejectedExecutionException 异常、忽略任务、使用提交任务的线程来执行此任务和将队列中等待最久的任务删除，然后提交此任务这四种策略，默认为抛出异常。 说说线程池中的线程创建时机？ 如果当前线程数少于 corePoolSize，那么提交任务的时候创建一个新的线程，并由这个线程执行这个任务； 如果当前线程数已经达到 corePoolSize，那么将提交的任务添加到队列中，等待线程池中的线程去队列中取任务； 如果队列已满，那么创建新的线程来执行任务，需要保证池中的线程数不会超过 maximumPoolSize，如果此时线程数超过了 maximumPoolSize，那么执行拒绝策略。 Executors工具类创建的几种线程池的特征 固定大小的线程池 newFixedThreadPoolSize 最大线程数与核心线程数相等，keepAliveTime设置为0（因为这里它是没用的，即使不为 0，线程池默认也不会回收 corePoolSize 内的线程），任务队列为LinkedBlockingQueue，无界队列。 过程分析：刚开始，每提交一个任务都创建一个 worker，当 worker 的数量达到 nThreads 后，不再创建新的线程，而是把任务提交到 LinkedBlockingQueue 中，而且之后线程数始终为 nThreads。 只有一个线程的线程池 newSingleThreadPool 和上一个类似，线程数设置为1 有需要就创建新线程，可复用之前创建的线程 newCachedThreadPool 核心线程数为0，最大线程数为Integer.MAX_VALUE，keepAliveTime为60秒，任务队列为SynchronousQueue，相当于是没有容量的队列。 【当一个线程往队列中写入一个元素时，写入操作不会立即返回，需要等待另一个线程来将这个元素拿走；同理，当一个读线程做读操作的时候，同样需要一个相匹配的写线程的写操作。数据必须从某个写线程交给某个读线程，而不是写到某个队列中等待被消费。不提供任何空间（一个都没有）来存储元素。】 合理配置线程池 CPU密集型：尽可能小的线程数，如N+1个线程 IO密集型：尽可能多，比如2N个线程 N指CPU核数 ReentrantLock Condition在使用 condition 时，必须先持有相应的锁。 volatile作用：内存可见性和禁止指令重排序。 volatile 有synchronized类似的语义，读一个 volatile 变量之前，需要先使相应的本地缓存失效，这样就必须到主内存读取最新值，写一个 volatile 属性会立即刷入到主内存。所以，volatile 读和 monitorenter 有相同的语义，volatile 写和 monitorexit 有相同的语义。 volatile 的禁止重排序并不局限于两个 volatile 的属性操作不能重排序，而且是 volatile 属性操作和它周围的普通属性的操作也不能重排序。 volatile 属性的读写操作都是无锁的，它不能替代 synchronized，因为它没有提供原子性和互斥性。因为无锁，不需要花费时间在获取锁和释放锁上，所以说它是低成本的。—— 【原子性是指一个写操作的多个指令是原子执行的】 JMM 规定了对于 volatile long 和 volatile double，JVM 需要保证写入操作的原子性。 synchronizedhttps://www.javadoop.com/post/java-memory-model 一个线程在获取到监视器锁以后才能进入 synchronized 控制的代码块，一旦进入代码块，首先，该线程对于共享变量的缓存就会失效，因此 synchronized 代码块中对于共享变量的读取需要从主内存中重新获取，也就能获取到最新的值。 退出代码块的时候的，会将该线程写缓冲区中的数据刷到主内存中，所以在 synchronized 代码块之前或 synchronized 代码块中对于共享变量的操作随着该线程退出 synchronized 块，会立即对其他线程可见（这句话的前提是其他读取共享变量的线程会从主内存读取最新值）。 在进入 synchronized 的时候，并不会保证之前的写操作刷入到主内存中，synchronized 主要是保证退出的时候能将本地内存的数据刷入到主内存。 Java虚拟机虚拟机的几大问题 运行时数据区域 垃圾收集 对象可达 引用类型 GC Roots 算法 收集器 内存分配与回收策略（回收主要是老年代的触发条件） 类加载机制 新生代分为几个区？使用什么算法进行垃圾回收？为什么使用这个算法？新生代有三个区，一个较大的Eden区，两个小的Survivor区。 使用复制算法。（也有标记过程，标记-复制） 一方面，针对算法本身，相对于标记-清除算法，不会有内存碎片的问题；相对于标记-整理算法，处理效率高很多（在整理时，还未进行对象清理，移动存活对象时需要将存活对象插入到待清理对象之前，有大量的移动操作，时间复杂度很高）。 复制算法主要问题在于内存利用率，而HotSpot的Eden和Survivor的默认比例是8:1，保证内存利用率达到了90%，所以影响也不是太大。 另一方面，新生代minor gc比较频繁，对gc效率有比较高的要求；对象生命周期比较短，小的survivor空间即可容纳大部分情况下的存活对象。 引申：jvm的几个知识点，算法，判断对象存活，GC roots有哪些，内存分配与回收策略，类加载机制 垃圾收集算法【见Java虚拟机】 标记-清除 标记-整理 复制 分代收集 新生代：复制算法 老年代：标记-清除 or 标记整理 垃圾收集器与内存分配策略【祥见JVM的几个大知识点】垃圾收集器 内存分配策略 Minor GC 和 Full GC Minor GC:回收新生代，因为新生代对象存活时间很短，因此 Minor GC 会频繁执行，执行的速度一般也会比 较快。 Full GC:回收老年代和新生代，老年代对象其存活时间长，因此 Full GC 很少执行，执行速度会比 Minor GC 慢很多。 分配策略 对象优先在 Eden 分配 大多数情况下，对象在新生代 Eden 上分配，当 Eden 空间不够时，发起 Minor GC。 大对象直接进入老年代 大对象是指需要连续内存空间的对象，最典型的大对象是那种很长的字符串以及数组。 经常出现大对象会提前触发垃圾收集以获取足够的连续空间分配给大对象。 -XX:PretenureSizeThreshold，大于此值的对象直接在老年代分配，避免在 Eden 和 Survivor 之间的大量内存复制。 长期存活的对象进入老年代 为对象定义年龄计数器，对象在 Eden 出生并经过 Minor GC 依然存活，将移动到 Survivor 中，年龄就增加 1 岁， 增加到一定年龄则移动到老年代中。 -XX:MaxTenuringThreshold 用来定义年龄的阈值。 动态对象年龄判定 虚拟机并不是永远要求对象的年龄必须达到 MaxTenuringThreshold 才能晋升老年代，如果在 Survivor 中相同年龄 所有对象大小的总和大于 Survivor 空间的一半，则年龄大于或等于该年龄的对象可以直接进入老年代，无需等到 MaxTenuringThreshold 中要求的年龄。 空间分配担保 在发生 Minor GC 之前，虚拟机先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果条件成立的话，那么 Minor GC 可以确认是安全的。 如果不成立的话虚拟机会查看 HandlePromotionFailure 的值是否允许担保失败，如果允许那么就会继续检查老年代 最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次 Minor GC;如果小 于，或者 HandlePromotionFailure 的值不允许冒险，那么就要进行一次 Full GC。 Full GC 的触发条件 对于 Minor GC，其触发条件非常简单，当 Eden 空间满时，就将触发一次 Minor GC。而 Full GC 则相对复杂，有以下条件: 调用 System.gc()只是建议虚拟机执行 Full GC，但是虚拟机不一定真正去执行。不建议使用这种方式，而是让虚拟机管理内存。 老年代空间不足 老年代空间不足的常见场景为前文所讲的大对象直接进入老年代、长期存活的对象进入老年代等。 为了避免以上原因引起的 Full GC，应当尽量不要创建过大的对象以及数组。除此之外，可以通过 -Xmn 虚拟机参数 调大新生代的大小，让对象尽量在新生代被回收掉，不进入老年代。还可以通过 -XX:MaxTenuringThreshold 调大对 象进入老年代的年龄，让对象在新生代多存活一段时间。 空间分配担保失败使用复制算法的 Minor GC 需要老年代的内存空间作担保，如果担保失败会执行一次 Full GC。 JDK 1.7 及以前的永久代空间不足 在 JDK 1.7 及以前，HotSpot 虚拟机中的方法区是用永久代实现的，永久代中存放的为一些 Class 的信息、常量、静 态变量等数据。 当系统中要加载的类、反射的类和调用的方法较多时，永久代可能会被占满，在未配置为采用 CMS GC 的情况下也 会执行 Full GC。如果经过 Full GC 仍然回收不了，那么虚拟机会抛出 java.lang.OutOfMemoryError。 为避免以上原因引起的 Full GC，可采用的方法为增大永久代空间或转为使用 CMS GC。 Concurrent Mode Failure 执行 CMS GC 的过程中同时有对象要放入老年代，而此时老年代空间不足(可能是 GC 过程中浮动垃圾过多导致暂时 性的空间不足)，便会报 Concurrent Mode Failure 错误，并触发 Full GC。 ClassLoader原理和应用 ClassLoader的作用 加载class字节码文件到jvm 确认每个类应由那个类加载器加载，这也影响到两个类是否相等的判断，影响的方法有equals()、isAssignableFrom()、isInstance()以及instanceof关键字 加载的类存放在哪里？ jdk8之前在方法区，8之后在元数据区。 什么时候触发类加载？ 隐式加载 遇到new、getstatic、putstatic、invokestatic4条字节码指令时 对类进行反射调用时 当初始化一个类时，如果父类还没初始化，优先加载父类并初始化 虚拟机启动时，需指定一个包含main函数的主类，优先加载并初始化这个主类 显式加载 通过ClassLoader的loadClass方法 通过Class.forName 通过ClassLoader的findClass方法 有哪些类加载器ClassLoader？ Bootstrap ClassLoader：加载JVM自身工作需要的类，由JVM自己实现。加载JAVA_HOME/jre/lib下的文件 ExtClassLoader：是JVM的一部分，由sun.misc.Launcher$ExtClassLoader实现，会加载JAVA_HOME/jre/lib/ext下的文件，或由System.getProperty(&quot;java.ext.dirs&quot;)指定的目录下的文件 AppClassLoader：应用类加载器，由sun.misn.Launcher$AppClassLoader实现，加载System.getProperty(&quot;java.class.path&quot;)目录下的文件，也就是classpath路径。 双亲委派模型 原理：当一个类加载器收到类加载请求时，如果存在父类加载器，会先由父类加载器进行加载，当父类加载器找不到这个类时（根据类的全限定名称。找不到是由于，每个类有自己的加载路径。），当前类加载器才会尝试自己去加载。 为什么使用双亲委派模型？它可以解决什么问题？ 双亲委派模型能够保证类在内存中的唯一性。 假如没有双亲委派模型，用户自己写了个全限定名为java.lang.Object的类，并用自己的类加载器去加载，同时BootstrapClassLoader加载了rt.jar包中的jdk本身的java.lang.Object，这样内存中就存在两份Object类了，会出现很多问题，例如根据全限定名无法定位到具体的类。 高吞吐量的话用哪种gc算法高吞吐量，如果指cpu多用于用户程序，需要停顿时间比较短的收集器，新生代在服务端一般用Parallel Scavenge，算法也是复制算法。 复制算法的性能比较高。 jvm参数调优详细过程，到为什么这么设置，好处，一些gc场景，如何去分析gc日志jvm调优的基本原则： 大多数Java应用不需要进行JVM优化 大多数导致GC频繁、内存使用率高的问题的原因是代码层面的问题（代码层面） 上线前应考虑将JVM参数设置最优 减少创建对象的数量（代码层面） 较少使用全局变量和大对象（代码层面） 优先架构调优和代码调优，JVM优化是不得已的手段，或者说是发现问题 分析gc情况优化代码比优化JVM参数更好（代码层面） https://juejin.im/post/5dea4cb46fb9a01626644c36 新生代配置原则： 1.追求响应时间优先 这种需求下，新生代尽可能设置大一些，并通过实际情况调整新生代大小，直至接近系统的最小响应时间。因为新生代比较大，发生垃圾回收的频率会比较低，响应时间快速。 2.追求吞吐量优先 吞吐量优先的应用，在新生代中的大部分对象都会被回收，所以，新生代尽可能设置大。此时不追求响应时间，垃圾回收可以并行进行。 3.避免设置过小新生代 设置过小，YGC会很频繁，同时，很可能导致对象直接进入老年代中，老年代空间不足发生FullGC。 老年代配置原则： 1.追求响应时间优先 这种情况下，可以使用CMS收集器，以获取最短回收停顿时间，但是其内存分配需要注意，如果设置小了会造成回收频繁并且碎片变多；如果设置大了，回收的时间会很长。所以，最优的方案是根据GClog分析垃圾回收信息，调整内存大小。 2.追求吞吐量优先 吞吐量优先通常需要分配一个大新生代、小老年代，将短期存活的对象在新生代回收掉。 JVM性能调优的监控工具了解那些？jps jstack jmap jps [option] 输出Java进程信息 123jps -ml111957 org.apache.catalina.startup.Bootstrap -config /export/Domains/testenv.jd.local/server1/conf/server.xml start136044 sun.tools.jps.Jps -ml jstack [option] pid 输出某个进行内的线程栈信息 123jstack 111957 | grep 1b6d0\"System_Clock\" #307 daemon prio=5 os_prio=0 tid=0x00007f71b53f3800 nid=0x1b6d0 runnable [0x00007f72606d9000] 12-l long listings，会打印出额外的锁信息，在发生死锁时可以用&lt;strong&gt;jstack -l pid&lt;/strong&gt;来观察锁持有情况 -m mixed mode，不仅会输出Java堆栈信息，还会输出C/C++堆栈信息（比如Native方法） jmap [option] pid 输出某个进程内的堆信息：JVM版本、使用的GC算法、堆配置、堆内存使用情况 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647jmap -heap 111957Attaching to process ID 111957, please wait...Debugger attached successfully.Server compiler detected.JVM version is 25.20-b23using thread-local object allocation.Parallel GC with 43 thread(s)Heap Configuration: MinHeapFreeRatio = 0 MaxHeapFreeRatio = 100 MaxHeapSize = 2147483648 (2048.0MB) NewSize = 357564416 (341.0MB) MaxNewSize = 715653120 (682.5MB) OldSize = 716177408 (683.0MB) NewRatio = 2 SurvivorRatio = 8 MetaspaceSize = 21807104 (20.796875MB) CompressedClassSpaceSize = 1073741824 (1024.0MB) MaxMetaspaceSize = 17592186044415 MB G1HeapRegionSize = 0 (0.0MB)Heap Usage:PS Young GenerationEden Space: capacity = 353370112 (337.0MB) used = 28186432 (26.88067626953125MB) free = 325183680 (310.11932373046875MB) 7.976461801047849% usedFrom Space: capacity = 2097152 (2.0MB) used = 1736768 (1.65631103515625MB) free = 360384 (0.34368896484375MB) 82.8155517578125% usedTo Space: capacity = 2097152 (2.0MB) used = 0 (0.0MB) free = 2097152 (2.0MB) 0.0% usedPS Old Generation capacity = 869793792 (829.5MB) used = 160875768 (153.42308807373047MB) free = 708918024 (676.0769119262695MB) 18.495851485681793% used36932 interned Strings occupying 3347024 bytes. 输出堆内存中对象个数、大小统计直方图 1jmap -histo:live 111957 | less 123456789B byte C char D double F float I int J long Z boolean [ 数组，如[I表示int[] [L+类名 其他对象 dump出堆信息，再使用jhat或其他工具分析 123jmap -dump:format=b,file=dump.dat 111957jhat -port 8888 dump.dat# 浏览器输入 ip:port可访问 jstat [ generalOption | outputOptions vmid [interval[s|ms] [count]] ] jvm统计信息 vmid是Java虚拟机ID，在Linux/Unix系统上一般就是进程ID。interval是采样时间间隔。count是采样数目。 123456789101112131415jstat -gc 111957 250 6 # gc信息 S0C S1C S0U S1U EC EU OC OU MC MU CCSC CCSU YGC YGCT FGC FGCT GCT2048.0 2048.0 0.0 0.0 345088.0 148448.5 1397248.0 119834.5 80640.0 78303.8 8448.0 7919.0 133 8.230 5 11.160 19.3902048.0 2048.0 0.0 0.0 345088.0 148457.6 1397248.0 119834.5 80640.0 78303.8 8448.0 7919.0 133 8.230 5 11.160 19.3902048.0 2048.0 0.0 0.0 345088.0 148457.6 1397248.0 119834.5 80640.0 78303.8 8448.0 7919.0 133 8.230 5 11.160 19.3902048.0 2048.0 0.0 0.0 345088.0 150425.8 1397248.0 119834.5 80640.0 78303.8 8448.0 7919.0 133 8.230 5 11.160 19.3902048.0 2048.0 0.0 0.0 345088.0 150425.8 1397248.0 119834.5 80640.0 78303.8 8448.0 7919.0 133 8.230 5 11.160 19.3902048.0 2048.0 0.0 0.0 345088.0 150427.8 1397248.0 119834.5 80640.0 78303.8 8448.0 7919.0 133 8.230 5 11.160 19.390 1234567S0C、S1C、S0U、S1U：Survivor 0/1区容量（Capacity）和使用量（Used） EC、EU：Eden区容量和使用量 OC、OU：年老代容量和使用量 PC、PU：永久代容量和使用量 YGC、YGT：年轻代GC次数和GC耗时 FGC、FGCT：Full GC次数和Full GC耗时 GCT：GC总耗时 Java IOJava中的NIO，BIO，AIO分别是什么 BIO:同步并阻塞，服务器实现模式为一个连接一个线程，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销，当然可以通过线程池机制改善。BIO方式适用于连接数目比较小且固定的架构，这种方式对服务器资源要求比较高，并发局限于应用中，JDK1.4以前的唯一选择，但程序直观简单易理解。 NIO:同步非阻塞，服务器实现模式为一个请求一个线程，即客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接有I/O请求时才启动一个线程进行处理。NIO方式适用于连接数目多且连接比较短（轻操作）的架构，比如聊天服务器，并发局限于应用中，编程比较复杂，JDK1.4开始支持。 AIO:异步非阻塞，服务器实现模式为一个有效请求一个线程，客户端的I/O请求都是由OS先完成了再通知服务器应用去启动线程进行处理.AIO方式使用于连接数目多且连接比较长（重操作）的架构，比如相册服务器，充分调用OS参与并发操作，编程比较复杂，JDK7开始支持。","tags":[]},{"title":"框架","date":"2020-07-27T16:29:48.000Z","path":"2020/07/28/technology/06框架/","text":"Spring&amp;SpringMVC请详细描述springmvc处理请求全流程？ 通用的流程： 客户端提交请求到DispatcherServlet DispatcherServlet寻找Handler（HandlerExecutionChain）(包括handler , common interceptors和MappedInterceptor) DispatcherServlet调用controller controller调用业务逻辑，返回ModelAndView DispatcherServlet寻找ViewResolver，找到对应视图 渲染视图显示到客户端 restful的一些细节（上述2、3、4过程的细化，restful的mav一般是空的）： getHandler取到一个HandlerExecutionChain mappedHandler，包含URL对应的controller方法HandlerMethod，和一些interceptors HandlerMethod取到对应的handlerAdapter，数据绑定就再这个ha中做的 mappedHandler执行拦截器的preHandle handlerAdapter执行controller方法，包含请求前的数据绑定（数据转换），和请求后的数据转换（转换后将数据按需要的格式写入response） mappedHandler执行拦截器的postHandle 以上过程如果有抛出异常，由全局异常处理器来处理 mappedHandler触发拦截器的afterCompletion ioc原理、aop原理和应用 ioc原理 控制反转（依赖注入） 本质是，spring维护了一个实例的容器，在需要使用某个实例的地方，自动注入这个实例 主要运用了反射机制，通过反射来创建约定的实例，并维护在容器中 aop原理 面向切面编程 AOP原理 原理是动态代理。代理模式的定义：给某一个对象提供一个代理，并由代理对象控制对原对象的引用。实现方式： 首先有接口A，类a实现接口A 接着创建一个b InvocationHandler类，实现InvocationHandler接口，持有一个被代理对象的实例target，invoke方法中触发method 12345678910/** * proxy: 代表动态代理对象，编译时候生成的 * method：代表正在执行的方法 * args：代表调用目标方法时传入的实参 */public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println(\"代理执行\" +method.getName() + \"方法\"); Object result = method.invoke(target, args); return result;&#125; 创建代理对象 1A a = (A) Proxy.newProxyInstance(A.class.getClassLoader(), new Class&lt;?&gt;[]&#123;A.class&#125;, handler) 比如日志、监控等公共行为可以通过AOP来实现，避免大量重复代码 元素 切面：拦截器类，定义切点以及通知 切点：具体拦截的某个业务点 通知：切面当中的方法，声明通知方法在目标业务层的执行位置，通知类型如下： 前置通知：@Before 在目标业务方法执行之前执行 后置通知：@After 在目标业务方法执行之后执行 返回通知：@AfterReturning 在目标业务方法返回结果之后执行 异常通知：@AfterThrowing 在目标业务方法抛出异常之后 环绕通知：@Around 功能强大，可代替以上四种通知，还可以控制目标业务方法是否执行以及何时执行 aspectj切面扫描的细节再看下 spring 事务实现Spring事务的底层依赖MySQL的事务，代码层面上利用AOP实现。 常用的是@Transactional注解，会被解析生成一个代理服务，TransactionInterceptor对它进行拦截处理，进行事务开启、 commit或者rollback的操作。 另外，spring还定义了事务传播行为，有7种类型，项目中常见的是PROPAGATION_REQUIRED。如果没有事务就新建事务，如果存在事务，就加入这个事务。 执行事务的时候使用TransactionInterceptor进行拦截，然后处理 事务传播行为 事务传播行为类型 说明 PROPAGATION_REQUIRED 如果当前没有事务，就新建一个事务，如果已经存在一个事务中，加入到这个事务中。这是最常见的选择。（如果父方法有事务，加入父方法的事务；父方法没有事务，则自己新建一个事务） PROPAGATION_SUPPORTS 支持当前事务，如果当前没有事务，就以非事务方式执行。（如果父方法有事务，加入父方法的事务；父方法没有事务，则以非事务执行） PROPAGATION_MANDATORY 使用当前的事务，如果当前没有事务，就抛出异常。（依赖父方法事务） PROPAGATION_REQUIRES_NEW 新建事务，如果当前存在事务，把当前事务挂起。（如果父方法有事务，把父方法事务挂起，自己新建事务；父方法没有事务，则自己新建一个事务） PROPAGATION_NOT_SUPPORTED 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。（如果父方法有事务，把父方法事务挂起，以非事务执行自己的操作；父方法没有事务，则以非事务执行）（总是以非事务执行，不报错） PROPAGATION_NEVER 以非事务方式执行，如果当前存在事务，则抛出异常。（总是以非事务执行，如果父方法存在事务，抛异常） PROPAGATION_NESTED 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与PROPAGATION_REQUIRED类似的操作。 REQUIRED、REQUIRES_NEW、NESTED的对比 REQUIRED共用一个事务。 REQUIRES_NEW 有独立的子事务，子事务异常不会导致父事务回滚，父事务异常也不会导致子事务回滚，相互独立。 NESTED 子事务嵌套在父事务中，父事务回滚会引起子事务回滚；父事务正常、子事务异常，子事务可以单独回滚。 源码详解 txNamespaceHandle注册的InfrastructureAdvisorAutoProxyCreator是一个BeanPostProcessor，主要是为了创建动态代理（wrapIfNecessary） 这几个类是可以自动创建代理的 在创建代理的时候，获取切面 txNamespaceHandler注册了一个Advisor（BeanFactoryTransactionAttributeSourceAdvisor），再在这个advisor中判断是否当前bean符合这个切面（主要实现就是看有没有@Transactional注解） TransactionInterceptor是advice，增强，执行切面工作 摘录：https://my.oschina.net/fifadxj/blog/785621 spring-jdb的事务流程： 1234567891011121314DefaultTransactionDefinition def = new DefaultTransactionDefinition();PlatformTransactionManager txManager = new DataSourceTransactionManager(dataSource);TransactionStatus status = txManager.getTransaction(def);try &#123; //get jdbc connection... //execute sql... txManager.commit(status);&#125;catch (Exception e) &#123; txManager.rollback(status); throw e;&#125; PlatformTransactionManager的getTransaction(), rollback(), commit()是spring处理事务的核心api，分别对应事务的开始，提交和回滚。 TransactionSynchronizationManager负责从ThreadLocal中存取jdbc connection 创建事务的时候会通过dataSource.getConnection()获取一个新的jdbc connection，然后绑定到ThreadLocal 在业务代码中执行sql时，通过DataSourceUtils.getConnection()从ThreadLocal中获取当前事务的jdbc connection, 然后在该jdbc connection上执行sql commit和rollback事务时，从ThreadLocal中获取当前事务的jdbc connection，然后对该jdbc connection进行commit和rollback mybatis-spring的事务流程： 配置 1234567891011121314&lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\" /&gt;&lt;/bean&gt;&lt;bean id=\"sqlSessionFactory\" class=\"org.mybatis.spring.SqlSessionFactoryBean\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\" /&gt; &lt;property name=\"transactionFactory\"&gt; &lt;bean class=\"org.apache.ibatis.spring.transaction.SpringManagedTransactionFactory\" /&gt; &lt;/property&gt; &lt;/bean&gt;&lt;bean id=\"sqlSession\" class=\"org.mybatis.spring.SqlSessionTemplate\"&gt; &lt;constructor-arg index=\"0\" ref=\"sqlSessionFactory\" /&gt;&lt;/bean&gt; mybatis-spring依赖DataSourceTransactionManager来处理事务，并没有创建自己的PlatformTransactionManager实现。 mybatis通过SqlSessionFactoryBuilder创建SqlSessionFactory，而mybatis-spring通过SqlSessionFactoryBean创建SqlSessionFactory。 配置使用SpringManagedTransactionFactory来创建MyBatis的Transaction实现SpringManagedTransaction 配置使用SqlSessionTemplate代替通过SqlSessionFactory.openSession()获取SqlSession 调用过程 可以看到mybatis-spring处理事务的主要流程和spring jdbc处理事务并没有什么区别，都是通过DataSourceTransactionManager的getTransaction(), rollback(), commit()完成事务的生命周期管理，而且jdbc connection的创建也是通过DataSourceTransactionManager.getTransaction()完成，mybatis并没有参与其中，mybatis只是在执行sql时通过DataSourceUtils.getConnection()获得当前thread的jdbc connection，然后在其上执行sql。 sqlSessionTemplate是DefaultSqlSession的一个代理类，它通过SqlSessionUtils.getSqlSession()试图从ThreadLocal获取当前事务所使用的SqlSession。如果是第一次获取时会调用SqlSessionFactory.openSession()创建一个SqlSession并绑定到ThreadLocal，同时还会通过TransactionSynchronizationManager注册一个SqlSessionSynchronization。 SqlSessionSynchronization是一个事务生命周期的callback接口，mybatis-spring通过SqlSessionSynchronization在事务提交和回滚前分别调用DefaultSqlSession.commit()和DefaultSqlSession.rollback() 这里的DefaultSqlSession只会进行一些自身缓存的清理工作，并不会真正提交事务给数据库，原因是这里的DefaultSqlSession使用的Transaction实现为SpringManagedTransaction，SpringManagedTransaction在提交事务前会检查当前事务是否应该由spring控制，如果是，则不会自己提交事务，而将提交事务的任务交给spring，所以DefaultSqlSession并不会自己处理事务。 DefaultSqlSession执行sql时，会通过SpringManagedTransaction调用DataSourceUtils.getConnection()从ThreadLocal中获取jdbc connection并在其上执行sql。 mybatis-spring做的最主要的事情是： 在SqlSession执行sql时通过用SpringManagedTransaction代替mybatis的JdbcTransaction，让SqlSession从spring的ThreadLocal中获取jdbc connection。 通过注册事务生命周期callback接口SqlSessionSynchronization，让SqlSession有机会在spring管理的事务提交或回滚时清理自己的内部缓存。 spring的循环依赖如何解决？为什么要三级缓存？https://juejin.im/post/5c98a7b4f265da60ee12e9b2 https://juejin.im/post/5e927e27f265da47c8012ed9 spring对循环依赖的处理有三种情况： 构造器的循环依赖：这种依赖spring是处理不了的，直接抛出BeanCurrentlylnCreationException异常。 单例模式下的setter循环依赖：通过“三级缓存”处理循环依赖。 非单例循环依赖：无法处理。 如何解决的？ 只能解决单例的属性循环依赖的情况。本质上是通过将创建好的、或正在创建中的bean缓存起来。比如A和B循环依赖，创建A时先将A的实例放入缓存，自动注入属性B时，发现缓存中没有B，那么来创建B的实例，将B实例化放入缓存，注入属性A，发现A在缓存中，取出来赋值给A。bean B创建完成返回，赋值给A的属性B。这时候A和B的bean就都创建好了。 为什么要三级？看起来一级就可以实现呀？ 为什么要三级缓存：循环依赖的关键点：提前暴露绑定A原始引用的工厂类到工厂缓存。等需要时触发后续操作处理A的早期引用，将处理结果放入二级缓存 只有一级singeltonObjects肯定是不行的，需要一个放半成品的地方 实际上二级就够了，可以解决循环依赖的问题 考虑到代理的情况，就需要objectFactories这个三级缓存了，因为代理的创建是在第三步，这时候动态代理还没产生，注入了也不是最终的实例。放入三级缓存时，重写了getObject方法，会调用BeanPostProcessor的getEarlyBeanReference，这时候取到的就会是动态代理后的。 singletonFactories ： 进入实例化阶段的单例对象工厂的cache （三级缓存） earlySingletonObjects ：完成实例化但是尚未初始化的，提前暴光的单例对象的Cache （二级缓存） singletonObjects：完成初始化的单例对象的cache（一级缓存） Zookeeperzk挂了怎么办？ todo 指zk集群挂了其中一台机器？ – 集群自己可以处理 挂的是master 挂的是follower 挂的是.. 集群全挂了？—那就是全挂了啊 趁早加入监控和降级策略 Dubbo&amp;Netty&amp;RPChttps://juejin.im/post/5e215783f265da3e097e9679 RPC remote procedure call 远程过程调用，是一种进程间的通信方式，是一种技术思想，而不是规范 一次完整的rpc调用流程。RPC的目标是把2-8封装起来，对用户透明。 (1):服务消费方(client)以本地调用方式调用服务。 (2):client stub接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体。 (3):client stub找到服务地址，并将消息发送到服务端。 (4):server stub收到消息后进行解码。 (5):server stub根据解码结果调用本地的服务。 (6):本地服务执行并将结果返回给server stub。 (7):server stub将返回结果打包成消息并发送至消费方。 (9):client stub接收到消息，并进行解码。 (9):服务消费方得到最终结果。 决定rpc效率的两个重要因素：通信效率，序列化和反序列化效率 常见rpc框架：dubbo、gRPC、Thrift、HSF（high speed service framework） netty 理解netty netty是一个异步事件驱动的网络应用程序框架，是基于NIO的多路复用模型实现的。 传统HTTP服务 【HTTP服务器之所以称为HTTP服务器，是因为编码解码协议是HTTP协议，如果协议是Redis协议，那它就成了Redis服务器，如果协议是WebSocket，那它就成了WebSocket服务器，等等。 使用Netty可以定制编解码协议，实现自己的特定协议的服务器。】 创建一个ServerSocket，监听并绑定一个端口 一系列客户端来请求这个端口 服务器使用Accept，获得一个来自客户端的Socket连接对象 启动一个新线程处理连接 读Socket，得到字节流 解码协议，得到HTTP请求对象 处理HTTP请求，得到一个结果，封装成一个HTTPResponse对象 编码协议，将结果序列化字节流写入Socket，发给客户端 循环步骤3 NIO 不是Java独有的概念，NIO代表IO多路复用。 由操作系统提供的功能，早期select，后期linux-epoll/max-kqueue。一般就说是epoll（没人用mac当服务器） Netty基于Java NIO进行了封装，提供易于操作的使用模式和接口。 BIO (Blocking IO)，如何理解blocking 服务端监听时，accept是阻塞的，只有新连接来了，accept才会返回，主线程才能继续 读写Socket时，read是阻塞的，只有请求消息来了（需要读完吗？），read才能返回，子线程才能继续处理 读写Socket时，write是阻塞的，只有客户端把消息接收了（客户端把消息接收了是什么表现？），write才能返回，子线程才能继续 NIO利用事件机制（=事件驱动机制）实现非阻塞。【可以用一个线程把Accept，读写操作，请求处理的逻辑全干了。如果什么事都没得做，它也不会死循环，它会将线程休眠起来，直到下一个事件来了再继续干活，这样的一个线程称之为NIO线程。】 伪代码 123456789101112131415while true &#123; events = takeEvents(fds) // 获取事件，如果没有事件，线程就休眠 for event in events &#123; if event.isAcceptable &#123; doAccept() // 新链接来了 &#125; elif event.isReadable &#123; request = doRead() // 读消息 if request.isComplete() &#123; doProcess() &#125; &#125; elif event.isWriteable &#123; doWrite() // 写消息 &#125; &#125;&#125; Reactor（基于事件驱动）线程模型 【netty可以基于以下模型灵活配置，比较常见的是用第三种。】 【在Netty里面，Accept连接可以使用单独的线程池去处理，读写操作又是另外的线程池来处理。】 【Accept连接和读写操作也可以使用同一个线程池来进行处理。请求处理逻辑既可以使用单独的线程池进行处理，也可以跟读写线程放在一块处理。】 【线程池中的每一个线程都是NIO线程。用户可以根据实际情况进行组装，构造出满足系统需求的高性能并发模型。】 Reactor单线程模型。一个NIO线程+一个accept线程。reactor线程负责分发，read、decode等操作都由其他线程处理。就和上面的伪代码差不多。 Reactor多线程模型。相比上一种，【其他线程】由线程池来托管。 Reactor主从模型。多个acceptor的NIO线程池用于接收客户端的连接。 TCP粘包拆包 现象 假设使用netty在客户端重复写100次数据”你好，我的名字是xxx!”给服务端，用ByteBuf存放这个数据 服务端接收后输出，一般存在三种情况 完整的一个字符串 字符串多了 字符串少了 原因：尽管client按照ByteBuf为单位发送数据，server按照ByteBuf读取，但操作系统底层是tcp协议，按照字节发送和接收数据，在netty应用层，重新拼装成的ByteBuf与客户端发送过来的ByteBuf可能不是对等的。 因此，我们需要自定义协议来封装和解封应用层的数据包。 netty中定义好的拆包器 固定长度的拆包器 FixedLengthFrameDecoder 行拆包器 LineBasedFrameDecoder 分隔符拆包器 DelimiterBasedFrameDecoder （行拆包器的通用版本，可自定义分隔符） 长度域拆包器 LengthFieldBasedFrameDecoder （最通用，在协议中包含长度域字段） 零拷贝 传统方式的拷贝 File.read(bytes)Socket.send(bytes) 需要四次数据拷贝和四次上下文切换 数据从磁盘读取到内核的read buffer 数据从内核缓冲区拷贝到用户缓冲区 数据从用户缓冲区拷贝到内核的socket buffer 数据从内核的socket buffer拷贝到网卡接口（硬件）的缓冲区 零拷贝的概念 上面的第二步和第三步是没有必要的，通过java的FileChannel.transferTo方法，可以避免上面两次多余的拷贝（需要操作系统支持） 调用transferTo,数据从文件由DMA引擎拷贝到内核read buffer 接着DMA从内核read buffer将数据拷贝到网卡接口buffer 上面的两次操作都不需要CPU参与，达到了零拷贝。 Netty中的零拷贝 体现在三个方面： bytefuffer Netty发送和接收消息主要使用bytebuffer，bytebuffer使用直接内存（DirectMemory）直接进行Socket读写。 原因：如果使用传统的堆内存进行Socket读写，JVM会将堆内存buffer拷贝一份到直接内存中然后再写入socket，多了一次缓冲区的内存拷贝。DirectMemory中可以直接通过DMA发送到网卡接口 Composite Buffers 传统的ByteBuffer，如果需要将两个ByteBuffer中的数据组合到一起，需要先创建一个size=size1+size2大小的新的数组，再将两个数组中的数据拷贝到新的数组中。 使用Netty提供的组合ByteBuf，就可以避免这样的操作。CompositeByteBuf并没有真正将多个Buffer组合起来，而是保存了它们的引用，从而避免了数据的拷贝，实现了零拷贝。 对FileChannel.transferTo的使用 Netty中使用了FileChannel的transferTo方法，该方法依赖于操作系统实现零拷贝。 dubbo 简介与特性：dubbo是一款高性能、轻量级的开源Java RPC框架，提供三大核心能力：面向接口的远程方法调用、智能容错和负载均衡、服务自动注册和发现。 【以下几点是官网上的特性介绍…】 面向接口的远程方法调用：提供高性能的基于代理的远程调用能力，服务以接口为粒度，为开发者屏蔽远程调用底层细节。 智能负载均衡：内置多种负载均衡策略（有哪些？），感知下游节点的健康状况，显著减少调用延迟，提高系统吞吐量。 服务自动注册与发现：支持多种注册中心服务（有哪些？），服务实例上下线实时感知（具体实现是什么？）。 高度可扩展能力：遵循微内核+插件的设计原则，所有核心能力如Protocol、Transport、Serialization被设计为可扩展点，平等的对待内置实现和第三方实现。（SPI设计模式？） 运行期流量调度：内置条件、脚本等路由策略，通过配置不同的路由规则，实现灰度发布、同机房优先等功能。 可视化的服务治理与运维：提供丰富服务治理、运维工具：随时查看服务元数据、服务健康状态以及调用统计，实时下发路由策略、调度配置参数。 dubbo架构 以上两张图说明dubbo执行流程： dubbo容器启动后，provider将自己提供的服务注册到注册中心（注册中心便知道有哪些服务上线了） consumer启动后，从注册中心订阅需要的服务。 注册中心以长连接的方式向consumer发送服务变更通知。 consumer同步调用provider的服务（如果服务有多个节点，可通过负载均衡算法选择一个节点进行调用） consumer和provider会定期将调用信息（调用时间、调用服务信息）发送给监控中心 Dubbo容器启动、服务生产者注册自己的服务、服务消费者从注册中心中订阅服务是在Dubbo应用启动时完成的；consumer调用provider是同步过程；注册中心向consumer发送服务变更通知是异步的；consumer和provider向监控中心发送信息是异步的。 调用链整体展开： 下面这张图看起来有点复杂了.. Dubbo配置的覆盖关系 (1):方法级优先、接口级次之，全局配置优先级最低。 (2):如果级别一样，则消费者优先，提供方次之。 dobbo高可用 注册中心Zookeeper宕机，还可以消费Dubbo暴露的服务。 Dubbo的监控中心宕机，不会影响Dubbo的正常使用，只是丢失了部分采样数据。 数据库宕机后，注册中心仍然可以通过缓存提供服务列表查询，但是不能注册新的服务。 注册中心集群的任意一个节点宕机，将自动切换到另外一台。 注册中心全部宕机，服务提供者和消费者可以通过本地缓存通讯。 服务提供者无状态，任意一台宕机后，不影响使用。 服务提供者全部宕机，服务消费者应用将无法使用，并且会无限次重连等待服务提供者恢复。 负载均衡策略 【默认为随机】 基于权重的随机负载均衡：Random LoadBalance，比如orderService想要远程调用userService，而userService分别在三台机器上，我们可以给每台机器设置权重，比如三台机器的权重依次为100、200、50，则总权重为350，则选择第一台的概率就是100/350. 基于权重的轮询负载均衡：RoundRobin LoadBalance（可以理解为按照权重占比进行轮询。占比少的，当权重比较低时就不会再去权重低的机器上请求。如果某台机器性能一般，但权重占比高，就很可能卡在这里） 最少活跃数负载均衡：LeastActive LoadBalance，比如三台服务器上一次处理请求所花费的时间分别为100ms、1000ms、300ms，则这一次请求回去上一次处理请求时间最短的机器，所以这次一号服务器处理这次请求。 一致性Hash负载均衡：ConsistentHash LoadBalance 原文：https://blog.csdn.net/revivedsun/java/article/details/71022871 一致性Hash负载均衡涉及到两个主要的配置参数为hash.arguments 与hash.nodes。 hash.arguments ： 当进行调用时候根据调用方法的哪几个参数生成key，并根据key来通过一致性hash算法来选择调用结点。例如调用方法invoke(String s1,String s2); 若hash.arguments为1(默认值)，则仅取invoke的参数1（s1）来生成hashCode。 hash.nodes： 为结点的副本数。 12345缺省只对第一个参数Hash，如果要修改，请配置&lt;dubbo:parameter key=\"hash.arguments\" value=\"0,1\" /&gt;缺省用160份虚拟节点，如果要修改，请配置&lt;dubbo:parameter key=\"hash.nodes\" value=\"320\" /&gt; 降级服务 当服务器压力剧增的情况下，根据实际业务及流量，对一些服务和页面有策略地不处理或者换种简单的方式处理，从而释放服务器资源以保证核心交易正常或高效运行。 mock=force:return+null:表示消费方对该服务的方法都返回null值，不发起远程调用。用来屏蔽不重要的服务不可用时对调用方的影响，可以直接在Dubbo客户端(localhost:7001)对服务消费者设置，屏蔽掉即可。 mock=fall:return+null:表示消费方对该服务的方法调用在失败后，再返回null，不抛出异常。用来容忍不重要服务不稳定时对调用方的影响，可以直接在Dubbo客户端(localhost:7001)对服务消费者设置，容错掉即可。 集群容错 Failover Cluster:失败自动切换，当出现失败，重试其他服务器。通常用于读操作，但重试会带来更长延迟。可通过retries=n来设置重试次数。 Failfast Cluster:快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增操作。 Forking Cluster:并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作，但需要浪费更多的服务资源。通过过fork=n设置最大并行数。 Broadcast Cluster:广播调用所有提供者，逐个调用，任意一台报错则报错，通常用于通知所有服务提供者更新缓存或日志等本地资源信息。 远程调用细节 服务提供者暴露一个服务的概要过程 ServiceConfig引用对外提供服务的实现类ref，如GreetingServiceImpl 通过ProxyFactory的实现类的getInvoker()方法使用ref生成一个AbstractProxyInvoker实例 DubboProtocol的export方法，会启动NettyServer监听服务连接，并将服务注册到注册中心 服务方如何处理请求 NettyServer是一个Handler，接收到一个请求 将请求派发到dubbo内部的业务线程池（默认线程模型为all，所有消息都派发给业务线程，包括请求事件、响应事件、连接事件、断开事件、心跳事件等，AllChannelHandler） 调用到DubboProtocol的connected方法，触发AbstractProxyInvoker执行，有返回值的写入Channel 消费方启动流程 ReferenceConfig的get()触发，到init() -&gt; createProxy()，创建一个代理，处理类是DubboInvoker（共享同一台服务方机器的NettyClient），再包装上RegistryDirectory和Cluster，做路由、负载均衡和容错 服务消费者消费一个服务的概要过程 ReferenceConfig的init()方法调用Protocol实现类的refer()方法生成Invoker实例 把Invoker转换为客户端需要的接口，如GreetingService，发生在ProxyFactory实现类的getProxy()方法中，主要是使用代理（对服务接口的调用转换为对Invoker的调用） 模块简单介绍 业务线程、用户线程、io线程 消息队列作用 解耦 异步 削峰/限流 原理介绍 todo如何保证RocketMQ 消息的顺序性，如何解决重复消费问题针对kafka来说 如何保证消息的顺序性： 一个分区内的消息是顺序的 一个主题的不同分区之间，消息不能保证有序 – 对同一类消息指定相同的key，相同的key会哈希到同一个分区，这样可以保证这部分消息的有序性 https://www.cnblogs.com/756623607-zhang/p/10506909.html 如何解决重复消费： kafka自带的消费机制 consumer消费后，会定期将消费过的offset偏移量提交给broker。如果consumer重启，会继续上次的offset开始消费。 业务上保证幂等性 如果进程挂了或机器宕机，没来得及提交offset，需要业务上进行幂等。 比如建立一张消息表。 生产者，发送消息前判断库中是否有记录（有记录说明已发送），没有记录，先入库，状态为待消费，然后发送消息并把主键id带上。 消费者，接收消息，通过主键ID查询记录表，判断消息状态是否已消费。若没消费过，则处理消息，处理完后，更新消息记录的状态为已消费。 MyBatisMyBatis，Mybatis与SpringMyBatis 消除了大部分 JDBC 的样板代码、手动设置参数以及检索结果。通过简洁的设计最大限度地简化开发和提升性能。 解除SQL与程序代码的耦合，通过提供dao层，将业务逻辑和数据访问逻辑分离开。设计更清晰，更易维护。 MyBatis整体架构 MyBatis层级结构 裸用sqlSession是上面的红框 spring用mapper/dao接口代理，本质上是一个MapperProxy，从下面的红框开始执行 spring事务是在哪个环节起作用？ https://mybatis.org/spring/zh/transactions.html 一个使用 MyBatis-Spring 的其中一个主要原因是它允许 MyBatis 参与到 Spring 的事务管理中。而不是给 MyBatis 创建一个新的专用事务管理器，MyBatis-Spring 借助了 Spring 中的 DataSourceTransactionManager 来实现事务管理。 一旦配置好了 Spring 的事务管理器，你就可以在 Spring 中按你平时的方式来配置事务。并且支持 @Transactional 注解和 AOP 风格的配置。在事务处理期间，一个单独的 SqlSession 对象将会被创建和使用。当事务完成时，这个 session 会以合适的方式提交或回滚。 事务配置好了以后，MyBatis-Spring 将会透明地管理事务。 所以，最外层是事务，每个事务会起一个SqlSession。 几篇文章： 入门，裸用mybatis：https://juejin.im/post/5aa5c6fb5188255587232e5a#heading-0 mybatis执行，包括整合spring后的流程：https://juejin.im/post/5e350d895188254dfd43def5#heading-9 关于JDBC：https://juejin.im/post/5c75e6666fb9a049cd54dc88 Mybatis和spring整合的使用：https://juejin.im/post/5cdfed6ef265da1b6720dcaf mybatis框架说明： 整体执行流程说明： sqlSession执行流程说明： 关键流程（以下整个可以看成裸用MyBatis的执行流程） config文件加载：解析xml文件配置项 mapper文件加载：上一个流程中的一个环节，解析完后封装成MappedStatement，存入configuration SqlSource创建流程：上一流程的一个环节，SqlSource是MappedStatement的一部分，主要存放sql和占位的参数名称 – 解析环节结束 SqlSession执行流程：sqlSessionFactory.openSession主要是建立了一个和数据库的连接connection 获取BoundSql流程：sqlSession.xx方法执行时，需要获取BoundSql，BoundSql本质上是SqlSource和执行请求的入参的一个组合 参数映射流程：根据顺序，或者根据名称（只是大略看了一眼） 结果集映射流程：根据名称（只是大略看了一眼） mybatis的openSession默认开启事务，autocommit为false，隔离级别为null mybatis的JdbcTransaction 整合spring的几个组件 org.mybatis.spring.SqlSessionFactoryBean 注入sqlSessionFactory org.mybatis.spring.mapper.MapperScannerConfigurer扫描指定包 将包下class文件加入到beanDefinition中，bean类型指定为MapperFactoryBean SqlSessionFactoryBean构建sqlSessionFactory时，扫描mapper xml文件，根据namespace在MapperRegistry中注入对应mapper接口的MapperProxyFactory MapperFactoryBean-&gt;getObject中生成mapper的代理类MapperProxy（通过MapperFactoryBean中的interface，即mapper的namespace找到MapperProxyFactory，再生产出代理类） 以下大概知道了 现在差一个中间环节，mapper的beanDefinition怎么变成MapperProxy..以及MapperFactoryBean的作用 还有个SqlSessionTemplate：https://juejin.im/post/5cea1f386fb9a07ea803a70e 还有MapperProxyFactory – 来创建MapperProxy Java动态代理：https://juejin.im/post/5c1ca8df6fb9a049b347f55c MapperFactoryBean MapperProxy MapperMethod – 到这里之后，流程就转到sqlSession.selectOne之类的了 Mybatis缓存 https://juejin.im/post/5e81fb126fb9a03c546c22bb MyBatis 系统中默认定义了两级缓存：一级缓存和二级缓存 默认情况下，只有一级缓存开启。（SqlSession级别的缓存，也称为本地缓存） 二级缓存需要手动开启和配置，它是基于 namespace 级别的缓存，缓存只作用于 cache 标签所在的映射文件中的语句。 附录Protocol的适配器类jad org.apache.dubbo.rpc.Protocol$Adaptive 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475ClassLoader:+-sun.misc.Launcher$AppClassLoader@18b4aac2 +-sun.misc.Launcher$ExtClassLoader@737996a0Location:/github/learn/dubbo2/dubbo/dubbo-common/target/classes//* * Decompiled with CFR. * * Could not load the following classes: * org.apache.dubbo.common.URL * org.apache.dubbo.common.extension.ExtensionLoader * org.apache.dubbo.rpc.Exporter * org.apache.dubbo.rpc.Invoker * org.apache.dubbo.rpc.Protocol * org.apache.dubbo.rpc.RpcException */package org.apache.dubbo.rpc;import java.util.List;import org.apache.dubbo.common.URL;import org.apache.dubbo.common.extension.ExtensionLoader;import org.apache.dubbo.rpc.Exporter;import org.apache.dubbo.rpc.Invoker;import org.apache.dubbo.rpc.Protocol;import org.apache.dubbo.rpc.RpcException;public class Protocol$Adaptiveimplements Protocol &#123; public void destroy() &#123; throw new UnsupportedOperationException(\"The method public abstract void org.apache.dubbo.rpc.Protocol.destroy() of interface org.apache.dubbo.rpc.Protocol is not adaptive method!\"); &#125; public int getDefaultPort() &#123; throw new UnsupportedOperationException(\"The method public abstract int org.apache.dubbo.rpc.Protocol.getDefaultPort() of interface org.apache.dubbo.rpc.Protocol is not adaptive method!\"); &#125; public List getServers() &#123; throw new UnsupportedOperationException(\"The method public default java.util.List org.apache.dubbo.rpc.Protocol.getServers() of interface org.apache.dubbo.rpc.Protocol is not adaptive method!\"); &#125; // 自适配方法1 public Invoker refer(Class class_, URL uRL) throws RpcException &#123; String string; if (uRL == null) &#123; throw new IllegalArgumentException(\"url == null\"); &#125; URL uRL2 = uRL; String string2 = string = uRL2.getProtocol() == null ? \"dubbo\" : uRL2.getProtocol(); if (string == null) &#123; throw new IllegalStateException(new StringBuffer().append(\"Failed to get extension (org.apache.dubbo.rpc.Protocol) name from url (\").append(uRL2.toString()).append(\") use keys([protocol])\").toString()); &#125; Protocol protocol = (Protocol)ExtensionLoader.getExtensionLoader(Protocol.class).getExtension(string); return protocol.refer(class_, uRL); &#125; // 自适配方法2 public Exporter export(Invoker invoker) throws RpcException &#123; String string; if (invoker == null) &#123; throw new IllegalArgumentException(\"org.apache.dubbo.rpc.Invoker argument == null\"); &#125; if (invoker.getUrl() == null) &#123; throw new IllegalArgumentException(\"org.apache.dubbo.rpc.Invoker argument getUrl() == null\"); &#125; URL uRL = invoker.getUrl(); String string2 = string = uRL.getProtocol() == null ? \"dubbo\" : uRL.getProtocol(); if (string == null) &#123; throw new IllegalStateException(new StringBuffer().append(\"Failed to get extension (org.apache.dubbo.rpc.Protocol) name from url (\").append(uRL.toString()).append(\") use keys([protocol])\").toString()); &#125; Protocol protocol = (Protocol)ExtensionLoader.getExtensionLoader(Protocol.class).getExtension(string); return protocol.export(invoker); &#125;&#125; 服务方Service的代理封装jad org.apache.dubbo.common.bytecode.Wrapper1 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102ClassLoader:+-sun.misc.Launcher$AppClassLoader@18b4aac2 +-sun.misc.Launcher$ExtClassLoader@737996a0Location:/github/learn/dubbo2/dubbo/dubbo-common/target/classes//* * Decompiled with CFR. * * Could not load the following classes: * org.apache.dubbo.common.bytecode.ClassGenerator$DC * org.apache.dubbo.common.bytecode.NoSuchMethodException * org.apache.dubbo.common.bytecode.NoSuchPropertyException * org.apache.dubbo.common.bytecode.Wrapper * org.apache.dubbo.demo.PoJo * org.apache.dubbo.demo.provider.HelloServiceImpl */package org.apache.dubbo.common.bytecode;import java.lang.reflect.InvocationTargetException;import java.util.Map;import org.apache.dubbo.common.bytecode.ClassGenerator;import org.apache.dubbo.common.bytecode.NoSuchMethodException;import org.apache.dubbo.common.bytecode.NoSuchPropertyException;import org.apache.dubbo.common.bytecode.Wrapper;import org.apache.dubbo.demo.PoJo;import org.apache.dubbo.demo.provider.HelloServiceImpl;public class Wrapper1extends Wrapperimplements ClassGenerator.DC &#123; public static String[] pns; public static Map pts; public static String[] mns; public static String[] dmns; public static Class[] mts0; public static Class[] mts1; public String[] getPropertyNames() &#123; return pns; &#125; public boolean hasProperty(String string) &#123; return pts.containsKey(string); &#125; public Class getPropertyType(String string) &#123; return (Class)pts.get(string); &#125; public String[] getMethodNames() &#123; return mns; &#125; public String[] getDeclaredMethodNames() &#123; return dmns; &#125; public void setPropertyValue(Object object, String string, Object object2) &#123; try &#123; HelloServiceImpl helloServiceImpl = (HelloServiceImpl)object; &#125; catch (Throwable throwable) &#123; throw new IllegalArgumentException(throwable); &#125; throw new NoSuchPropertyException(new StringBuffer().append(\"Not found property \\\"\").append(string).append(\"\\\" field or setter method in class org.apache.dubbo.demo.provider.HelloServiceImpl.\").toString()); &#125; public Object getPropertyValue(Object object, String string) &#123; try &#123; HelloServiceImpl helloServiceImpl = (HelloServiceImpl)object; &#125; catch (Throwable throwable) &#123; throw new IllegalArgumentException(throwable); &#125; throw new NoSuchPropertyException(new StringBuffer().append(\"Not found property \\\"\").append(string).append(\"\\\" field or getter method in class org.apache.dubbo.demo.provider.HelloServiceImpl.\").toString()); &#125; // 核心在这里 public Object invokeMethod(Object object, String string, Class[] arrclass, Object[] arrobject) throws InvocationTargetException &#123; HelloServiceImpl helloServiceImpl; try &#123; helloServiceImpl = (HelloServiceImpl)object; &#125; catch (Throwable throwable) &#123; throw new IllegalArgumentException(throwable); &#125; try &#123; if (\"testGeneric\".equals(string) &amp;&amp; arrclass.length == 1) &#123; return helloServiceImpl.testGeneric((PoJo)arrobject[0]); &#125; if (\"sayHello\".equals(string) &amp;&amp; arrclass.length == 1) &#123; return helloServiceImpl.sayHello((String)arrobject[0]); &#125; &#125; catch (Throwable throwable) &#123; throw new InvocationTargetException(throwable); &#125; throw new NoSuchMethodException(new StringBuffer().append(\"Not found method \\\"\").append(string).append(\"\\\" in class org.apache.dubbo.demo.provider.HelloServiceImpl.\").toString()); &#125;&#125; ExtensionLoader和getAdaptiveExtension ExtensionLoader缓存了每种组件的名称-实例映射 组件适配器xx$Adaptive从相应的ExtensionLoader中取需要的实例","tags":[]},{"title":"数据库","date":"2020-07-27T16:29:48.000Z","path":"2020/07/28/technology/01数据库/","text":"1create database test default charset utf8 collate utf8_general_ci; 简单版架构与执行流程系统架构MySQL架构 □ 连接池组件□ 管理服务和工具组件□ SQL接口组件□ 查询分析器组件□ 优化器组件□ 缓冲（Cache）组件□ 插件式存储引擎□ 物理文件 innodb架构 执行流程图https://juejin.im/post/5c6ece80f265da2de7134d86 MySQL客户端通过协议将SQL语句发送给MySQL服务器。 服务器会先检查查询缓存中是否有执行过这条SQL，如果命中缓存，则将结果返回，否则进入下一个环节（查询缓存默认不开启）。 服务器端进行SQL解析，预处理，然后由查询优化器生成对应的执行计划。 服务器根据查询优化器给出的执行计划，再调用存储引擎的API执行查询。 将结果返回给客户端，如果开启查询缓存，则会备份一份到查询缓存中。 innodb内存结构 后台线程 内存池 物理文件 内存池https://dev.mysql.com/doc/refman/5.7/en/innodb-buffer-pool.html 物理存储结构MySQL二进制文件的格式STATEMENT：逻辑SQL语句 ROW：记录表的行更改情况 MIXED：混合 主要是statement 部分row 表空间文件ibdata 也可以单表一个表空间文件（独立表空间文件），.idb，存该表的数据、索引、插入缓冲bitmap等，其他数据还是在总的表空间文件里 重做日志关于每个页（Page）的更改的物理情况 从重做日志缓冲往磁盘写入时，是按512个字节，也就是一个扇区的大小进行写入。因为扇区是写入的最小单位，因此可以保证写入必定是成功的。因此在重做日志的写入过程中不需要有doublewrite。 从表3-2可以看到重做日志条目是由4个部分组成： redo_log_type占用1字节，表示重做日志的类型 space表示表空间的ID，但采用压缩的方式，因此占用的空间可能小于4字节 page_no表示页的偏移量，同样采用压缩的方式 redo_log_body表示每个重做日志的数据部分，恢复时需要调用相应的函数进行解析 LSN（log sequence number， 表示从日志记录创建开始到特定的日志记录已经写入的字节数） http://mysql.taobao.org/monthly/2017/09/07/ innodb物理逻辑存储结构索引组织表在InnoDB存储引擎表中，每张表都有个主键（Primary Key），如果在创建表时没有显式地定义主键，则InnoDB存储引擎会按如下方式选择或创建主键： 首先判断表中是否有非空的唯一索引（Unique NOT NULL），如果有，则该列即为主键。 如果不符合上述条件，InnoDB存储引擎自动创建一个6字节大小的指针。 主键的选择根据的是定义索引的顺序，而不是建表时列的顺序。 InnoDB逻辑存储结构表空间tablespace，段segment，区extent，页page，行row 常见的段有数据段、索引段、回滚段等 任何情况下，每个区的大小都为1MB 每个页的大小为16KB，即一个区有64个页 InnoDB 1.0.x版本开始引入压缩页，即每个页的大小可以通过参数KEY_BLOCK_SIZE设置为2K、4K、8K，因此每个区对应页的数量就应该为512、256、128。 InnoDB 1.2.x版本新增了参数innodb_page_size，通过该参数可以将默认页的大小设置为4K、8K，但是页中的数据库不是压缩。这时区中页的数量同样也为256、128。总之，不论页的大小怎么变化，区的大小总是为1M。 表空间共享表空间 可以有独立表空间 innodb_file_per_table=ON 段数据段、索引段、回滚段等 数据即索引，索引即数据 数据段即为B+树的叶子节点（图4-1的Leaf node segment），索引段即为B+树的非索引节点（图4-1的Non-leaf node segment） 区大小为1M 1.0.x压缩页，KEY_BLOCK_SIZE设置页为2K、4K、8K，对应页数512、256、128 1.2.x非压缩页的页大小设置，innodb_page_size设置为4K、8K，页数为256、128. 页在InnoDB存储引擎中，常见的页类型有： 数据页（B-tree Node） undo页（undo Log Page） 系统页（System Page） 事务数据页（Transaction system Page） 插入缓冲位图页（Insert Buffer Bitmap） 插入缓冲空闲列表页（Insert Buffer Free List） 未压缩的二进制大对象页（Uncompressed BLOB Page） 压缩的二进制大对象页（compressed BLOB Page） 行每个页最多允许存放16KB/2-100行的记录，即7992行记录 – 没看懂 数据页结构 – 再看吧https://mp.weixin.qq.com/s?__biz=MzIxNTQ3NDMzMw==&amp;mid=2247483678&amp;idx=1&amp;sn=913780d42e7a81fd3f9b747da4fba8ec&amp;chksm=979688eca0e101fa0913c3d2e6107dfa3a6c151a075c8d68ab3f44c7c364d9510f9e1179d94d&amp;scene=21#wechat_redirect 索引索引的实现B+树B+树的高度一般在2~4层，查找某一键值的行记录时最多只需要2~4次的IO，查询时间在0.02~0.04秒（当前一般的机械磁盘每秒至少可以做100次IO）。 CardinalityCardinality/n_rows_in_table应尽可能地接近1 Cardinality值非常关键，表示索引中不重复记录数量的预估值 联合索引的使用 最左匹配原则 覆盖索引，即从辅助索引中就可以得到查询的记录（而不需要查询聚集索引中的记录，减少大量的IO操作） hash索引innodb有自适应哈希索引，是数据库自身创建的，DBA不能干预 冲突机制：链表方式 哈希函数：除法散列 h(k)=k mod m 全文检索 – 不会，pass倒排索引 inverted index 在辅助表（Auxiliary Table）中存储了单词与单词自身在一个或多个文档中所在位置之间的映射。 通常利用关联数组实现。 两种表现形式： inverted file index，其表现形式为{单词，单词所在文档的ID} full inverted index，其表现形式为{单词，（单词所在文档的ID，在具体文档中的位置）} innodb采用full inverted index。 全文检索通过MATCH函数进行查询，查询带有指定word的文档。 explain查询计划分析看下面的问答 锁和事务innodb存储引擎中的锁锁的类型行级锁 共享锁 S Lock：允许事务读一行数据 排他锁 X Lock：允许事务删除或更新一行数据 表级锁 意向共享锁 IS Lock：事务想要获得一张表中某几行的共享锁 意向排他锁 IX Lock：事务想要获得一张表中某几行的排他锁 InnoDB存储引擎支持的是行级别的锁，因此意向锁其实不会阻塞除全表扫以外的任何请求。 Record Lock: 单个行记录上的锁 Gap Lock: 间隙锁，锁定一个范围，但不包含记录本身 Next-Key Lock：Gap Lock + Record Lock，锁定一个范围，并锁定记录本身，邻键锁。解决幻读。锁定范围如(10,11]，前开后闭。 A next-key lock is a combination of a record lock on the index record and a gap lock on the gap before the index record. PS1: MySQL官方定义的不可重复读包括不可重复读和幻读，都可以通过Next-Key Lock来解决，即设定为RR隔离级别。 PS2: 还有previous-key lock，锁定范围如[10,11)，前闭后开。 当查询索引含有唯一值时，存储引擎会对next-key lock做优化，降级为record lock. next-key lock加锁范围的示例123456CREATE TABLE `lock_test` ( `id` bigint(20) NOT NULL, `value` varchar(255) NOT NULL, PRIMARY KEY (`id`), KEY `value_index` (`value`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='锁测试表' 这篇文章说的比较清楚，需要考虑主键的位置 首先是对应记录本身加锁，即j，其次前后记录都会加间隙锁，可以理解为(g,m)，新插入的记录如果索引也是g或m，就看主键在原g或m记录的前面还是后面。 (29,’g’) 成功 (31,’g’) 阻塞 (49,’m’) 阻塞 (51,’m’) 成功 一致性非锁定读是指innodb存储引擎通过行多版本控制的方式来读取当前执行时间数据库中行的数据。 如果读取的行正在执行delete或update操作，读取操作不会因此去等待行上锁的释放。相反，innodb存储引擎会去读取行的一个快照数据。 之所以称为非锁定读，是因为不需要等待访问的行上X锁的释放。 快照数据是通过undo段来完成。 undo用来在事务中回滚数据。 读快照数据是不需要上锁的，因为没有事务需要对历史的数据进行修改操作。 多版本并发控制multi version concurrency control, MVCC RC: Read Committed 读已提交 RR: Repeatable Read 可重复读 innodb默认隔离级别 RC和RR都使用非锁定一致性读 RC：总是读取被锁定行的最新一份快照数据 RR：总是读取事务开始时的那份快照 一致性锁定读12select .. for updateselect .. in share mode 设置隔离级别123select @@global.transaction_isolation, @@transaction_isolation;set global transaction isolation level read committed;set session transaction isolation level read committed; 锁的算法有两个索引，要分别加锁 插入意向锁是一种gap lock，insert操作在插入行之前加的锁 锁问题脏读：一个事务可以读到另一个事务未提交的数据。发生在隔离级别Read Uncommitted下。 不可重复读：两次读取数据值不一样。在隔离级别Read Committed下。 幻读：第二次读到了之前不存在的行。 还有丢失更新，一般在当前数据库的任何隔离级别下都不会导致丢失更新。因为操作在事务里，而事务里的更新操作是会加排他锁的，另一个事务里的更新操作会阻塞。只有不在事务里的操作才可能引起“丢失更新”.. 阻塞InnoDB存储引擎在大部分情况下都不会对异常进行回滚。 等待锁超时会抛出异常，默认50秒。 innodb_lock_wait_timeout 死锁死锁和解决方案指两个或两个以上的事务再执行过程中，因争夺锁资源而造成的一种互相等待的现象。（若无外力作用，事务都将无法推进下去） 解决方式： 超时机制，将超时的一个进行回滚，另一个等待的就可以进行下去。 等待图。进行死锁检测。 死锁的示例 AB-BA死锁，A等待B，B在等待A 当前事务持有了待插入记录的下一个记录的X锁，但是在等待队列中存在一个S锁的请求，则可能会发生死锁 其他一个SQL用的是非聚集索引，那么对非聚集索引加锁，也会对聚集索引加锁。 如果一个加锁SQL没有对应的索引，那会对整张表（中的记录）进行加锁 事务的实现原子性、持久性是通过redo log实现的 （重做日志，属于物理日志，记录页的物理修改） 一致性通过undo log实现 （逻辑日志，记录行） redo恢复提交事务修改的页操作 undo回滚记录到某个特定版本 redo两部分组成，内存中的重做日志缓冲，磁盘的重做日志文件。 与二进制日志的区别：二进制日志在事务提交完成后进行一次写入。重做日志在事务进行中不断地被写入（不是按事务提交的顺序进行写入的）。 重做日志以块（block）的方式进行保存，块大小为512字节，和磁盘扇区大小一样，因此写入可以保证原子性，不需要doublewrite技术。 日志块由三部分组成，重做日志块512字节，日志块头12字节，尾8字节。 redo块里有LSN编号，log sequence number， checkpoint表示已刷新到磁盘上的LSN。奔溃恢复的时候，只需恢复redo log当中比checkpoint大的那些LSN所在的页数据。 Log sequence number表示当前的LSN，Log flushed up to表示刷新到重做日志文件的LSN，Last checkpoint at表示刷新到磁盘的LSN。 undoredo通常是物理日志，记录的是页的物理修改操作。 undo是逻辑日志，根据每行记录进行记录。——这里的逻辑日志和物理日志，可以这么理解：物理日志记录一整页的修改，不区分行；逻辑日志，不等于是一个反向的sql语句，而是一行数据、每个列的值，等等。 性能分析慢查询日志分析何时开启慢查询日志 慢查询的时间设置多长 分析慢查询日志的工具 查询计划分析profile查询性能分析性能优化服务器层面 增加缓冲池大小innodb_buffer_pool_size 内存预热——不太了解 增加重做日志大小innodb_log_file_size 使用SSD磁盘提高读写速度 库表设计层面 中间表 冗余字段 字段拆分 数据拆分：分库分表 SQL层面 正确建立索引和使用索引 limit优化 补充插入缓冲 应用于非唯一的非聚集索引 属于buffer pool的一部分（所以也需要double write） 对于非聚集索引的插入或者更新操作，不是每一次都直接写入索引页中，而是先判断非聚集索引页是否在缓存中，如果在缓存中则直接插入，否则先放入一个insert buffer对象中。 再以一定的频率对insert buffer和非聚集索引页做merge操作，通常可以将多个插入操作合并成一个插入操作（针对同一个索引页），提高了非聚集索引的插入性能。 redo log重做日志 物理逻辑日志（非纯物理） lsn 当前lsn，写入重做日志文件lsn，checkpoint 刷新到磁盘数据页的lsn doublewrite 应用于buffer pool的数据页写入磁盘 两部分组成： 内存中两次写缓存，2MB； 磁盘上的两次写文件，2MB（两个区） doublewrite每次写1M，in a large sequential chunk，再马上调用fsync，所以wr中不会有部分写失败的问题（？）。回答：也是有可能失败的，分两种情况分析： doublewrite缓冲写入磁盘失败，表示脏页没有刷到磁盘，这时候数据库进行恢复，会使用redo log及现有磁盘数据重新计算出正确的buffer pool，再进行double write的流程 doublewrite缓冲写入磁盘成功，脏页写磁盘时断电，那么磁盘数据可能有损了，恢复的时候根据doublewrite文件先恢复磁盘数据，再根据redo log和恢复后的磁盘数据恢复buffer pool（怎么理解页数据有损？比如redo日志是 偏移量2的位置写入aaa，这是可以重复写入的，但可能，偏移量10的地方是异常时写入一半的数据，整体上是不对的） MySql查询中哪些情况不会使用索引？ 使用or like以”%xx”开始匹配 联合（复合）索引，不符合最左匹配 索引列数据类型隐形转换，比如列是字符串，但用数值来查询就用不上索引 在where子句中，对索引列有数学运算、或者使用函数，用不了索引 MySQL估计全表扫描比查询索引快时（比如数据量非常少） MYSQL 索引类型、什么情况下用不上索引、什么情况下不推荐使用索引 MySQL性能优化的最佳21条经验 – 没大用 mysql explain执行计划详解 – 有错字之类的 type: const 命中唯一索引或主键的时候 数据库隔离级别 读未提交 读提交 可重复读 可串行化 数据库索引，底层是怎样实现的，为什么要用B+树索引？MySQL底层使用B+树实现的。 MyISAM引擎，B+树主索引、辅助索引叶节点是数据记录的地址，称为非聚集索引（与InnoDB区分） InnoDB的主键索引是聚集索引，叶节点存的完整的数据记录；辅助索引，叶节点存的是主键的值。 为什么用B+树索引？ 数据文件比较大，一般存储在磁盘上 索引的组织结构要尽量减少查找过程中磁盘IO次数。 数据库系统利用磁盘预读原理，将一个节点的大小设为一个页的大小，则只需要一次IO就可以将一个节点的数据都读入 B+树只有叶子节点存放数据，非叶子节点作为索引，这样树出度大，树高小，一般3层，查询目标数据的io次数比较少，效率高。 使用节点大小正好等于磁盘一页大小的B+树，可以减少io操作次数，提高查询效率。 从数组、哈希表、二叉树等数据结构的对比来回答，见下面这篇文章 MySQL为什么不用数组、哈希表、二叉树等数据结构作为索引呢 orderby底层执行过程 Mysql主从同步的实现原理？原理：在主库上记录二进制日志，在备库重放日志的方式实现异步数据复制。 复制有三个步骤： 主库记录二进制日志，每次准备提交事务（完成数据库更新）前先记录二进制日志（记录日志完后，再执行数据库更新） 备库将主库的二进制文件复制到本地的中继日志中。 备库会启动一个工作线程，称为IO工作线程，负责和主库建立一个普通的客户端连接 如果该进程追赶上了主库，它将进入睡眠状态，直到主库有新的事件产生，会被唤醒，将接收到的事件记录到中继日志中 备库的SQL线程读取中继日志并在备库执行 中继日志一般在系统缓存中，开销低，也可以根据配置选项来决定是否写入自己的二进制日志中 常见复制架构： 一主多从 主主 环型复制 MySQL复制详解 MySQL是怎么用B+树？innodb引擎用B+树当索引，索引文件同时是数据文件。聚集索引，也就是主键索引，叶节点存储的完整行数据；辅助索引，也称为非聚集索引，叶节点存对应行记录的主键。 MyISAM引擎也是用B+树当索引，为非聚集索引，索引不是数据文件，叶节点存的是行记录的地址。 谈谈数据库乐观锁与悲观锁？ 悲观锁，认为操作会发生冲突，提前加锁，直到自己操作结束再释放锁。 MySQL的显式锁定 写锁 select .. for update &amp; 读锁 select .. lock in share mode 乐观锁，认为不会发生冲突，在提交更新的时候会判断一下期间数据有没有被修改。类似于CAS操作，常用方式有版本号、时间戳。 mvcc，怎么实现rr rc todomvcc利用了数据页的快照，rc总是读取最新的一份快照，rr读取的截止本事务开始前的那些快照。 mysql间隙锁有没有了解，死锁有没有了解，写一段会造成死锁的sql语句，死锁发生了如何解决，mysql有没有提供什么机制去解决死锁其实这些称为锁的算法。 record lock，记录锁，锁住的是索引记录本身。（总是锁住索引记录，即使表没有建索引，innodb也会创建一个隐藏的聚集索引，） gap lock，间隙锁，锁住的索引记录的范围区间，不包含记录本身。不同事务可以持有同一区间的间隙锁，间隙锁的作用只是用来阻止插入。共享间隙锁和排他间隙锁没有区别。在RC隔离级别下，会关闭索引的（disabled for searches and index scans）间隙锁，只用于外键约束检查和重复主键检查。 Gap locks in InnoDB are “purely inhibitive”, which means that their only purpose is to prevent other transactions from inserting to the gap. https://dev.mysql.com/doc/refman/8.0/en/innodb-locking.html next-key lock，临键锁，锁住范围，并锁住下一个记录。 关于插入意向锁的补充： insert intention lock，插入意向锁，是insert操作产生的一种间隙锁。不同事务要插入同一个索引区间，只要他们的不插入同一个数据位置，就不用互相等待。（插入意向锁之间，如果不是插入同一个位置，不用等待；一个插入意向锁和一个普通间隙锁，是会等待的，因为普通间隙就是为了防止在间隙中插入） This lock signals the intent to insert in such a way that multiple transactions inserting into the same index gap need not wait for each other if they are not inserting at the same position within the gap. Suppose that there are index records with values of 4 and 7. Separate transactions that attempt to insert values of 5 and 6, respectively, each lock the gap between 4 and 7 with insert intention locks prior to obtaining the exclusive lock on the inserted row, but do not block each other because the rows are nonconflicting. 事务T1： 12345678910mysql&gt; CREATE TABLE child (id int(11) NOT NULL, PRIMARY KEY(id)) ENGINE=InnoDB;mysql&gt; INSERT INTO child (id) values (90),(102);mysql&gt; START TRANSACTION;mysql&gt; SELECT * FROM child WHERE id &gt; 100 FOR UPDATE;+-----+| id |+-----+| 102 |+-----+ 事务T2，T2会等待锁： 12mysql&gt; START TRANSACTION;mysql&gt; INSERT INTO child (id) VALUES (101); 关于临键锁的补充： That is, a next-key lock is an index-record lock plus a gap lock on the gap preceding the index record. If one session has a shared or exclusive lock on record R in an index, another session cannot insert a new index record in the gap immediately before R in the index order. When InnoDB scans an index, it can also lock the gap after the last record in the index. https://dev.mysql.com/doc/refman/8.0/en/innodb-next-key-locking.html 首先，next-key lock是记录本身+与前一个记录的范围。 其次，在innodb加锁时，会将记录与下一个记录之前的范围也加锁。 简单说，R+R之前范围+R之后范围都回加锁。 MySQL几种常用的存储引擎区别InnoDB与MyISAM比较典型的几个区别： innodb支持事务、MVCC快照读、行级锁粒度、hash索引、聚集索引、支持外键 myisam支持全文索引、空间索引、数据压缩 innodb存储成本高、内存成本高、插入速度低，myisam反过来 来源：MySQL技术内幕 explain 可以看到哪些信息，什么信息说明什么，explain的结果列讲一下https://dev.mysql.com/doc/refman/8.0/en/explain-output.html Column JSON Name Meaning id select_id The SELECT identifier select标识 select_type None The SELECT type select类型 table table_name The table for the output row 表名 partitions partitions The matching partitions 使用的分区 type access_type The join type join类型 possible_keys possible_keys The possible indexes to choose 可能使用的索引 key key The index actually chosen 实际使用的索引 key_len key_length The length of the chosen key 实际使用的索引的长度 ref ref The columns compared to the index 与索引进行对比的列 rows rows Estimate of rows to be examined 预估要检查的行数 filtered filtered Percentage of rows filtered by table condition 符合条件的数据的百分比 Extra None Additional information 额外的信息 select_type 常见的有SIMPLE（简单查询，无union、subqueries）、PRIMARY（子查询的外层）、SUBQUERY、UNION等 type system：表中只有一行数据，const的特殊情况 const：至多有一行matching，可以理解为主键或唯一索引的= （单表，对tbl_name来说，1是const） 1234SELECT * FROM tbl_name WHERE primary_key=1;SELECT * FROM tbl_name WHERE primary_key_part1=1 AND primary_key_part2=2; eq_ref：主键或唯一索引的= （多表关联，other_table的结果不定，所以对ref_table来说，选择不是const） 123456SELECT * FROM ref_table,other_table WHERE ref_table.key_column=other_table.column;SELECT * FROM ref_table,other_table WHERE ref_table.key_column_part1=other_table.column AND ref_table.key_column_part2=1; ref：（非主键与非唯一索引的）其他索引的= 12345678SELECT * FROM ref_table WHERE key_column=expr;SELECT * FROM ref_table,other_table WHERE ref_table.key_column=other_table.column;SELECT * FROM ref_table,other_table WHERE ref_table.key_column_part1=other_table.column AND ref_table.key_column_part2=1; fulltext 用到了全文索引 ref_or_null 类似ref，会额外检索包含null的行 index_merge 用到了多个索引，索引合并优化 unique_subquery 替换下面的in子查询，子查询返回不重复的集合 1value IN (SELECT primary_key FROM single_table WHERE some_expr) index_subquery 区别于unique_subquery，用于非唯一索引，可以返回重复值 1value IN (SELECT key_column FROM single_table WHERE some_expr) range 索引范围查找，包括主键、唯一索引、其他索引——即，所有key =, &lt;&gt;, &gt;, &gt;=, &lt;, &lt;=, IS NULL, &lt;=&gt;, BETWEEN, LIKE, or IN() 1234567891011SELECT * FROM tbl_name WHERE key_column = 10;SELECT * FROM tbl_name WHERE key_column BETWEEN 10 and 20;SELECT * FROM tbl_name WHERE key_column IN (10,20,30);SELECT * FROM tbl_name WHERE key_part1 = 10 AND key_part2 IN (10,20,30); index 类似all，但是只扫描索引，有两种情况 覆盖索引，select中的列都在索引中，extra中显示using index 利用索引的顺序进行全表扫描（比如有order by），extra中不显示using index all 全表扫描 rows和filtered rows：MySQL认为需要检查的行数 filtered：rows中会被过滤出来的——即符合条件的——的数据的百分比 rows*filtered=查询出的结果数 extra 常见的有 using index 列信息只从索引出，不用再从实际行取。使用了覆盖索引 using where 没有可用的索引，通过where条件过滤 using filesort 需要额外排序 Using index condition Index Condition Pushdown (ICP) 简单说，就是在索引里进行where过滤。比如 like ‘%xx’本来是用不到索引的，如果开了ICP，则可以用这个条件在索引上做过滤，筛出更小的一部分数据，减少IO。参考文章 ….还有好多 group by 和 order by不能利用索引的有序性的话，需要额外进行排序，using filesort group by，先排序后分组 distinct基于group by 排序算法：双路排序（两次IO，第一次IO后用rowid+排序关键字进行排序，排完序后根据rowid再读一次数据）和单路排序（一次IO，查询的列和排序关键字一起在内存中进行排序，空间换时间） https://blog.csdn.net/wuseyukui/article/details/72627667 https://blog.csdn.net/qq_37113604/article/details/88973260 索引优化 todo 看看高性能书，有硬件层面和开发层面？https://juejin.im/post/5b68e3636fb9a04fd343ba99#heading-3 如果MySQL评估使用索引比全表扫描还慢，则不会使用索引 前导模糊查询（like ‘%xx’）不会使用索引，可以优化为非前导模糊查询（like ‘xx%’） 数据类型出现隐式转换的时候不会命中索引，特别是当列类型是字符串，一定要将字符常量值用引号引起来 复合索引，要满足最左匹配原则 union、in、or 都能够命中索引，建议使用 in 查询的CPU消耗：or (id=1 or id=2)&gt; in (id in (1,2)) &gt;union(id = 1 union id = 2) 用or分割开的条件，如果or前的条件中列有索引，而后面的列中没有索引，那么涉及到的索引都不会被用到 因为or后面的条件列中没有索引，那么后面的查询肯定要走全表扫描，在存在全表扫描的情况下，就没有必要多一次索引扫描增加IO访问。 负向条件查询不能使用索引，可以优化为 in 查询 负向条件有：!=、&lt;&gt;、not in、not exists、not like 等。 范围条件查询可以命中索引 范围条件有：&lt;、&lt;=、&gt;、&gt;=、between等（返回数据的比例超过30%，会不使用索引） 查询条件（带有计算函数）执行计算不会命中索引 利用覆盖索引进行查询，避免回表 建议索引的列设置为非null 更新十分频繁的字段上不宜建立索引 区分度不大的字段上不宜建立索引 业务上具有唯一特性的字段，建议建立唯一索引 多表关联时，关联字段建议有索引 创建索引时避免以下错误观念 索引越多越好，认为一个查询就需要建一个索引。 宁缺勿滥，认为索引会消耗空间、严重拖慢更新和新增速度。 抵制唯一索引，认为业务的唯一性一律需要在应用层通过“先查后插”方式解决。 过早优化，在不了解系统的情况下就开始优化。 其他数据库有使用过哪些NoSQL数据库？MongoDB和Redis适用哪些场景？工程中用过Redis，主要是小部分数据的缓存 其他不太了解 NoSql not only sql 非关系型数据库 memcache、redis、mongoDB 如何选择？ Redis和memcache有什么区别？Redis为什么比memcache有优势？不太了解 考虑redis的时候，有没有考虑容量？大概数据量会有多少？没有，公司维护的Redis组件 – redis &amp; nosql 需要再深入一点呀 Redis的缓存淘汰策略、更新策略 过期策略 定期删除：默认每隔100ms随机抽取一些设置了过期时间的key，检查是否过期，如果过期就删除（因为全表扫描非常耗时、耗性能，所以是随机，也因此要配合惰性删除） 惰性删除：在客户端要获取某个key时，判断key是否设置过期以及是否过期，如果过期先删除 内存淘汰策略 Redis在使用内存达到某个阈值（通过maxmemory配置)的时候，就会触发内存淘汰机制，选取一些key来删除。 12# maxmemory &lt;bytes&gt; 配置内存阈值# maxmemory-policy noeviction noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。默认策略 allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key。 allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个key。 volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key。 volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key。 volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先移除。 如何选取合适的策略？比较推荐的是两种lru策略。根据自己的业务需求。如果你使用Redis只是作为缓存，不作为DB持久化，那推荐选择allkeys-lru；如果你使用Redis同时用于缓存和数据持久化，那推荐选择volatile-lru。 redis过期策略和内存淘汰策略 java实现LRU redis的数据结构Redis基础 Redis 键值（Key-Value）存储数据库 string 字符类型 map 散列类型 list 列表类型 set 集合类型 sortedset 有序集合类型 redis如何实现分布式锁，zk如何实现分布式锁，两者的区别。如果service还没执行完，分布式锁在redis中已经过期了，怎么解决这种问题redis实现分布式锁：setNX，创建成功表明获得了锁（要注意设置超时、谁加锁谁解锁、解锁的原子性） zk实现分布式锁：在路径下创建临时顺序节点，序号最小的节点表示获得了锁，其他竞争者监听自己的前一个节点 redisson给的答案是锁获取成功后，注册一个定时任务，每隔一定时间(this.internalLockLeaseTime / 3L, 10s)就去续约 加一个监听器，如果key快要超时了，就进行续约（重置成30s）","tags":[]},{"title":"算法","date":"2020-07-27T16:29:48.000Z","path":"2020/07/28/technology/07算法/","text":"算法海量数据5亿整数的大文件，怎么排？内存估算 假设每个数最多64位，8字节 5,0000,0000x8 ~ 500MBx8 = 4000MB ~ 4G 假设5亿数不重复 内存装的下： 直接快排，得算好久吧.. 5亿个整数排序 内存装不下： 读文件，数取模%4000存入4000个小文件，每个文件约1M 读每个小文件，快排 多路归并排序输出 海量数据处理思路 分治/hash映射 + hash统计 + 堆/快排/归并排序 hash分成n个小文件，满足内存要求：好处是，可以让相同的数或字符串进入同一个小文件 小文件排序或统计，或没有本步骤，输出另一份小文件 最终要求 全排序：使用多路归并 找top k：直接小顶堆（找最大top k）or大顶堆；或者每个小文件先找top k，再对比n个top k 找两文件不同：两两小文件set对比 数据结构 bitmap 可用于整数去重等 trie树 名字来源retrieval 1亿个手机号码，判断重复不允许有误差的： hash到n个小文件中 每个文件做统计 个数大于1的是重复的 允许有误差的： 布隆过滤器 排序常见的排序算法 冒泡排序-复杂度O(n^2)-交换排序 对所有相邻记录的关键码值进行比较，如果是逆序（L.r[1].key &gt; L.r[2].key），则将其交换，最终达到有序化。 对无序区从前向后依次将相邻记录的关键码进行比较，若逆序，则将其交换，从而使得关键码值小的记录向上“飘浮”（左移），关键码值大的记录向下“坠落”（右移）。 每经过一趟冒泡排序，都使无序区中关键码值最大的记录进入有序区，对于由n个记录组成的记录序列，最多经过n-1趟冒泡排序，就可将这n个记录重新按关键码顺序排列。可看出，若“在一趟排序过程中没有进行过交换记录的操作”，则可结束整个排序过程。 123456789101112131415161718192021/** * 冒泡排序--更像坠落排序 * * @param nums */@Overridepublic void sort(T[] nums) &#123; int len = nums.length; boolean isSorted = false; // i区分无序区和有序区 for (int i = len - 1; i &gt;= 0 &amp;&amp; !isSorted; i--) &#123; isSorted = true; // j将大元素右移 for (int j = 0; j &lt; i; i++) &#123; if (less(nums[j + 1], nums[j])) &#123; isSorted = false; swap(nums, j, j + 1); &#125; &#125; &#125;&#125; 选择排序-复杂度O(n^2)-选择排序 每一趟从待排序的记录中选出关键码最小的记录，顺序放在已排好序的子序列后面，直到全部记录排序完毕。 123456789101112131415161718/** * 选择排序 * * @param nums */@Overridepublic void sort(T[] nums) &#123; for (int i = 0; i &lt; nums.length; i++) &#123; // index指向每轮最小的数 int index = i; for (int j = i + 1; j &lt; nums.length; j++) &#123; if (less(nums[j], nums[index])) &#123; index = j; &#125; &#125; swap(nums, i, index); &#125;&#125; 插入排序-复杂度O(n^2) -插入排序 基本思想是，将待排序的记录，按其关键码的大小插入到已经排好序的有序子表中，直到全部记录插入完成为止。 12345678910111213/** * 插入排序 * * @param nums */@Overridepublic void sort(T[] nums) &#123; for (int i = 1; i &lt; nums.length; i++) &#123; for (int j = i; j &gt; 0 &amp;&amp; less(nums[j], nums[j - 1]); j--) &#123; swap(nums, j, j - 1); &#125; &#125;&#125; 归并排序-复杂度O(nlogn)-归并排序 1234567891011121314151617181920212223242526272829303132333435363738/** * 归并排序 * * @param nums */public void sort(T[] nums, Class&lt;T&gt; clazz) &#123; T[] copy = (T[]) Array.newInstance(clazz, nums.length); System.arraycopy(nums, 0, copy, 0, nums.length); sort(nums, copy, 0, nums.length);&#125; private void sort(T[] nums, T[] copy, int begin, int end) &#123; if (begin + 1 == end) &#123; return; &#125; int half = (end - begin) / 2; sort(nums, copy, begin, begin + half); sort(nums, copy, begin + half, end); merge(copy, nums, begin, begin + half, end);&#125; private void merge(T[] nums, T[] copy, int begin, int mid, int end) &#123; int i = begin, j = mid, k = begin; while (i &lt; mid &amp;&amp; j &lt; end) &#123; if (nums[i].compareTo(nums[j]) &lt; 0) &#123; copy[k] = nums[i++]; &#125; else &#123; copy[k] = nums[j++]; &#125; k++; &#125; while (i &lt; mid) &#123; copy[k++] = nums[i++]; &#125; while (j &lt; end) &#123; copy[k++] = nums[j++]; &#125;&#125; 快速排序-复杂度O(nlogn)-交换排序 1234567891011121314151617181920212223242526272829/** * 快速排序 * * @param nums */@Overridepublic void sort(T[] nums) &#123; sort(nums, 0, nums.length - 1);&#125; private void sort(T[] nums, int begin, int end) &#123; int left = begin + 1, right = end; while (left &lt; right) &#123; while (left &lt;= end &amp;&amp; less(nums[left], nums[begin])) &#123; left++; &#125; while (right &gt;= begin &amp;&amp; less(nums[begin], nums[right])) &#123; right--; &#125; if (left &lt; right) &#123; swap(nums, left, right); &#125; &#125; if (right &lt;= end &amp;&amp; right &gt;= begin) &#123; swap(nums, begin, right); sort(nums, begin, right - 1); sort(nums, right + 1, end); &#125;&#125; 堆排序-复杂度O(nlogn)-堆排序 位置 k 的节点的父节点位置 为 k/2，而它的两个子节点的位置分别为 2k 和 2k+1。 12345678910111213141516171819202122232425262728293031323334353637383940/** * 堆排序 排成最大堆 * 数组第 0 个位置不能有元素 * * @param nums */@Overridepublic void sort(T[] nums) &#123; int cnt = nums.length - 1; for (int k = cnt / 2; k &gt;= 1; k--) &#123; sink(nums, k, cnt); &#125; while (cnt &gt; 1) &#123; swap(nums, 1, cnt); cnt--; sink(nums, 1, cnt); &#125;&#125; /** * 下沉 * * @param nums * @param k */private void sink(T[] nums, int k, int len) &#123; while (k * 2 &lt;= len) &#123; int child = k * 2; // 判断child + 1未越界 if (child + 1 &lt; len &amp;&amp; less(nums[child], nums[child + 1])) &#123; child++; &#125; // 如果子节点比k小，退出循环 if (less(nums[child], nums[k])) &#123; break; &#125; swap(nums, k, child); k = child; &#125;&#125; https://book.open-falcon.org/zh_0_2/intro/)","tags":[]},{"title":"面向对象","date":"2020-07-27T16:29:48.000Z","path":"2020/07/28/technology/04面向对象/","text":"设计模式http://c.biancheng.net/view/1317.html First of all，组合一般优于继承。 创建模式工厂方法是类创建型，其他属于对象创建型。 单例模式Singleton饿汉模式，懒汉模式（volatile + synchronized + double check） 原型模式Prototype例子：Jave的clone() 简单工厂模式 简单工厂模式的工厂类一般是使用静态方法，通过接收的参数的不同来返回不同的对象实例。不修改代码的话，是无法扩展的。 不属于GoF，会违背开闭原则 创建产品的逻辑在一个方法里，增加产品要修改代码 工厂方法模式FactoryMethod 工厂方法是针对每一种产品提供一个工厂类。通过不同的工厂实例来创建不同的产品实例。 在同一等级结构中，支持增加任意产品。 是对简单工厂的抽象，不同产品有对应的工厂。只要知道工厂，就能得到产品。 例子：Spring的FactoryBean 例子1：java.sql的Connection DriverManager是这个抽象工厂，Connection是不同的产品的接口，Driven是不同产品的接口。 12Connection con = DriverManager.getConnection( \"jdbc:mysql://127.0.0.1:3306/test\", \"root\", \"root\"); 抽象工厂模式AbstractFactory 抽象工厂是应对产品族概念的。比如说，每个汽车公司可能要同时生产轿车，货车，客车，那么每一个工厂都要有创建轿车，货车和客车的方法。 增加汽车公司容易，增加车的类型难？——增加车的类型得改所有的汽车公司.. 建造者模式Builder例子：lombok的@Builder 注重零部件的组装过程，工厂方法模式注重零部件的创建过程，两者可以结合使用。 结构型模式将类或对象按某种布局组成更大的结构。 分为类结构型和对象结构型，前者继承机制，后者组合或聚合来组合对象。 适配器分为结构型和对象型，其他都是对象型。 代理模式Proxy为某对象提供一种代理，以控制对该对象的访问，限制、增强或修改该对象的一些特性。 例子：JDK动态代理 适配器模式Adapter将一个类的接口转换成客户希望的另外一个接口 例子：Gson的TypeAdapter，是json和Type之间的适配器 和策略一样，只不过意图不同，适配器根据策略返回对象，对象再继续执行不同的逻辑？ 桥接模式Bridge将抽象和实现分离，用组合关系代替继承关系 —— 很糊，不清楚 https://juejin.im/post/6844903919982739469 每个驱动程序都是适配器模式 (Adapter) 的一个例子, 而使用驱动程序的应用是桥接模式的一个例子. 桥接模式将应用的开发和驱动的开发分离开来. Bridge, State, Strategy (Adapter 在某种程度上也算) 有一些相似, 这些设计模式都采用了将功能代理给其它类去做的方式. 但是, 他们解决的问题并不相同. Abstract Factory 可以和 Bridge 一起使用（比如驱动程序的例子）, 通常用于 Abstraction 只能使用特定的 Implementation 的情况. 在这种情况下, 可以用 Abstract Factory 封装 Client 代码中建立 Abstraction 和 Implementation 连接的部分. 装饰模式Decorator增加额外的功能 —— 感觉和代理很像 https://zhuanlan.zhihu.com/p/97499017 让别人帮助你做你并不关心的事情，叫代理模式【代理模式强调要让别人帮你去做一些本身与你业务没有太多关系的职责（记录日志、设置缓存）。代理模式是为了实现对象的控制，因为被代理的对象往往难以直接获得或者是其内部不想暴露出来。】 为让自己的能力增强，使得增强后的自己能够使用更多的方法，拓展在自己基础之上的功能的，叫装饰器模式【增强后你还是你，只不过能力更强了而已】 外观模式Facade为多个复杂的子系统提供一个一直的接口，使子系统更容易被访问 简单理解：对内部复杂的逻辑进行封装，或者不想暴露太多 例子 https://blog.csdn.net/weixin_34416649/article/details/87998378 org.springframework.jdbc.support.JdbcUtils getResultSetValue org.apache.ibatis.session.Configuration的newExecutor等 Tomcat的getRequest()取到的是RequestFacade，RequestFacade是对HttpServletRequest的有限or更简单的访问 slf4j是简单的日志外观模式框架，可以有logback、log4j、Commons-logging和JDK自带的logging等各种实现 享元模式Flyweight运用共享技术，支持大量细粒度对象的复用 例子 https://juejin.im/post/6844903683860217864 String在jvm字符串常量中的复用 Integer中-128~127的复用 数据库连接池复用 组合模式Composite将对象组合成树状层次结构，使用户对单个对象和组合对象有一致的访问性。 例子 https://juejin.im/post/6844903687228407821 putAll 接收的参数为父类 Map 类型，所以 HashMap 是一个容器类，Map 的子类为叶子类，如果 Map 的其他子类也实现了 putAll 方法，那么它们都既是容器类，又都是叶子类 同理，ArrayList 中的 addAll(Collection&lt;? extends E&gt; c) 方法也是一个组合模式的应用 1234&gt; public void putAll(Map&lt;? extends K, ? extends V&gt; m) &#123;&gt; putMapEntries(m, true);&gt; &#125;&gt; 行为模式描述多个类或对象之间怎样协作完成单个对象无法单独完成的任务 分为类行为模式和对象行为模式，前者继承机制在类间分派行为，后者组合或聚合再对象间分配行为。 模板方法Template Method定义算法骨架，将算法具体步骤延迟到子类 使得子类在可以不改变该算法结构的情况下重定义该算法的某些特定步骤。 例子 https://juejin.im/post/6844903689103081479 Servlet的doGet、doPost等 mybatis的BaseExecutor。BaseExecutor 的子类有四个分别是 SimpleExecotor、ReuseExecutor、BatchExecutor、ClosedExecutor，由于这里使用了模板方法模式，一级缓存等固定不变的操作都封装到了 BaseExecutor 中，子类就不必再关心一级缓存等操作，只需要专注实现4个基本方法的实现即可。这四个方法分别是：doUpdate() 方法、doQuery() 方法、doQueryCursor() 方法、doFlushStatement() 方法，其余功能都在 BaseExecutor 中实现。 策略模式Strategy定义一系列算法，每个算法封装起来，可以相互替换 例子很多了，Gson里的TypeAdapter，用哪个Adapter也是策略吧 命令模式Command暂时没用过… 责任链模式Chain of Responsiblity 为了避免请求发送者与多个请求处理者耦合在一起，将所有请求的处理者通过前一对象记住其下一个对象的引用而连成一条链；当有请求发生时，可将请求沿着这条链传递，直到有对象处理它为止。 优点： 降低了对象之间的耦合度。 增强了系统的可扩展性。 增强了给对象指派职责的灵活性。 责任链简化了对象之间的连接。每个对象只需保持一个指向其后继者的引用，不需保持其他所有处理者的引用，这避免了使用众多的 if 或者 if···else 语句。 责任分担。每个类只需要处理自己该处理的工作，不该处理的传递给下一个对象完成，明确各类的责任范围，符合类的单一职责原则。 主要缺点： 不能保证每个请求一定被处理。由于一个请求没有明确的接收者，所以不能保证它一定会被处理，该请求可能一直传到链的末端都得不到处理。 对比较长的职责链，请求的处理可能涉及多个处理对象，系统性能将受到一定影响。 职责链建立的合理性要靠客户端来保证，增加了客户端的复杂性，可能会由于职责链的错误设置而导致系统出错，如可能会造成循环调用。 例子： Servlet的Filter等 netty的boundIn和boundOut handler 状态模式State https://alphagao.com/2017/08/07/stragety-pattern-vs-state-pattern/ 策略模式关注的焦点在于具体的某一个行为，准确的说是某一行为的具体执行过程。策略模式中具体行为策略的改变是由调用方主动指定的。 状态模式关注的焦点在于内部状态的改变而引起的行为的变化。不需要调用方干涉。 观察者模式Observer https://juejin.im/post/6844903697781096455 spring的事件机制是从java的事件机制拓展而来，ApplicationContext 中事件处理是由 ApplicationEvent 类和 ApplicationListener 接口来提供的。如果一个Bean实现了 ApplicationListener 接口，并且已经发布到容器中去，每次 ApplicationContext 发布一个 ApplicationEvent 事件，这个Bean就会接到通知 ApplicationContext：事件源，其中的 publishEvent()方法用于触发容器事件 ApplicationEvent：事件本身，自定义事件需要继承该类，可以用来传递数据 ApplicationListener：事件监听器接口，事件的业务逻辑封装在监听器里面 中介者模式Mediator不清楚 迭代器模式Iterator访问者模式Visitor备忘录模式Memento解释器模式Interpreter面向对象简单说一下面向对象的特征以及六大原则特征： 封装：把客观事物封装成抽象的类 抽象继承！：（实现继承or接口继承）让某个类型的对象获得另一个类型的对象的属性和方法 多态：一个类实例的相同方法在不同情形有不同的表现形式。（编译多态与运行时多态）一般指运行时多态..? 多态存在的必要条件：继承、重写、父类引用指向子类对象 原则： 单一职责：一个类的功能要单一，不能包罗万象 开放封闭：一个模块，在扩展性方面应该是开发的，在更改性方面是封闭的 里氏替换：子类应当可以替换父类，并出现在父类能够出现的任何地方 依赖倒置：高层次的模块不应该依赖于低层次的模块，他们都应该依赖于抽象；抽象不应该依赖于具体实现，具体实现应该依赖于抽象。 接口分离：模块间要通过抽象接口隔离开，而不是通过具体的类强耦合起来，即面向接口编程。 迪米特原则：一个类对自己依赖的类知道的越少越好。类间解耦，低耦合、高内聚。","tags":[]},{"title":"系统设计","date":"2020-07-27T16:29:48.000Z","path":"2020/07/28/technology/03系统设计/","text":"分布式谈谈分布式锁、以及分布式全局唯一ID的实现比较？分布式锁实现方式及比较 为什么需要分布式锁？ 效率：避免不同节点重复相同的工作，这些工作会浪费资源。比如针对一个操作发多封邮件。 正确性：避免破坏正确性的发生，比如多个节点操作了同一条数据，其中一个操作结果被覆盖了，造成数据丢失。 常见实现方式 数据库 表的一行数据表示一个资源，select..for update来加锁，可同时存节点信息，支持重入。 理解简单，但需要自己实现，以及维护超时、事务和异常处理等，性能局限于数据库，性能相对比较低，不适合高并发场景。 zookeeper curator封装 – 具体怎么用还没看过呢 Zk 排他锁和共享锁有区别。排他锁，利用zk有序节点，序号最小的节点表示获取到锁，其他未获取到锁的节点监听自己的前一个节点。 （共享锁，能获取到资源都算？回家再看看。）还有个读写锁，一个节点获取读锁，只要序号小于他的都为读锁，就表示获取到读锁；一个节点获取写锁，需要自己的序号最小，才表示获取到写锁。可重入锁之类的，zk节点写值吧，原理和Java的reentrantLock类似，获取多少次，state自加多少次，解锁再一次次自减，直到state为0表示完全释放。 （文章里说zk分布式锁和数据库mysql差不多。。真的么） redis setNX 自己参照一篇文章实现的比较简单，主要利用setNX，不支持重入，非公平。有超时释放，有加锁身份，解锁原子性 下面的文章里介绍的Redission。不太了解，加锁原子性lua脚本，用了hset，hashmap的结构，key是资源，value是锁定次数，可重入。。还有公平锁的实现。。。 分布式锁 分布式全局唯一ID/分布式ID生成器 实现方式及比较 uuid 缺点：长，占用空间大，间接导致数据库性能下降 缺点：不是有序的，导致索引在写的时候有过多的随机写 数据库自增主键 缺点：完全依赖于数据库，有性能瓶颈 缺点：不易扩展 snowflake Twitter，Scala实现的… 雪花算法，带有时间戳的全局唯一ID生成算法 固定ID格式： 12341位的时间戳（精确到毫秒，41位的长度可使用69年）10位的机器ID（10位长度最多支持1024个服务器节点部署）12位的计数序列号（12位支持每节点每毫秒最多生成4096个序列号） 分布式锁的实现方式，zk实现和Redis实现的比较实现方式：CAP的应用 MySQL唯一索引 实现：锁名称建立唯一索引，先插入数据的线程获得锁 缺点：完全依赖数据库的可用性（单点问题，无主从切换）和性能 Redis 实现：setnx key value expire_time 优缺点：为解决无主从切换的问题，可以使用Redis集群，或者sentinel哨兵模型。当master节点出现故障，哨兵从slave中选取节点称为新master节点。文章说，Redis集群的复制是AP模式，可能存在数据不一致，导致存在两个线程获得到锁的情况。（一个线程在原master获得锁，另一个线程在新master获得锁） 对数据一致性非常敏感的场景，建议使用CP模型（比如zk） zk 实现： 线程向zk的锁目录，申请创建有序的临时节点 如果建成的节点序号最小，表明获得到锁 如果序号非最小，监听自己的前一个节点 删除节点表示释放锁；当获取锁的客户端异常、无心跳，临时节点会被删除，也表示释放锁 优缺点：CP模式，zk的分布式锁比Redis的可靠，但Redis的性能更高。要根据自己的业务场景，再选型。 对一致性哈希的理解一致性哈希算法 求出各节点的哈希值，将其配置到0~2^32的圆（continuum）上 用同样方法求出存储数据的哈希值，映射到相同的圆上 从数据映射到的位置开始顺时针查找，将数据保存到找到的第一个节点上。如果超过2^32仍然找不到节点，就保存到第一个节点上 【如果添加一个节点node5，只会影响该节点的逆时针方向的第一个节点node4会受到影响（原来在node4上的数据要重新分配一些到node5上）】 一致性哈希算法对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性 在服务节点太少时，容易因节点分部不均匀而造成数据倾斜问题，可引入虚拟节点机制，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。同时数据定位算法不变，只是多了一步虚拟节点到实际节点的映射。 对分布式数据一致性的理解https://juejin.im/post/5ce7b325e51d45772a49ac9d 数据不一致性的情形 主库、从库和缓存的数据一致性：相同数据冗余。为保证数据库的高可用和高性能，会采用主从（备）架构并引入缓存。数据不一致存在于数据冗余的时间窗口内。 多副本数据之间的数据一致性：相同数据副本。一份数据有多个副本存储到不同节点上，客户端可以访问任一节点进行读写。常用协议包括Paxos、ZAB、Raft、Quorum、Gossip等。 分布式服务之间的数据一致性：微服务架构下，不同服务操作不同的库表，要求库表间要保持一致（等价于分布式事务） – 【感觉题目问的是这个】 对CAP理论的理解 https://www.zhihu.com/question/54105974/answer/139037688 C代表一致性，A代表可用性（在一定时间内，用户的请求都会得到应答），P代表分区容忍性。 一个分布式系统里面，节点组成的网络本来应该是连通的。然而可能因为一些故障，使得有些节点之间不连通了，整个网络就分成了几块区域。数据就散布在了这些不连通的区域中。这就叫分区。 当你一个数据项只在一个节点中保存，那么分区出现后，和这个节点不连通的部分就访问不到这个数据了。这时分区就是无法容忍的。 提高分区容忍性的办法就是一个数据项复制到多个节点上，那么出现分区之后，这一数据项就可能分布到各个区里。容忍性就提高了。 然而，要把数据复制到多个节点，就会带来一致性的问题，就是多个节点上面的数据可能是不一致的。要保证一致，每次写操作就都要等待全部节点写成功，而这等待又会带来可用性的问题。 总的来说就是，数据存在的节点越多，分区容忍性越高，但要复制更新的数据就越多，一致性就越难保证。为了保证一致性，更新所有节点数据所需要的时间就越长，可用性就会降低。 理解数据库本地事务 分布式事务 ACID 原子性 atomicity：一个事务(transaction)中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。 一致性 consistency：事务的执行前后，是从一个一致性状态转移到另一个一致性状态。【是通过原子性和隔离性保证的。】 隔离性 isolation：事务并发执行时，每个事务有各自完整的数据空间。有不同的隔离级别，大部分通过锁实现。 持久性 durability：事务只要成功执行，对数据库所做的更新会永久保存下来。 隔离级别 innodb实现原理：主要通过锁和日志来保证ACID 通过锁机制和mvcc实现隔离性 redo log（重做日志）实现持久性 undo log实现原子性和一致性【可以回滚】 分布式事务 – 主要是要保证原子性 分布式事务 指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于不同的分布式系统的不同节点之上。 一次大操作由不同的小操作组成，小操作分布在不同的服务器上，且属于不同的应用，分布式服务需要保证这些小操作要么全部成功，要么全部失败。 分布式事务的场景 Service多个节点 – 指微服务等，比如一个交易平台，订单、库存、余额等在不同的服务下，一次交易需要原子性得更新。 resource多个节点 – 指分库分表了，比如转账双方的余额在不同的表里，一次转账双方都要正确更新。 理论 CAP BASE 解决方案 2PC 第一阶段：预提交，并反映是否可以提交。【事务管理器要求每个涉及到事务的数据库预提交(precommit)此操作，并反映是否可以提交.】 第二阶段：提交，或者回滚。【事务协调器要求每个数据库提交数据，或者回滚数据。】 优点：实现成本低 缺点：单点问题（事务管理器单点，可能引起资源管理器一直阻塞），同步阻塞（precommit后，资源管理器一直处于阻塞中，直到提交、释放资源），可能存在数据不一致（比如协调者发出了commit通知，但只有部分参与者收到通知并执行了commit，其余参与者则没收到通知处于阻塞状态，就产生不一致了） TCC try - confirm - cancel 协调者变成多点，引入进群 引入超时，超时后进行补偿，并且不会锁定整个资源，缓解同步阻塞 数据一致性：通过补偿机制，由业务活动管理器控制一致性 本地消息表：核心是将需要分布式处理的任务通过消息日志的方式来异步执行。消息日志可以存储到本地文本、数据库或消息队列，再通过业务规则自动或人工发起重试。 MQ事务 SAGA seata 分布式事务描述分布式事务之TCC服务设计？ 不了解 TCC分布式事务 try - confirm - cancel 高并发&amp;高性能&amp;高可用系统设计高并发&amp;高性能&amp;高可用 高并发：系统能够同时、并行处理很多请求，常用指标响应时间、吞吐量、并发用户数、QPS 如何提高： 垂直扩展 增加单机硬件性能，比如CPU增加核数、升级更好的网卡，换好的硬盘，扩充内存 提升单机架构性能。比如使用缓存，使用异步增加单服务吞吐量 水平扩展 增加服务器数量 高性能：程序处理速度快，占用内存少，CPU占用率低 高并发和高性能紧密相关，提供性能大部分可以提高并发 增加服务器资源（内存、CPU、服务器数量）绝大部分能提高并发和性能 注意事项： 避免因为io阻塞让CPU闲置，浪费CPU 避免频繁创建、销毁线程，导致浪费资源在调度上 高可用：减少停工时间，保持服务的高度可用性（一直能用） 全年停机不超过31.5秒 6个9：一直能用的概率为99.9999% 注意事项： 避免单点 使用集群 心跳机制，监控服务器状态，挂了就进行故障修复 限流算法缓存：提升系统访问速度和增大系统处理容量 降级：当服务器压力剧增的情况下，根据当前业务情况对一些服务和页面有策略地降级，以此释放服务器资源以保证核心任务的正常运行 限流：限流的目的是通过对并发访问/请求进行限速，或者对一个时间窗口内的请求进行限速来保护系统，一旦达到限制速率则可以拒绝服务、排队或等待、降级等处理 限流算法： 固定窗口法 实现：固定时间内限定个数，比如限定每分钟100个请求 缺点：无法应对两个时间边界内的突发流量。比如在计数器清零的前一秒和后一秒都进来100个请求，那么系统短时间内就收到了两倍（200个）请求，有可能超负荷。 原因：统计精度不够 滑动窗口法 实现：简单来说就是随着时间的推移，时间窗口也会持续移动，有一个计数器不断维护着窗口内的请求数量，这样就可以保证任意时间段内，都不会超过最大允许的请求数。例如当前时间窗口是0s~60s，请求数是40，10s后时间窗口就变成了10s~70s，请求数是60。 可以用Redis有序集合实现.. 缺点：还是不能解决细粒度请求过于集中的问题，比如限制一分钟60个请求，但在59s时发送了60个请求过来。 漏桶算法 算法思想：与令牌桶算法有点相反。不限制流入速率，但以固定的速度将水流出。如果流入速度太大会导致水满溢出，溢出的请求被丢弃。 实现一：基于queue。queue的大小表示桶的大小，queue满了请求会被拒绝；另维护一个定时器，根据设定的出水速度去queue中取一个任务，比如限定一秒钟5个请求，就200ms去取一个任务，取到就执行，取不到就轮空。 实现二：基于meter，计数器。【…写的不是很清楚，看起来和固定窗口法很像了，没有体现固定的出水，只表示时间粒度比较细】 令牌桶法 算法思路：以固定的速率生成令牌，把令牌放到固定容量的桶里，超过桶容量的令牌则丢弃。每来一个请求获取一次令牌，只有获得令牌的请求才能放行，没有获得令牌的请求丢弃。 【令牌是匀速生成的，如果请求超高频，则完全被限制成令牌的生成速率；如果请求突发，也最多只允许令牌数的上限。】 Guava RateLimiter 令牌桶与漏桶的比较 漏桶能够强行限制数据的传输速率 令牌桶限制数据的平均传输速率，允许某种程度的突发传输 【看起来比较喜欢令牌桶】 两次点击，怎么防止重复下订单？两次点击的场景 没有刷新和前端控制，同一个按钮点了两次 网络问题以为失败（其实成功了）又提交了一次 rpc等重试服务 刷新前后各点一次（或者表单刷新又提交了一次） 点了后退按钮，再前进 处理方案： 前端 弹出确认界面，或disable入口并倒计时等 后端 约定【所谓重复订单，需要定义这是相同的订单】，需要和客户端配合实现 比如支付可以用订单ID作判断 如果是下单，可以用uuid或服务端先生成一个全局唯一的订单ID，客户端如果未接收到下单成功的响应，多次重试都用这一个订单ID来提交。（如果是刷新，需要客户端去服务端请求最新购物车数据，已成功下单的商品已被移除；如果是未刷新页面的重试，则使用同一个订单ID；或者提示用户刷新、提示是否重试） 后端的去重判断方式 https://www.cnblogs.com/jett010/articles/9056567.html – 本质上分布式锁的应用 基于数据库中对应订单ID的状态做判断，ID已存在（下单），或者状态已变更（修改订单，比如取消、退款等）。如果查询和更新是分开的两个操作，会存在时间差，比如查询完后状态被别的线程修改了，可以用加数据库锁的方式解决这个问题（悲观锁或乐观锁） 利用数据库唯一性索引，性能比较低 Redis分布式锁，key是订单ID，要点是加锁和解锁的原子性 ps redis的计数器是原子操作 https://redis.io/commands/incr 秒杀场景设计，应付突然的爆发流量一个秒杀系统的设计思考 两个核心问题：并发读、并发写 对应到架构设计，就是高可用、一致性和高性能的要求 高性能：涉及高读和高写。 核心理念：高读-&gt;尽量“少读”或“读少”，高写-&gt;数据拆分 动静分离：将页面拆分，静态部分做CDN缓存（秒级失效，若干CDN节点），动态部分异步请求。 数据拆分、静态缓存、数据整合（指动态数据、静态数据怎么整合在一起，一种服务端将动态数据插入到静态页面，另一种前端异步调用） 热点优化 热点操作：零点刷新、零点下单、零点加购物车等，属于用户行为不好改变，但可以做一些限制，比如用户频繁刷新页面时进行提示阻断。 热点数据： 热点识别：分为静态热点（可以提前预测的。大促前夕，可以根据大促的行业特点、活动商家等纬度信息分析出热点商品，或者通过卖家报名的方式提前筛选；另外，还可以通过技术手段提前预测，例如对买家每天访问的商品进行大数据计算，然后统计出 TOP N 的商品，即可视为热点商品）和动态热点（无法提前预测的。比如直播临时做了个广告可能导致一件商品短期内被大量购买）。 动态热点的识别实现思路：1. 异步采集交易链路各个环节的热点key信息，比如nginx采集访问url或agent采集热点日志（一些中间件本身已具备热点发现能力），提前识别潜在的热点数据。2. 聚合分析热点数据，达到一定规则的热点数据，通过订阅分发推送到链路系统，各系统根据自身需求决定如何处理热点数据，或限流或缓存，从而实现热点保护 最好做到秒级实时，这样动态发现才有意义。实际上也是对核心节点的数据采集和分析能力提出了较高的要求。 热点隔离：将热点数据隔离出来，不影响非热点数据的访问。 – 【我怎么觉得参与秒杀的商品都可以直接作为热点呢？？】 业务隔离。秒杀作为一种营销活动，卖家需要单独报名，从技术上来说，系统可以提前对已知热点做缓存预热 – 【静态热点吧..】 系统隔离。系统隔离是运行时隔离，通过分组部署和另外 99% 进行分离，另外秒杀也可以申请单独的域名，入口层就让请求落到不同的集群中 – 【也是静态热点吧..】 热点数据，可以启用单独的缓存集群或者DB服务组，从而更好的实现横向或纵向能力扩展 – 【可以是动态的，假如一个商品被动态标记为热点后】 热点优化：对这1%的数据做针对性的优化 缓存：热点缓存是最为有效的办法。 限流：流量限制更多是一种保护机制。–属于有损服务。 系统优化：提升硬件水平、调优JVM性能、代码层面优化 代码层面优化：1. 减少序列化（减少RPC调用，一种可行的方案是将多个关联性较强的应用进行 “合并部署”，从而减少不同应用之间的 RPC 调用（微服务设计规范））2. 直接输出流数据（只要涉及字符串的I/O操作，无论是磁盘 I/O 还是网络 I/O，都比较耗费 CPU 资源，因为字符需要转换成字节，而这个转换又必须查表编码。所以对于常用数据，比如静态字符串，推荐提前编码成字节并缓存，具体到代码层面就是通过 OutputStream() 类函数从而减少数据的编码转换；另外，热点方法toString()不要直接调用ReflectionToString实现，推荐直接硬编码，并且只打印DO的基础要素和核心要素– 这整段不是很懂，toString懂啊哈哈）3. 裁剪日志异常堆栈，超大流量下频繁地输出完整堆栈，会加剧系统当前负载（可以通过日志配置文件控制异常堆栈输出的深度）4. 去组件框架：极致优化要求下，可以去掉一些组件框架，比如去掉传统的 MVC 框架，直接使用 Servlet 处理请求。这样可以绕过一大堆复杂且用处不大的处理逻辑，节省毫秒级的时间，当然，需要合理评估你对框架的依赖程度 一致性：秒杀场景下的一致性问题，主要是库存扣减的准确性问题 减库存的方式： 下单减库存（用户体验好，但存在恶意下单不付款的问题） 付款减库存（用户体验差，很多人下单成功但有人不能付款） 预扣库存：缓解了以上两种方式的问题。预扣库存实际就是“下单减库存”和 “付款减库存”两种方式的结合，将两次操作进行了前后关联，下单时预扣库存，付款时释放库存。 劣势：并没有彻底解决以上问题。比如针对恶意下单的场景，虽然可以把有效付款时间设置为 10 分钟，但恶意买家完全可以在 10 分钟之后再次下单。 实际业界也多用这种方式，下单后一般都有个 “有效付款时间”，超过该时间订单自动释放，是典型的预扣库存方案。 恶意下单的解决方案主要还是结合安全和反作弊措施来制止。比如，识别频繁下单不付款的买家并进行打标，这样可以在打标买家下单时不减库存；再比如为大促商品设置单人最大购买件数，一人最多只能买 N 件商品；又或者对重复下单不付款的行为进行次数限制阻断等 超卖分为两种：1. 商家可以补货的，允许超卖；2. 不允许超卖，限定库存字段不能为负数：1）一是在通过事务来判断，即保证减后库存不能为负，否则就回滚；2）直接设置数据库字段类型为无符号整数，这样一旦库存为负就会在执行 SQL 时报错 一致性性能的优化 高并发读：“分层校验”。即在读链路时，不做一致性校验，只做不影响性能的检查（如用户是否具有秒杀资格、商品状态是否正常、用户答题是否正确、秒杀是否已经结束、是否非法请求等），在写链路的时候，才对库存做一致性检查，在数据层保证最终准确性。 不同层次尽可能过滤掉无效请求，只在“漏斗” 最末端进行有效处理，从而缩短系统瓶颈的影响路径。 高并发写 更换DB选型：换用缓存系统（带有持久化功能的缓存，如Redis，适合减库存操作逻辑单一的，无事务要求的） 优化DB性能：库存数据落地到数据库实现其实是一行存储（MySQL），因此会有大量线程来竞争 InnoDB 行锁。但并发越高，等待线程就会越多，TPS 下降，RT 上升，吞吐量会受到严重影响。 两种方法： 应用层排队。加入分布式锁，控制集群对同一行记录进行操作的并发度，也能控制单个商品占用数据库连接的数量 数据层排队。（应用层排队是有损性能的，数据层排队是最为理想的。）业界中，阿里的数据库团队开发了针对InnoDB 层上的补丁程序（patch），可以基于DB层对单行记录做并发排队，从而实现秒杀场景下的定制优化。另外阿里的数据库团队还做了很多其他方面的优化，如 COMMIT_ON_SUCCESS 和 ROLLBACK_ON_FAIL 的补丁程序，通过在 SQL 里加入提示（hint），实现事务不需要等待实时提交，而是在数据执行完最后一条 SQL 后，直接根据 TARGET_AFFECT_ROW 的结果进行提交或回滚，减少网络等待的时间（毫秒级）。目前阿里已将包含这些补丁程序的 MySQL 开源：AliSQL。 高可用 流量削峰 答题：通过提升购买的复杂度，达到两个目的：防止作弊&amp;延缓请求。本质是通过在入口层削减流量，从而让系统更好地支撑瞬时峰值。 排队：最为常见消息队列，通过把同步的直接调用转换成异步的间接推送，缓冲瞬时流量。（弊端：请求积压、用户体验）（排队本质是在业务层将一步操作转变成两步操作，从而起到缓冲的作用，但鉴于此种方式的弊端，最终还是要基于业务量级和秒杀场景做出妥协和平衡。） 过滤：过滤的核心目的是通过减少无效请求的数据IO 保障有效请求的IO性能。 读限流：对读请求做限流保护，将超出系统承载能力的请求过滤掉 读缓存：对读请求做数据缓存，将重复的请求过滤掉 写限流：对写请求做限流保护，将超出系统承载能力的请求过滤掉 写校验：对写请求做一致性校验，只保留最终的有效数据 Plan B 架构阶段：考虑系统的可扩展性和容错性，避免出现单点问题。例如多地单元化部署，即使某个IDC甚至地市出现故障，仍不会影响系统运转 编码阶段：保证代码的健壮性，例如RPC调用时，设置合理的超时退出机制，防止被其他系统拖垮，同时也要对无法预料的返回错误进行默认的处理 测试阶段：保证CI的覆盖度以及Sonar的容错率，对基础质量进行二次校验，并定期产出整体质量的趋势报告 发布阶段：系统部署最容易暴露错误，因此要有前置的checklist模版、中置的上下游周知机制以及后置的回滚机制 运行阶段：系统多数时间处于运行态，最重要的是运行时的实时监控，及时发现问题、准确报警并能提供详细数据，以便排查问题 故障发生：首要目标是及时止损，防止影响面扩大，然后定位原因、解决问题，最后恢复服务 预防：建立常态压测体系，定期对服务进行单点压测以及全链路压测，摸排水位 管控：做好线上运行的降级、限流和熔断保护。需要注意的是，无论是限流、降级还是熔断，对业务都是有损的，所以在进行操作前，一定要和上下游业务确认好再进行。就拿限流来说，哪些业务可以限、什么情况下限、限流时间多长、什么情况下进行恢复，都要和业务方反复确认 监控：建立性能基线，记录性能的变化趋势；建立报警体系，发现问题及时预警 恢复：遇到故障能够及时止损，并提供快速的数据订正工具，不一定要好，但一定要有在系统建设的整个生命周期中，每个环节中都可能犯错，甚至有些环节犯的错，后面是无法弥补的或者成本极高的。所以高可用是一个系统工程，必须放到整个生命周期中进行全面考虑。同时，考虑到服务的增长性，高可用更需要长期规划并进行体系化建设。 亿级数据从千万的数据到亿级的数据，会面临哪些技术挑战？你的技术解决思路？https://zhuanlan.zhihu.com/p/51081227 最常见的数据库，如MySql、Oracle等，都采用行式存储，比较适合OLTP。如果用MySql等行数据库来实现OLAP，一般都会碰到两个瓶颈： 数据量瓶颈：mysql比较适合的数据量级是百万级，再多的话，查询和写入性能会明显下降。因此，一般会采用分库分表的方式，把数据规模控制在百万级。 查询效率瓶颈：mysql对于常用的条件查询，需要单独建立索引或组合索引。非索引字段的查询需要扫描全表，性能下降明显。 分表 垂直业务拆分=分库+微服务（可分库基础上再分表） https://zhuanlan.zhihu.com/p/54594681 监控集群监控的时候，重点需要关注哪些技术指标？这些指标如何优化？ 系统指标：cpu使用率、内存使用率、机器连通性、系统负载（cpu.load） 网络指标：网络流入速度、网络流出速度、网络流入包数、网络流出包数、TCP连接数、TCP Active Opens（主动打开数）、IP接收包数、IP丢包数、TCP接收包数、TCP发送包数、TCP包传输错误数、TCP重传数 磁盘指标：磁盘使用率、iNode使用率、磁盘繁忙占比、磁盘读速度、磁盘写速度、磁盘读次数、磁盘写次数 容器指标？：线程数[容器指标]/平均到每核、进程数[容器指标]/平均到每核…. 监控知识体系 [小米监控](","tags":[]},{"title":"汇总的汇总","date":"2020-07-23T13:53:59.000Z","path":"2020/07/23/汇总的汇总/","text":"todo项 volatile的加屏障具体 innodb的几章 汇总完这一篇，复习、整理下框架，说清楚主要架构和流程 算法还剩一些 设计模式 新的技术 数据库MySql查询中哪些情况不会使用索引？ 使用or like以”%xx”开始匹配 联合（复合）索引，不符合最左匹配 索引列数据类型隐形转换，比如列是字符串，但用数值来查询就用不上索引 在where子句中，对索引列有数学运算、或者使用函数，用不了索引 MySQL估计全表扫描比查询索引快时（比如数据量非常少） MYSQL 索引类型、什么情况下用不上索引、什么情况下不推荐使用索引 MySQL性能优化的最佳21条经验 – 没大用 mysql explain执行计划详解 – 有错字之类的 type: const 命中唯一索引或主键的时候 数据库隔离级别 读未提交 读提交 可重复读 可串行化 数据库索引，底层是怎样实现的，为什么要用B+树索引？MySQL底层使用B+树实现的。 MyISAM引擎，B+树主索引、辅助索引叶节点是数据记录的地址，称为非聚集索引（与InnoDB区分） InnoDB的主键索引是聚集索引，叶节点存的完整的数据记录；辅助索引，叶节点存的是主键的值。 为什么用B+树索引？ 数据文件比较大，一般存储在磁盘上 索引的组织结构要尽量减少查找过程中磁盘IO次数。 数据库系统利用磁盘预读原理，将一个节点的大小设为一个页的大小，则只需要一次IO就可以将一个节点的数据都读入 B+树只有叶子节点存放数据，非叶子节点作为索引，这样树出度大，树高小，一般3层，查询目标数据的io次数比较少，效率高。 使用节点大小正好等于磁盘一页大小的B+树，可以减少io操作次数，提高查询效率。 从数组、哈希表、二叉树等数据结构的对比来回答，见下面这篇文章 MySQL为什么不用数组、哈希表、二叉树等数据结构作为索引呢 orderby底层执行过程 Mysql主从同步的实现原理？原理：在主库上记录二进制日志，在备库重放日志的方式实现异步数据复制。 复制有三个步骤： 主库记录二进制日志，每次准备提交事务（完成数据库更新）前先记录二进制日志（记录日志完后，再执行数据库更新） 备库将主库的二进制文件复制到本地的中继日志中。 备库会启动一个工作线程，称为IO工作线程，负责和主库建立一个普通的客户端连接 如果该进程追赶上了主库，它将进入睡眠状态，直到主库有新的事件产生，会被唤醒，将接收到的事件记录到中继日志中 备库的SQL线程读取中继日志并在备库执行 中继日志一般在系统缓存中，开销低，也可以根据配置选项来决定是否写入自己的二进制日志中 常见复制架构： 一主多从 主主 环型复制 MySQL复制详解 MySQL是怎么用B+树？innodb引擎用B+树当索引，索引文件同时是数据文件。聚集索引，也就是主键索引，叶节点存储的完整行数据；辅助索引，也称为非聚集索引，叶节点存对应行记录的主键。 MyISAM引擎也是用B+树当索引，为非聚集索引，索引不是数据文件，叶节点存的是行记录的地址。 谈谈数据库乐观锁与悲观锁？ 悲观锁，认为操作会发生冲突，提前加锁，直到自己操作结束再释放锁。 MySQL的显式锁定 写锁 select .. for update &amp; 读锁 select .. lock in share mode 乐观锁，认为不会发生冲突，在提交更新的时候会判断一下期间数据有没有被修改。类似于CAS操作，常用方式有版本号、时间戳。 mvcc，怎么实现rr rc todomysql间隙锁有没有了解，死锁有没有了解，写一段会造成死锁的sql语句，死锁发生了如何解决，mysql有没有提供什么机制去解决死锁gap lock MySQL几种常用的存储引擎区别InnoDB与MyISAM比较典型的几个区别： innodb支持事务、MVCC快照读、行级锁粒度、hash索引、聚集索引、支持外键 myisam支持全文索引、空间索引、数据压缩 innodb存储成本高、内存成本高、插入速度低，myisam反过来 来源：MySQL技术内幕 explain 可以看到哪些信息，什么信息说明什么，explain的结果列讲一下https://dev.mysql.com/doc/refman/8.0/en/explain-output.html Column JSON Name Meaning id select_id The SELECT identifier select标识 select_type None The SELECT type select类型 table table_name The table for the output row 表名 partitions partitions The matching partitions 使用的分区 type access_type The join type join类型 possible_keys possible_keys The possible indexes to choose 可能使用的索引 key key The index actually chosen 实际使用的索引 key_len key_length The length of the chosen key 实际使用的索引的长度 ref ref The columns compared to the index 与索引进行对比的列 rows rows Estimate of rows to be examined 预估要检查的行数 filtered filtered Percentage of rows filtered by table condition 符合条件的数据的百分比 Extra None Additional information 额外的信息 select_type 常见的有SIMPLE（简单查询，无union、subqueries）、PRIMARY（子查询的外层）、SUBQUERY、UNION等 type system：表中只有一行数据，const的特殊情况 const：至多有一行matching，可以理解为主键或唯一索引的= （单表，对tbl_name来说，1是const） 1234SELECT * FROM tbl_name WHERE primary_key=1;SELECT * FROM tbl_name WHERE primary_key_part1=1 AND primary_key_part2=2; eq_ref：主键或唯一索引的= （多表关联，other_table的结果不定，所以对ref_table来说，选择不是const） 123456SELECT * FROM ref_table,other_table WHERE ref_table.key_column=other_table.column;SELECT * FROM ref_table,other_table WHERE ref_table.key_column_part1=other_table.column AND ref_table.key_column_part2=1; ref：（非主键与非唯一索引的）其他索引的=和&lt;=&gt;（等和不等） 12345678SELECT * FROM ref_table WHERE key_column=expr;SELECT * FROM ref_table,other_table WHERE ref_table.key_column=other_table.column;SELECT * FROM ref_table,other_table WHERE ref_table.key_column_part1=other_table.column AND ref_table.key_column_part2=1; fulltext 用到了全文索引 ref_or_null 类似ref，会额外检索包含null的行 index_merge 用到了多个索引，索引合并优化 unique_subquery 替换下面的in子查询，子查询返回不重复的集合 1value IN (SELECT primary_key FROM single_table WHERE some_expr) index_subquery 区别于unique_subquery，用于非唯一索引，可以返回重复值 1value IN (SELECT key_column FROM single_table WHERE some_expr) range 索引范围查找，包括主键、唯一索引、其他索引——即，所有key =, &lt;&gt;, &gt;, &gt;=, &lt;, &lt;=, IS NULL, &lt;=&gt;, BETWEEN, LIKE, or IN() 1234567891011SELECT * FROM tbl_name WHERE key_column = 10;SELECT * FROM tbl_name WHERE key_column BETWEEN 10 and 20;SELECT * FROM tbl_name WHERE key_column IN (10,20,30);SELECT * FROM tbl_name WHERE key_part1 = 10 AND key_part2 IN (10,20,30); index 类似all，但是只扫描索引，有两种情况 覆盖索引，select中的列都在索引中，extra中显示using index 利用索引的顺序进行全表扫描（比如有order by），extra中不宣誓using index all 全表扫描 rows和filtered rows：MySQL认为需要检查的行数 filtered：rows中会被过滤出来的——即符合条件的——的数据的百分比 rows*filtered=查询出的结果数 extra 常见的有 using index 列信息只从索引出，不用再从实际行取。使用了覆盖索引 using where 没有可用的索引，通过where条件过滤 using filesort 需要额外排序 ….还有好多 索引优化 todo 看看高性能书，有硬件层面和开发层面？https://juejin.im/post/5b68e3636fb9a04fd343ba99#heading-3 如果MySQL评估使用索引比全表扫描还慢，则不会使用索引 前导模糊查询（like ‘%xx’）不会使用索引，可以优化为非前导模糊查询（like ‘xx%’） 数据类型出现隐式转换的时候不会命中索引，特别是当列类型是字符串，一定要将字符常量值用引号引起来 复合索引，要满足最左匹配原则 union、in、or 都能够命中索引，建议使用 in 查询的CPU消耗：or (id=1 or id=2)&gt; in (id in (1,2)) &gt;union(id = 1 union id = 2) 用or分割开的条件，如果or前的条件中列有索引，而后面的列中没有索引，那么涉及到的索引都不会被用到 因为or后面的条件列中没有索引，那么后面的查询肯定要走全表扫描，在存在全表扫描的情况下，就没有必要多一次索引扫描增加IO访问。 负向条件查询不能使用索引，可以优化为 in 查询 负向条件有：!=、&lt;&gt;、not in、not exists、not like 等。 范围条件查询可以命中索引 范围条件有：&lt;、&lt;=、&gt;、&gt;=、between等（返回数据的比例超过30%，会不使用索引） 查询条件（带有计算函数）执行计算不会命中索引 利用覆盖索引进行查询，避免回表 建议索引的列设置为非null 更新十分频繁的字段上不宜建立索引 区分度不大的字段上不宜建立索引 业务上具有唯一特性的字段，建议建立唯一索引 多表关联时，关联字段建议有索引 创建索引时避免以下错误观念 索引越多越好，认为一个查询就需要建一个索引。 宁缺勿滥，认为索引会消耗空间、严重拖慢更新和新增速度。 抵制唯一索引，认为业务的唯一性一律需要在应用层通过“先查后插”方式解决。 过早优化，在不了解系统的情况下就开始优化。 其他数据库有使用过哪些NoSQL数据库？MongoDB和Redis适用哪些场景？工程中用过Redis，主要是小部分数据的缓存 其他不太了解 NoSql not only sql 非关系型数据库 memcache、redis、mongoDB 如何选择？ Redis和memcache有什么区别？Redis为什么比memcache有优势？不太了解 考虑redis的时候，有没有考虑容量？大概数据量会有多少？没有，公司维护的Redis组件 – redis &amp; nosql 需要再深入一点呀 Redis的缓存淘汰策略、更新策略 过期策略 定期删除：默认每隔100ms随机抽取一些设置了过期时间的key，检查是否过期，如果过期就删除（因为全表扫描非常耗时、耗性能，所以是随机，也因此要配合惰性删除） 惰性删除：在客户端要获取某个key时，判断key是否设置过期以及是否过期，如果过期先删除 内存淘汰策略 Redis在使用内存达到某个阈值（通过maxmemory配置)的时候，就会触发内存淘汰机制，选取一些key来删除。 12# maxmemory &lt;bytes&gt; 配置内存阈值# maxmemory-policy noeviction noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。默认策略 allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key。 allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个key。 volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key。 volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key。 volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先移除。 如何选取合适的策略？比较推荐的是两种lru策略。根据自己的业务需求。如果你使用Redis只是作为缓存，不作为DB持久化，那推荐选择allkeys-lru；如果你使用Redis同时用于缓存和数据持久化，那推荐选择volatile-lru。 redis过期策略和内存淘汰策略 java实现LRU redis的数据结构Redis基础 Redis 键值（Key-Value）存储数据库 string 字符类型 map 散列类型 list 列表类型 set 集合类型 sortedset 有序集合类型 redis如何实现分布式锁，zk如何实现分布式锁，两者的区别。如果service还没执行完，分布式锁在redis中已经过期了，怎么解决这种问题redis实现分布式锁：setNX，创建成功表明获得了锁（要注意设置超时、谁加锁谁解锁、解锁的原子性） zk实现分布式锁：在路径下创建临时顺序节点，序号最小的节点表示获得了锁，其他竞争者监听自己的前一个节点 redisson给的答案是锁获取成功后，注册一个定时任务，每隔一定时间(this.internalLockLeaseTime / 3L, 10s)就去续约 加一个监听器，如果key快要超时了，就进行续约（重置成30s） JavaJava基础Java反射原理， 注解原理？反射原理：在运行状态下，对于任何一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能调用它的任意方法和属性，并能改变它的属性。总结来说，反射把Java类中的各个成分映射成为一个个Java对象，并且可以进行操作。 注解原理：注解的本质是一个继承了Annotation接口的接口。 解析一个类或者方法的注解有两种形式，一是编译期扫描，如@Override，编译器会检查方法是否真的重写了父类的某个方法；二是运行期发射，虚拟机规范定义了一系列和注解相关的属性表，字段、属性或类上有注解时（被注解修饰了），会写相应信息进字节码文件，Class类中提供了一些接口用于获取注解或判断是否被某个注解修饰。 延伸阅读：JAVA 注解的基本原理 ps: Java类执行的过程/类加载过程（2-6）/类的生命周期（2-8） – tbc 更准确的说法 编译：Java文件编译成.class字节码文件 加载：类加载器通过全限定名，将字节码加载进JVM，存储在方法区，将其转换为一个与目标类型对应的Class对象实例 验证：格式（.class文件规范）验证和语义（final不能继承等）验证？ 准备：静态变量赋初值与内存空间，final修饰的内存空间直接赋原值（？），不是开发人员赋的初值 解析：符号引用转换为直接引用，分配地址（?） 初始化：先初始化父类，再初始化自身；静态变量赋值，静态代码块执行。 使用 卸载 Java中==、equals与hashCode的区别和联系https://juejin.im/entry/59b3897b5188257e733c24eb – 后面写的比较乱 https://juejin.im/post/5a4379d4f265da432003874c – equals与hashCode Java数据类型 8种基本数据类型 （整型）数值类型 byte short int long 1.2.4.8 （浮点）数值类型 float double 4.8 字符型 char 2 存储 Unicode 码，用单引号赋值。 布尔类型 boolean 1 3种引用类型：类、接口、数组 == 比较两个数据是否相等，基本类型比较数值是否相等，引用类型比较地址是否相等。 equals()方法 Object类型定义的，比较二者== 1234//object的equals方法public boolean equals(Object obj) &#123; return (this == obj);&#125; 想自定义对象逻辑“相等”（值相等、或内容相等）的含义时，重写equals方法。 重写equals准则： 自反性：对于任何非空引用值 x，x.equals(x) 都应返回 true。 对称性：对于任何非空引用值 x 和 y，当且仅当 y.equals(x) 返回 true 时，x.equals(y) 才应返回 true。 传递性：对于任何非空引用值 x、y 和 z，如果 x.equals(y) 返回 true， 并且 y.equals(z) 返回 true，那么 x.equals(z) 应返回 true。 一致性：对于任何非空引用值 x 和 y，多次调用 x.equals(y) 始终返回 true 或始终返回 false， 前提是对象上 equals 比较中所用的信息没有被修改。 非空性：对于任何非空引用值 x，x.equals(null) 都应返回 false。 一般只判断同类型的对象 主要不要违反了对称性、传递性 hashCode()方法 1public native int hashCode(); equals的对象hashcode一定相等，hashcode相同的对象不一定equals 为什么对象的hashcode会相同？ hashcode的实现取决于jvm，比较典型的一种是基于内存地址进行哈希计算，也有基于伪随机的实现。 哈希计算会存在哈希碰撞。 https://juejin.im/entry/597937cdf265da3e114cd300 谈谈final、finally、finalize的区别 – 放一起有点奇怪final 修饰类、方法或变量 修饰类：表明类不能被继承 方法：禁止在子类中被覆盖（private方法会隐式被指定为final） 变量： 基本数据类型的变量：数值在初始化后不能更改 引用类型的变量：初始化后不能再指向另一个对象（指向的地址不可变） finally： 一般与try catch一起使用，无论程序抛出异常或正常执行，finally块的内容一定会被执行。 最常用的地方：通过try-catch-finally来进行类似资源释放、保证解锁等动作。 finalize Object的protected方法，子类可以覆盖该方法以实现资源清理工作，GC在回收对象之前调用该方法。 日常开发中基本不用，也不推荐使用。Java9中被标记为deprecated! – 不想多说 https://juejin.im/post/5b9bb81ef265da0ac2565a0f java如何实现序列化的，Serialization底层如何实现的简单说来，是将类信息和数据信息递归写成字节信息 java中的反射field的赋值底层实现 以UnsafeBooleanFieldAccessorImpl为例，也是利用unsafe 偏移 ps: Unsafe工具类 static final Unsafe unsafe = Unsafe.getUnsafe(); 1234567891011121314151617181920212223242526272829// set public void set(Object obj, Object value) throws IllegalArgumentException, IllegalAccessException &#123; ensureObj(obj); if (isFinal) &#123; throwFinalFieldIllegalAccessException(value); &#125; if (value == null) &#123; throwSetIllegalArgumentException(value); &#125; if (value instanceof Boolean) &#123; // 这里 unsafe.putBoolean(obj, fieldOffset, ((Boolean) value).booleanValue()); return; &#125; throwSetIllegalArgumentException(value); &#125;// get public Object get(Object obj) throws IllegalArgumentException &#123; return Boolean.valueOf(getBoolean(obj)); &#125; public boolean getBoolean(Object obj) throws IllegalArgumentException &#123; ensureObj(obj); // 这里 return unsafe.getBoolean(obj, fieldOffset); &#125; Java容器1. Java容器有哪些？哪些是同步容器,哪些是并发容器？容器分两个大类，Collection和Map。Collection又分List、Set、Queue、Vector几个大类，Map有HashMap、TreeMap、LinkedHashMap、HashTable，其中，Vector、HashTable是同步容器。 并发容器一般在juc包下，有ConcurrentHashMap、CopyOnWriteArrayList等。 ps: List: ArrayList、LinkedList Set: HashSet、LinkedHashSet、TreeSet Queue: LinkedList、PriorityQueue 引申：几个容器的主要方法的操作流程，容器体系结构 2. ArrayList和LinkedList的插入和访问的时间复杂度？ArrayList：插入O(n) 访问O(1) LinkedList：插入O(1) 访问O(n) HashMap在什么情况下会扩容，或者有哪些操作会导致扩容？java8中 放入新值（putValue–put/putMapEntries）后，元素个数size大于阈值threshold，会触发扩容。 链表树化时，如果表长table.length小于64，会用扩容代替树化。 put值前，如果表长为0，会触发扩容 HashMap put方法的执行过程？ 如果table为空，或长度为0，初始化。默认loadFactor为0.75，默认capacity为16（capacity是table的长度），threshold一般为capacity*loadFactor。 通过hash定位槽，如果槽为空，构造新节点赋值给槽 若槽不为空，则在槽的链表或树中找到key相同的节点，替换节点值为新值；或是没有key相同的节点，就在树中或链表尾部加入新节点；若链表加入新节点后长度达到8（槽不算，aka槽下原有7个节点），则进行红黑树转化 如果是新加入节点，modCount、元素个数size自增1，如果元素个数超过阈值，则进行扩容 Java8扩容的执行过程？ 计算新容量newCap和新阈值newThr（ps: 当容量已到最大值时，不再扩容；2倍扩容；） 创建新的数组，赋值给table 将键值对重新映射到新数组上 如果无链表，直接根据hash&amp;(newCap-1)定位 如果是树节点，委托红黑树来拆分和重新映射 为链表，根据hash&amp;oldCap的值分成0、1两组，映射到j和j+oldCap（0低位，1高位）（链表顺序不变） HashMap概述 查找 根据hash定位槽 在槽中查找给定key（hash相等、key相等），找到直接返回，否则最后返回null 若槽节点key相等，返回槽节点 若槽节点为树节点，委托给树查找 遍历链表查找 遍历 从index = 0, table[index]开始，找到一个不为null的槽，遍历链表 插入 如果table为空，或长度为0，初始化。（默认loadFactor为0.75，默认capacity为16（capacity是table的长度），threshold一般为capacity*loadFactor。） 通过hash定位槽，如果槽为空，构造新节点赋值给槽 若槽不为空，则在槽的链表或树中找到key相同的节点，替换节点值为新值；或是没有key相同的节点，就在树中或链表尾部加入新节点；若链表加入新节点后长度达到8（槽不算，aka槽下原有7个节点），则进行红黑树转化 如果是新加入节点，modCount、元素个数size自增1，如果元素个数超过阈值，则进行扩容 扩容 计算新容量newCap和新阈值newThr（ps: 当容量已到最大值时，不再扩容；2倍扩容；） 创建新的数组，赋值给table 将键值对重新映射到新数组上 如果无链表，直接根据hash&amp;(newCap-1)定位 如果是树节点，委托红黑树来拆分和重新映射 为链表，根据hash&amp;oldCap的值分成0、1两组，映射到j和j+oldCap（0低位，1高位）（链表顺序不变） 删除 定位到槽 找到删除节点 删除节点，并修复链表或红黑树 链表树化 链表树化有两个条件，不满足采用扩容，满足再扩容 树化时，将Node节点替换为TreeNode，保留next信息 替换后，再从head开始，进行红黑树化（标记红黑节点、父子节点，如果root节点不是first节点，再修正next和prev？）【链表转成红黑树后，原链表的顺序仍然会被引用仍被保留了（红黑树的根节点会被移动到链表的第一位）】 在扩容过程中，树化要满足两个条件： 链表长度大于等于 TREEIFY_THRESHOLD 8 桶数组容量大于等于 MIN_TREEIFY_CAPACITY 64 红黑树拆分（扩容时候） 红黑树中保留了next引用，拆分原理和链表相似 根据hash拆分成两组（这时候会生成新的next关系） 各组内根据情况，链化或者重新红黑树化 红黑树链化 将TreeNode替换为Node ConcurrentHashMap概述相比较HashMap，主要是增加了写操作时候的同步处理。扩容迁移时，多个线程帮助迁移。 为什么要用synchronized代替ReentrantLock？ 优化后的synchronized性能与ReentrantLock差不多，基于JVM也保证synchronized在各平台上的使用一致。 锁粒度降低了；在大量数据操作下，基于api的ReentrantLock会有更大的内存开销。 sizeCtl 默认为0 当table为null时，持有一个initial table size用于初始化 当sizeCtl&lt;0时 -1表示正在初始化 非-1的负数 123（sizeCtl的低16位-1）表示有多少个线程参与扩容迁移 sizeCtl的高16位-(1 + the number of active resizing threads) sizeCtl&gt;0时，(n &lt;&lt; 1) - (n &gt;&gt;&gt; 1) = 0.75n （表示阈值，超过阈值需要扩容） 插入 计算hash 循环执行 如果数组为空，初始化initTable 如果hash定位到的槽为空，CAS替换为新节点，退出循环 如果槽不为空，节点hash为-1，说明正在迁移，helpTransfer 槽不为空，且不在迁移，那么，对头节点加锁，链表或红黑树形式插入或更新节点 addCount 迁移 transfer的第二个参数为空的时候，触发扩容，创建nextTable，在addCount和tryPresize中有这样的调用。 addCount是size不精确情况下，可能触发扩容；tryPresize是已知精确size的情况下做扩容。 计算步长stride 如果nextTab未创建，则创建之，并赋给nextTable 循环迁移 分配迁移区间i和bound（i从前往后，bound = i - stride + 1`，总之就是stride） 如果区间已达边界，将sc减1，表示本线程退出迁移。如果是最后一个迁移线程，标记finish和advance为true，进入下一循环recheck；非最后线程，直接退出方法。 如果finish为true，table = nextTab; sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1);，退出 若未达边界，且槽为空，CAS槽为fwd，进入下一循环 槽不为空，且槽已经是fwd，进入下一循环 最后一种情形，进行迁移 为链表，根据节点hash二进制第k位为0或1分成两组（n=2^k），1连接到高位槽上 为红黑树，分组同链表，分好的组根据节点个数判断是否链化或新生成红黑树 HashMap检测到hash冲突后，将元素插入在链表的末尾还是开头？Java8是加载链表末尾 Java7是开头 头插法会改变链表中元素原本的顺序，在并发情况下可能会产生链表成环的问题。 Java7到Java8的改变HashMap为何从头插入改为尾插入 java7的问题老生常谈，HashMap的死循环 1.8还采用了红黑树，讲讲红黑树的特性，为什么人家一定要用红黑树而不是AVL、B树之类的？插入、删除、查找的最坏时间复杂度都为 O(logn)。 红黑树特性： 每个节点要么是黑色，要么是红色 根节点是黑色的 每个叶节点是黑色的（Java实现中，叶子节点是null，遍历时看不到黑色的叶子节点，反而每个叶子节点是红色的） 如果一个节点是红色的，那么它的两个子节点是黑色的（意味着可以有连续的黑色节点，但不能有连续的红色节点。若给定N个黑色节点，最短路径情况是连续N个黑色，树高为N-1；最长路径情况是红黑相间，树高为2N-2） 对任一节点，从节点到它每个叶子节点的路径包含相同数量的黑色节点（最主要特性，插入、删除要调整以遵守这个规则） 面试旧敌之红黑树（直白介绍深入理解） 为什么用红黑树？ 红黑树的统计性能（理解为增删查平均性能）优于AVL树。 AVL：名字来源发明者G. M. Adelson-Velsky和E. M. Landis。本质是平衡二叉搜索树（查找树），任何节点的左右子树高度差不超过1，是高度平衡的二叉查找树。 B树：重温数据结构：理解 B 树、B+ 树特点及使用场景 平衡二叉树节点最多有两个子树，而 B 树每个节点可以有多个子树，M 阶 B 树表示该树每个节点最多有 M 个子树 AVL树高度平衡，查找效率高，但维护这个平衡的成本比较大，插入、删除要做的调整比较耗时。 红黑树的插入、删除、查找各种操作的性能比较平衡。 B树和B+树多用于数据存储在磁盘上的场景，比较矮胖，一次读取较多数据，减少IO。节点内是有序列表。列表的插入、删除成本比较高，如果是链表形式，则查找效率比较低（不能用二分查找提高查询效率）。 【自己的理解：B树节点内是有序列表，通过二分查找提高效率】 为什么STL和linux都使用红黑树作为平衡树的实现？ - Acjx的回答 - 知乎 https://www.zhihu.com/question/20545708/answer/58717264 谈谈Java容器ArrayList、LinkedList、HashMap、HashSet的理解，以及应用场景 ArrayList LinkedList HashMap HashSet 数据结构 （可变）数组 （双向）链表 数组+红黑树 底层实现是HashMap 插入时间复杂度 o(n) o(1) 删除时间复杂度 o(n) o(1) 访问时间复杂度 o(1) 支持随机访问 o(n) 不支持随机.. 应用场景 经常访问 经常修改 映射..？ 去重 sortset底层，原理，怎么保证有序TreeSet具体实现是TreeMap，底层是红黑树 containsKey、get、put、remove 时间复杂度log(n) 红黑树 通过对任何一条（根到叶子的）路径上的各个节点的着色方式的限制，确保没有一条路径会比其他路径长出2倍，因而近乎是平衡的 性质： 每个节点是红色的，或是黑色的 根节点是黑色的 每个叶子节点（Nil）是黑色的 如果一个节点是红色的，则它的两个子节点是黑色的 对每个节点，从该节点到其子孙节点的所有路径上包含相同个数的黑色节点。（红节点不能有红孩子）（从该节点出发的所有下降路径，有相同的黑节点个数） 黑高度：从一个节点到达一个叶子节点的任意一条路径上黑色节点的个数 红黑树的黑高度定义为根节点的黑高度 优先级队列的底层原理？堆，默认是小顶堆 入队 123456789101112131415161718192021222324public boolean offer(E e) &#123; if (e == null) throw new NullPointerException(); modCount++; int i = size; if (i &gt;= queue.length) grow(i + 1); siftUp(i, e); size = i + 1; return true;&#125;private static &lt;T&gt; void siftUpComparable(int k, T x, Object[] es) &#123; Comparable&lt;? super T&gt; key = (Comparable&lt;? super T&gt;) x; while (k &gt; 0) &#123; // 如果父节点比自己大 int parent = (k - 1) &gt;&gt;&gt; 1; Object e = es[parent]; if (key.compareTo((T) e) &gt;= 0) break; es[k] = e; k = parent; &#125; es[k] = key;&#125; 出队 1234567891011121314151617181920212223242526272829303132333435363738public E poll() &#123; final Object[] es; final E result; if ((result = (E) ((es = queue)[0])) != null) &#123; modCount++; final int n; final E x = (E) es[(n = --size)]; es[n] = null; if (n &gt; 0) &#123; final Comparator&lt;? super E&gt; cmp; if ((cmp = comparator) == null) siftDownComparable(0, x, es, n); else siftDownUsingComparator(0, x, es, n, cmp); &#125; &#125; return result;&#125;private static &lt;T&gt; void siftDownComparable(int k, T x, Object[] es, int n) &#123; // assert n &gt; 0; Comparable&lt;? super T&gt; key = (Comparable&lt;? super T&gt;)x; int half = n &gt;&gt;&gt; 1; // loop while a non-leaf while (k &lt; half) &#123; // 从孩子中选一个小的 int child = (k &lt;&lt; 1) + 1; // assume left child is least Object c = es[child]; int right = child + 1; if (right &lt; n &amp;&amp; ((Comparable&lt;? super T&gt;) c).compareTo((T) es[right]) &gt; 0) c = es[child = right]; if (key.compareTo((T) c) &lt;= 0) break; es[k] = c; k = child; &#125; es[k] = key;&#125; DelayQueuehttps://www.cnblogs.com/jobs/archive/2007/04/27/730255.html DelayQueue = BlockingQueue + PriorityQueue + Delayed Java并发线程池的工作原理，几个重要参数，然后给了具体几个参数分析线程池会怎么做，最后问阻塞队列的作用是什么？线程池解决两个问题： 由于减少了每个任务的调度开销，通常在执行大量异步任务时提供优秀的性能。 提供了管理、调控资源的方式 Executors工厂方法： newFixedThreadPool 固定size的线程池。为了满足资源管理的需求，需要限制当前线程数量的场景。适用于负载比较重的服务器。 corePoolSize == maximumPoolSize keepAliveTimes = 0 LinkedBlockingQueue 队列大小Integer.MAX_VALUE，等价于无界 当线程池中线程数达到corePoolSize后，新任务将在队列中等待 由于使用无界队列，运行中的线程池不会拒绝任务 newSingleThreadExecotor 单个线程的线程池。需要保证顺序执行任务的场景，并且在任意时间点不会有多个线程是活动的。 corePoolSize = maximumPoolSize = 1 keepAliveTimes = 0 LinkedBlockingQueue 如果当前线程池无线程，就创建一个线程来运行任务 当线程数达到1后，新的任务都加入到队列中 newCachedThreadPool 大小无界的线程池（自动资源回收？），适用于有很多短期异步执行任务的小程序，或者是负载比较轻的服务器。 corePoolSize = 0, maximumPoolSize = Integer.MAX_VALUE keepAliveTimes = 60s SynchronousQueue 是一个没有容量的阻塞队列，一个插入操作必须等待另一个线程对应的移除操作 提交任务时如果有空闲线程，就空闲线程取到这个任务执行；否则创建一个线程来执行任务 适用于将主线程的任务传递给空闲线程执行 重要参数： core and maximum pool sizes corePoolSize 核心最大线程：新任务加入时，如果运行线程个数小于核心线程数，即使有其他工作线程是空闲的，也会创建新线程 – 线程池预热 maximumPoolSize 线程池最大线程：阻塞队列满时，如果运行线程数小于maximumPoolSize，才可创建新线程运行任务 corePoolSize=maximumPoolSize时，等价于newFixedThreadPool maximumPoolSize=本质上无限的数（比如Integer.MAX_VALUE），等价于newCachedThreadPool ？ 一般只在构造时设置这两个参数，但也可以通过两个set方法改变 这两个参数会自动调整么？ On-demand construction 默认情况下，只有任务提交时才会创建线程（包括核心线程） 也可以通过prestartCoreThread或者prestartAllCoreTheads来预先创建线程。比如构建了一个阻塞队列不为空的线程池时，会想要这么做（预先创建线程）。 Creating new threads 默认使用defaultThreadFactory来创建线程，相同的线程组ThreadGroup、优先级priority和非守护线程状态non-daemon status. 也可以使用自定义的threadFactory，自定义线程名称、线程组、优先级等。 threadFactory创建线程失败的什么东西没看懂 Keep-alive times keepAliveTime 如果线程数多于核心线程数，超过这个时间的空闲线程将会被停掉（指销毁掉？） queuing 入队规则 rejected tasks 四个拒绝策略 RejectedExecutionHandler ThreadPoolExecutor.AbortPolicy 抛出RejectedExecutionException CallerRunsPolicy 调用者自身来执行 DiscardPolicy 丢弃任务，任务不会被执行 DiscardOldestPolicy work queue的首个任务将会被丢弃，重试添加当前任务（可能再次失败，自旋执行） hook methods beforeExecute afterExecute 可用来设置运行环境，重新初始化本地线程，获取统计数据，添加日志。 terminated executor终止时提供的钩子方法 queue maintenance getQueue可用于监控和调试当前work queue，其他用途不建议。remove和purge可用于大量任务取消时候的存储清理。 reclamation （清除？）一个在程序中无引用、并且无剩余线程的线程池，即使无显式shutdown关闭，也可以被清除回收。可以通过这些方式设置线程池的线程在无使用时（最终）销毁：设置keep-alivet times；使用小的核心线程数比如0，或者设置allowCoreThreadTimeOut。 ScheduledThreadPoolExcutor 延迟运行命令，或周期执行命令 LinkedBlockingQueue和DelayQueue的实现原理 LinkedBlockingQueue 就是生产者消费者的实现 应用了ReentrantLock（putLock &amp; tackLock）和lock的Condition（notEmpty &amp; notFull） DelayQueue 应用了PriorityQueue，时间小的在队头 ReentrantLock（lock）和Condition（available） FutureTask是用AQS实现的 get=acquireShared，run/cancel后=release 谈谈Java线程的基本状态，其中的wait() sleep() yield()方法的区别。线程的基本状态 新建、运行（运行中、就绪）、等待、超时等待、阻塞、终止 wait() Object的方法，在某个对象上等待，等待这个对象将它唤醒，释放锁。运行-&gt;等待/超时等待 sleep() Thread的静态方法，当前线程睡眠，不释放锁。运行-&gt;超时等待 yield() Thread的方法，让出当前cpu。还是运行这个大状态，从运行中变成就绪状态。 简单谈谈JVM内存模型，以及volatile关键字运行时数据区域包括堆、方法区（包括运行时常量池）、Java虚拟机栈、本地方法栈、程序计数器、直接内存。 堆：所有对象在这里分配内存【所有线程共享】 方法区：存放已被加载的类信息、常量、静态变量、即时编译器编译后的代码等信息【所有线程共享】 Java虚拟机栈：生命周期与线程相同，描述的是Java方法执行时候的内存模型，每个方法被执行的时候都会创建一个栈帧，存储局部变量表、操作数栈、常量池引用（动态链接）、方法出口等信息。【线程私有】 本地方法栈：与虚拟机栈类似，只不过方法是本地方法【线程私有】 程序计数器：记录正在执行的虚拟机字节码指令的地址（如果是本地方法则为空）【线程私有】 直接内存：JDK1.4引入NIO，可以使用native函数库分配堆外内存，然后通过堆内的DirectByteBuffer作为这部分内存的引用、进行操作。可以提高性能，避免堆外内存和堆内内存的来回拷贝。 Java内存模型 JMM Java memory model 用来屏蔽不同硬件和操作系统的内存访问差异，实现Java在各平台上一致的内存访问效果。 JMM规定，所有变量都存在主内存中（类似于操作系统的普通内存）；每个线程有自己的工作内存（=CPU的寄存器或高速缓存），保存了该线程使用的变量的主内存副本拷贝。线程只能操作工作内存。 存在缓存不一致问题。 主内存与工作内存交互操作 内存模型三大特性 原子性：上述8个操作是原子的（double&amp;long等64位变量的操作，JVM允许非原子），一系列操作合起来其实是非原子的 可见性：指一个线程修改了共享变量的值，其他线程可以立即得知这个修改。JMM是通过变量修改后将新值同步到主内存（并使其他工作内存中的这个变量副本无效）、在变量读取前从主内存刷新变量值来实现的。 有序性：从本线程来看，所有操作都是有序的；从线程外看，操作是无序的，因为发生了指令重排。JMM允许编译器和处理器进程指令重排，重排不会影响到单线程的执行结果，但会影响多线程的执行正确性。 volatile关键字通过添加内存屏障的方式来禁止指令重排（重排序时不能把屏障后的指令重排到屏障前） 先行发生原则 单一线程原则：在一个线程内，在程序前面的操作先行发生于后面的操作。 管程锁定原则：一个 unlock 操作先行发生于后面对同一个锁的 lock 操作。 volatile变量规则：对一个 volatile 变量的写操作先行发生于后面对这个变量的读操作。 线程启动规则：Thread 对象的 start() 方法调用先行发生于此线程的每一个动作。 线程加入规则：Thread 对象的结束先行发生于 join() 方法返回。 线程中断规则：对线程 interrupt() 方法的调用先行发生于被中断线程的代码检测到中断事件的发生。 对象终结规则：一个对象的初始化完成(构造函数执行结束)先行发生于它的 finalize() 方法的开始。 传递性：如果操作 A 先行发生于操作 B，操作 B 先行发生于操作 C，那么操作 A 先行发生于操作 C。 volatile关键字 volatile关键字 保证了不同线程对该变量操作的内存可见性 禁止指令重排序，保证（volatile读写）有序性 volatile的底层如何实现，怎么就能保住可见性了？具体在👆 在缓存行和主内存之间，利用的是缓存一致性协议。 在写入缓存和缓存行之间，利用的是内存屏障。 从规范上讲是内存屏障，x86实现上是lock前缀指令，既有原子性的效果，也有StoreLoad内存屏障的效果。 内存屏障的保守插入方式，为了使写操作一定刷新到缓存行，（因为缓存一致性和禁止重排序）读操作一定读到最新值： 在每个volatile读后面，插入LoadLoad和LoadStore 在每个volatile写前面插入StoreStore，写后面插入StoreLoad 123456789101112131415161718192021public class VolatileExample &#123; int a = 0; volatile boolean flag = false; public void writer() &#123; a = 1; // StoreStore 确保a的值已写入 flag = true; // StoreLoad 确保flag的值在后面的Load之前已写入 &#125; public void reader() &#123; if (flag) &#123; // volatile读后 后面的写和读不能重排序到读flag前，确保是基于最新flag值做操作 // LoadStore // LoadLoad int i = a; System.out.println(i); &#125; &#125;&#125; 线程之间的交互方式有哪些？有没有线程交互的封装类？ 线程之间的协作 join() 在线程中调用另一个线程的join()方法，会将本线程挂起，直到目标线程结束 wait() notify() notifyAll() 调用 wait() 使得线程等待某个条件满足，线程在等待时会被挂起，当其他线程的运行使得这个条件满足时，其它线程会调用 notify() 或者 notifyAll() 来唤醒挂起的线程。 属于Object（不是Thread） await() signal() signalAll() java.util.concurrent 类库中提供了 Condition 类来实现线程之间的协调，可以在 Condition 上调用 await() 方法使线程等待，其它线程调用Condition的 signal() 或 signalAll() 方法唤醒等待的线程。 线程交互的封装类 CountDownLatch 用来控制一个线程等待多个线程。 维护了一个计数器 cnt，每次调用 countDown() 方法会让计数器的值减 1，减到 0 的时候，那些因为调用 await() 方 法而在等待的线程就会被唤醒。 12345678910111213141516public class CountdownLatchExample &#123; public static void main(String[] args) throws InterruptedException &#123; final int totalThread = 10; CountDownLatch countDownLatch = new CountDownLatch(totalThread); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; totalThread; i++) &#123; executorService.execute(() -&gt; &#123; System.out.print(\"run..\"); countDownLatch.countDown();&#125;); &#125; countDownLatch.await(); System.out.println(\"end\"); executorService.shutdown();&#125; &#125;run..run..run..run..run..run..run..run..run..run..end 等待所有run结束 CyclicBarrier 用来控制多个线程互相等待，只有当多个线程都到达时，这些线程才会继续执行。 和 CountdownLatch 相似，都是通过维护计数器来实现的。线程执行 await() 方法之后计数器会减 1，并进行等待，直到计数器为 0，所有调用 await() 方法而在等待的线程才能继续执行。 CyclicBarrier 和 CountdownLatch 的一个区别是，CyclicBarrier 的计数器通过调用 reset() 方法可以循环使用，所以它才叫做循环屏障。 123456789101112131415161718192021public class CyclicBarrierExample &#123; public static void main(String[] args) &#123; final int totalThread = 10; CyclicBarrier cyclicBarrier = new CyclicBarrier(totalThread); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; totalThread; i++) &#123; executorService.execute(() -&gt; &#123; System.out.print(\"before..\"); try &#123; cyclicBarrier.await(); &#125; catch (InterruptedException | BrokenBarrierException e) &#123; e.printStackTrace(); &#125; System.out.print(\"after..\"); &#125;);&#125; executorService.shutdown(); &#125;&#125;before..before..before..before..before..before..before..before..before..before..after..after..after..after..after..after..after..after..after..after.. 等待所有before结束 Semaphore Semaphore 类似于操作系统中的信号量，可以控制对互斥资源的访问线程数。 123456789101112131415161718192021public class SemaphoreExample &#123; public static void main(String[] args) &#123; final int clientCount = 3; final int totalRequestCount = 10; Semaphore semaphore = new Semaphore(clientCount); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; totalRequestCount; i++) &#123; executorService.execute(()-&gt;&#123; try &#123; semaphore.acquire(); System.out.print(semaphore.availablePermits() + \" \"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; semaphore.release(); &#125;&#125;); &#125; executorService.shutdown(); &#125;&#125;1 0 1 1 1 2 2 2 0 1 有限个资源 Java的锁机制 – 内容巨多Java锁机制 AQS机制 背景知识 指令流水线：现代处理器的体系结构中，采用流水线的方式对指令进行处理。每个指令的工作可分为5个阶段：取指令、指令译码、执行指令、访存取数和结果写回。 CPU多级缓存：计算机系统中，存在CPU高速缓存，用于减少处理器访问内存所需平均时间。当处理器发出内存访问请求时，会先查看缓存中是否有请求数据，若命中则直接返回该数据；若不存在，则先从内存中将数据载入缓存，再将其返回处理器。 问题引入 原子性：即一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。（比如i++，如果对实例变量i的操作不做额外的控制，那么多个线程同时调用，就会出现覆盖现象，丢失部分更新。） – 因为指令流水线 可见性：是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值（存在可见性问题的根本原因是由于缓存的存在）– 因为存在缓存 顺序性：即程序执行的顺序按照代码的先后顺序执行 – 因为存在指令重排 JMM内存模型 主要目标是定义程序中各个变量的访问规则，即在虚拟机中将变量存储到内存和从内存中取出变量这样的底层细节。这里的变量指共享变量（存在竞争问题的变量），如实例字段、静态字段、数组对象元素等。不包括线程私有的局部变量、方法参数等。 内存划分：分为主内存和工作内存，【每个线程都有自己的工作内存，它们共享主内存。】【线程对共享变量的所有读写操作都在自己的工作内存中进行，不能直接读写主内存中的变量。】【不同线程间也无法直接访问对方工作内存中的变量，线程间变量值的传递必须通过主内存完成。】 主内存（Main Memory）存储所有共享变量的值。 工作内存（Working Memory）存储该线程使用到的共享变量在主内存的的值的副本拷贝。 内存间交互规则【一个变量如何从主内存拷贝到工作内存，如何从工作内存同步到主内存中】 8种原子操作 lock: 将一个变量标识为被一个线程独占状态 unclock: 将一个变量从独占状态释放出来，释放后的变量才可以被其他线程锁定 read: 将一个变量的值从主内存传输到工作内存中，以便随后的load操作 load: 把read操作从主内存中得到的变量值放入工作内存的变量的副本中 use: 把工作内存中的一个变量的值传给执行引擎，每当虚拟机遇到一个使用到变量的指令时都会使用该指令 assign: 把一个从执行引擎接收到的值赋给工作内存中的变量，每当虚拟机遇到一个给变量赋值的指令时，都要使用该操作 store: 把工作内存中的一个变量的值传递给主内存，以便随后的write操作 write: 把store操作从工作内存中得到的变量的值写到主内存中的变量 原子操作的使用规则 read、load、use必须成对顺序出现，但不要求连续出现。assign、store、write同之； 变量诞生和初始化：变量只能从主内存“诞生”，且须先初始化后才能使用，即在use/store前须先load/assign； lock一个变量后会清空工作内存中该变量的值，使用前须先初始化；unlock前须将变量同步回主内存； 一个变量同一时刻只能被一线程lock，lock几次就须unlock几次；未被lock的变量不允许被执行unlock，一个线程不能去unlock其他线程lock的变量。 对于double和long，虽然内存模型允许对非volatile修饰的64位数据的读写操作分为两次32位操作来进行，但商用虚拟机几乎把64位数据的读写实现为了原子操作，可以忽略这个问题。 先行发生原则 【Java内存模型具备一些先天的“有序性”，即不需要通过任何同步手段（volatile、synchronized等）就能够得到保证的有序性，这个通常也称为happens-before原则。】 如果两个操作的执行次序不符合先行原则且无法从happens-before原则推导出来，那么它们就不能保证它们的有序性，虚拟机可以随意地对它们进行重排序。 程序次序规则（Program Order Rule）：一个线程内，逻辑上书写在前面的操作先行发生于书写在后面的操作。 监视器锁规则（Monitor Lock Rule）：一个unLock操作先行发生于后面对同一个锁的lock操作。“后面”指时间上的先后顺序。 volatile变量规则（Volatile Variable Rule）：对一个volatile变量的写操作先行发生于后面对这个变量的读操作。“后面”指时间上的先后顺序。 传递规则（Transitivity）：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C。 线程启动规则（Thread Start Rule）：Thread对象的start()方法先行发生于此线程的每个一个动作。 线程中断规则（Thread Interruption Rule）：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生（通过Thread.interrupted()检测）。 线程终止规则（Thread Termination Rule）：线程中所有的操作都先行发生于线程的终止检测，我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值手段检测到线程已经终止执行。 对象终结规则（Finaizer Rule）：一个对象的初始化完成（构造函数执行结束）先行发生于他的finalize()方法的开始。 问题解决 原子性 由JMM保证的原子性变量操作 基本数据类型的读写（工作内存）是原子的 JMM的lock和unlock指令可以实现更大范围的原子性保证，虚拟机提供synchronized关键字和Lock锁来保证原子性。 可见性 volatile关键字修饰的变量，被线程修改后会立即同步回主内存，其他线程要读取这个变量会从主内存刷新值到工作内存。（因为缓存一致性协议会让其他工作内存中的该变量拷贝无效，必须得从主内存再读取）即read、load、use三者连续顺序执行，assign、store、write连续顺序执行。 synchronized/Lock 由lock和unlock的使用规则保证【这里有疑问啊，synchronized有lock和unlock，但是Lock没有吧…Lock怎么保证可见性？还是说Lock保证不了可见性。可见性只能由volatile保证？–参见ConcurrentHashMap，有synchronized，还配合volatile使用—ConcurrentHashMap有些是不加锁的操作，比如get，所以还是用volatile保证可见性。synchronized 锁的是某个node节点，对这个node节点的】 synchronized有语义规定，说是通过内存屏障实现的 线程解锁前，必须把共享变量的最新值刷新到主内存中线程加锁前，将清空工作内存中共享变量的值，从而使用共享变量时需要从主内存中重新读取最新的值 Lock用了cas，有lock cmpxchg，lock前缀指令保证了可见性，同时有内存屏障的作用 同时，这俩还能保证临界区操作的所有变量的可见性因为内存屏障 LOCK前缀的指令具有如下效果： 把写缓冲区中所有的数据刷新到内存中 注意，是所有的数据，可不仅仅是对state的修改 ReentrantLock对可见性的支持 All threads will see the most recent write to a volatile field, along with any writes which preceded that volatile read/write. Reentrantlock的lock和unlock方法实际上会cas一个state的变量，state是volatile的，因此夹在两次state之间的操作都能保证可见性。这应该算是happen before的传递性… 顺序性 volatile 禁止指令重排序 synchronized/Lock “一个变量在同一个时刻只允许一条线程对其执行lock操作” – 感觉这个也没用，不然双重检查的单例怎么还用volatile关键字来防止重排序 – 最多保证原子性，被加锁内容按照顺序被多个线程执行 锁机制 volatile： 保证可见性和顺序性【实现方式：lock前缀指令+依赖MESI缓存一致性协议】 volatile修饰的变量，在进行写操作的时候会多一行汇编代码，lock指令，做两件事： 将当前处理器缓存行的数据写回系统内存 引起其他处理器里缓存了该内存地址的数据无效。【实现缓存一致性协议，处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了（处理器发现自己缓存行对应的内存地址被修改，就会将自己的缓存设置成无效状态）】 final：有两个重排序规则 – 不甚了解 写final域的重排序规则：在构造函数内对一个final域的写入，与随后把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序。 读final域的重排序规则：初次读一个包含final域的对象的引用，与随后初次读这个final域，这两个操作之间不能重排序。 synchronized关键字 使用哪个对象的监视器： 修饰对象方法时，使用当前对象的监视器 修饰静态方法时，使用类类型（Class 的对象）监视器 修饰代码块时，使用括号中的对象的监视器 必须为 Object 类或其子类的对象 无锁 -&gt; 偏向锁 -&gt; 轻量级锁 -&gt; 重量级锁 简单理解，只有一个线程CAS时，如果CAS成功，表示没有锁竞争，保持偏向锁状态，如果CAS失败，说明有竞争，（先撤销偏向锁，将对象头设置成无锁状态，并设置为不可偏向）升级为轻量级锁。 几种锁的适用场景 偏向锁：锁不仅不存在线程竞争，而且总是由同一个线程多次获得，这时候偏向锁的代价最低。适用只有一个线程访问同步块的场景。（如果有别的线程来获取锁，发现） 轻量级锁：同步块执行时间非常快的，执行完就替换回mark word，别的线程要加锁也很快，CAS。（如果同步块执行很久，竞争线程自旋cas非常久，就很耗cpu，所以会升级到重量级锁，竞争线程阻塞挂起） 重量级锁：同步块执行时间比较长的，原因如2 锁升级机制 1. 偏向锁：线程检查锁对象的状态是否是可偏向的，是的话，检查mark word中的线程ID是不是自己，是的话进入代码块，不是的话，将线程ID cas进mark word。cas失败的话，说明之前是别的线程（假设A）取到的了，等待全局安全点，JVM暂停线程A，检查线程A的状态：如果A不在活动中，将锁对象的mark word中的线程ID置空，再cas成自己的线程ID；如果A在活动中（未退出代码块），升级为轻量级锁：JVM在线程A中分配锁记录，拷贝锁对象mark word，并将锁对象mark word指向这个锁记录；在线程B中分配锁记录，拷贝锁对象mark word，并持续自旋cas（如果自旋n次还失败，就要再次升级成重量级锁了..）... 2. 轻量级锁：如果不止一个线程尝试获取锁，就会升级到轻量级锁。**通过自适应自旋CAS的方式获取锁。**如果获取失败，说明存在竞争，膨胀为重量级锁，线程阻塞。默认自旋10次。**将对象头中的Mark Word复制到栈帧（一块空间，称为锁记录）中，然后用CAS将对象头中的Mark Word替换为指向栈帧中锁记录的指针。** 3. 重量级锁：通过系统的线程互斥锁来实现的，未获取到锁的线程会阻塞挂起 大佬的图，来源见水印 右下角的轻量级锁释放的补充说明： 在某个线程A正持有轻量级锁的时候（还在代码块内运行，时间比较长），某个线程B自旋cas竞争锁（肯定是cas失败了）失败了，这时候就会升级成重量级锁了，mark word指向了互斥量的指针，这和线程A中锁记录的值不同，线程A后续释放锁就失败了（意识到已经升级成重量级锁，唤醒其他挂起的线程） ![img](../../image/172a2f26935d33c8.png) AQS 【内存屏障和”lock”前缀指令】理解volatile通过编译器，既会增加”lock”前缀指令，也会加上内存屏障（mfence等） 内存屏障是抽象概念，各个硬件、处理器实现不同 lock前缀指令和mfence等是具体实现 mesi协议保证缓存和主存间的一致性 有了msei协议，为什么汇编层面还需要lock(volatile)来实现可见性？ - Rob Zhang的回答 - 知乎 https://www.zhihu.com/question/334662600/answer/747038084 内存屏障能保证从storebuffer到缓存再到主存的一致性，在多线程运行中可以作为mesi的补充（因为mesi管不到那么多），但内存屏障 lock前缀主要是为了提供原子操作，虽然它也包含了内存屏障功能（强制将寄存器、缓存（、storebuffer/invalid queue或类似的东西）等强制同步到主存） 关于内存屏障的几个问题？ - cao的回答 - 知乎 https://www.zhihu.com/question/47990356/answer/108650501 x86在Windows下的内存屏障是用lock前缀指令来达到效果的 简单理解： 内存屏障保证了寄存器和缓存之间的一致性 lock前缀保证操作原子性 二者都能保证可见性 x86架构的内存屏障 sfence: Store Barrier = StoreStore Barriers 写屏障 所有sfence之前的store指令都在sfence之前被执行，并刷出到CPU的L1 Cache中； 所有在sfence之后的store指令都在sfence之后执行，禁止重排序到sfence之前。 所以，所有Store Barrier之前发生的内存更新都是可见的。 lfence: Load Barrier = LoadLoad Barriers 读屏障 所有在lfence之后的load指令，都在lfence之后执行，并且一直等到load buffer被该CPU读完才能执行之后的load指令（即要刷新失效的缓存）。配合sfence，使所有sfence之前发生的内存更新，对lfence之后的load操作都可见。 mfence: Full Barrier = StoreLoad Barriers 全屏障 综合了sfence和lfence的作用，强制所有在mfence之前的store/load指令都在mfence之前被执行，之后的store/load指令都在之后执行，禁止跨越mfence重排序。并且都刷新到缓存&amp;重新载入无效缓存。 Mark Word 对象头【见JMM】 todo主要有锁标志位，根据不同的锁状态其他位上存有不同的值，比如 偏向锁：拥有锁的线程ID，偏向状态 轻量级锁：拥有锁的锁记录地址 重量级锁：监视器锁的地址 synchronized底层实现加在方法上和加在同步代码块中编译后的区别、类锁、对象锁 编译时候加入监视器锁 1234567891011public class SyncTest &#123; public void syncBlock() &#123; synchronized (this) &#123; System.out.println(\"hello block\"); &#125; &#125; public synchronized void syncMethod() &#123; System.out.println(\"hello method\"); &#125;&#125; 加在方法上：方法上有synchronized关键字，flags里有ACC_SYNCHRONIZED https://blog.csdn.net/hosaos/java/article/details/100990954 ACC_SYNCHRONIZED是获取监视器锁的一种隐式实现(没有显示的调用monitorenter，monitorexit指令) 如果字节码方法区中的ACC_SYNCHRONIZED标志被设置，那么线程在执行方法前会先去获取对象的monitor对象，如果获取成功则执行方法代码，执行完毕后释放monitor对象 123456789101112131415public synchronized void syncMethod(); descriptor: ()V flags: ACC_PUBLIC, ACC_SYNCHRONIZED Code: stack=2, locals=1, args_size=1 0: getstatic #2 // Field java/lang/System.out:Ljava/io/PrintStream; 3: ldc #5 // String hello method 5: invokevirtual #4 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 8: return LineNumberTable: line 15: 0 line 16: 8 LocalVariableTable: Start Length Slot Name Signature 0 9 0 this Lwyq/learning/quickstart/juc/SyncTest; 加在同步块上：monitorenter / monitorexit 关键字 12345678910111213141516171819202122232425262728293031323334353637383940public void syncBlock(); descriptor: ()V flags: ACC_PUBLIC Code: stack=2, locals=3, args_size=1 0: aload_0 1: dup 2: astore_1 3: monitorenter 4: getstatic #2 // Field java/lang/System.out:Ljava/io/PrintStream; 7: ldc #3 // String hello block 9: invokevirtual #4 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 12: aload_1 13: monitorexit 14: goto 22 17: astore_2 18: aload_1 19: monitorexit 20: aload_2 21: athrow 22: return Exception table: from to target type 4 14 17 any 17 20 17 any LineNumberTable: line 9: 0 line 10: 4 line 11: 12 line 12: 22 LocalVariableTable: Start Length Slot Name Signature 0 23 0 this Lwyq/learning/quickstart/juc/SyncTest; StackMapTable: number_of_entries = 2 frame_type = 255 /* full_frame */ offset_delta = 17 locals = [ class wyq/learning/quickstart/juc/SyncTest, class java/lang/Object ] stack = [ class java/lang/Throwable ] frame_type = 250 /* chop */ offset_delta = 4 volatile在编译上的体现1234567public class VolatileTest &#123; private volatile int i; public void plus() &#123; i = 2; &#125;&#125; 字节码网上查到的是变量上flags有ACC_VOLATILE标识，自己编译出来没看到… 123456789101112131415public void plus(); descriptor: ()V flags: ACC_PUBLIC Code: stack=2, locals=1, args_size=1 0: aload_0 1: iconst_2 2: putfield #2 // Field i:I 5: return LineNumberTable: line 11: 0 line 12: 5 LocalVariableTable: Start Length Slot Name Signature 0 6 0 this Lwyq/learning/quickstart/juc/VolatileTest; 看文章说还是lock前缀指令 http://gee.cs.oswego.edu/dl/jmm/cookbook.html – x86架构下，实现是lock前缀指令，支持”SSE2”扩展 (Pentium4 and later)的版本支持mfence指令（比lock前缀更推荐），cas的cmpxchg的实现需要lock前缀 ​ https://www.cnblogs.com/xrq730/p/7048693.html 锁总线，其它CPU对内存的读写请求都会被阻塞，直到锁释放，不过实际后来的处理器都采用锁缓存替代锁总线，因为锁总线的开销比较大，锁总线期间其他CPU没法访问内存 lock后的写操作会回写已修改的数据，同时让其它CPU相关缓存行失效，从而重新从主存中加载最新的数据 不是内存屏障却能完成类似内存屏障的功能，阻止屏障两边的指令重排序 整理一下最终的实现： lock前缀指令会引起处理器缓存回写到内存 一个处理器的缓存回写到内存会导致其他处理器的缓存无效，这是MESI实现的（缓存一致性协议） 另外，lock前缀指令能完成内存屏障的功能，阻止屏障前后的指令重排序 ​ 这篇文章https://juejin.im/post/5ea938426fb9a043856f2f6a提到，x86下使用`lock`来实现`StoreLoad`，并且只有 StoreLoad 有效果。x86 上怎么使用 Barrier 的说明可以在 openjdk 的代码中看到，在这里src/hotspot/cpu/x86/assembler_x86.hpp。 ​ 3种重排序类型1是编译器重排序，2和3是处理器重排序。会导致多线程程序出现内存可见性问题。 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-LevelParallelism，ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。 aqs，countDownLatch如何实现 todo计算密集型/IO密集型 任务 分别如何设置线程池的核心线程数和最大线程数，为什么这么设置https://blog.csdn.net/weixin_40151613/java/article/details/81835974 计算密集型： CPU使用率比较高，（也就是一些复杂运算，逻辑处理） 线程数设置为CPU核数 IO密集型： cpu使用率较低，程序中会存在大量I/O操作占据时间，导致线程空余出来 一般设置线程数为CPU核数的2倍 最佳线程数目 = （（线程等待时间+线程CPU时间）/线程CPU时间 ）* CPU数目 线程等待时间越长，需要越多的线程 补充 高并发、任务执行时间短的业务：线程池线程数可以设置为CPU核数+1，减少线程上下文的切换 并发不高、任务执行时间长的业务： 假如是业务时间长集中在IO操作上，也就是IO密集型的任务，因为IO操作并不占用CPU，所以不要让所有的CPU闲下来，可以适当加大线程池中的线程数目，让CPU处理更多的业务 假如是业务时间长集中在计算操作上，也就是计算密集型任务，和（1）一样，线程池中的线程数设置得少一些，减少线程上下文的切换 并发高、业务执行时间长，解决这种类型任务的关键不在于线程池而在于整体架构的设计 数据能否做缓存 增加服务器 业务执行时间长的问题，也可能需要分析一下，看看能不能使用中间件（任务时间过长的可以考虑拆分逻辑放入队列等操作）对任务进行拆分和解耦。 死锁死锁定义：多个进程循环等待它方占有的资源而无限期地僵持下去的局面。 产生死锁的必要条件： 互斥（mutualexclusion），一个资源每次只能被一个进程使用 不可抢占（nopreemption），进程已获得的资源，在未使用完之前，不能强行剥夺 占有并等待（hold andwait），一个进程因请求资源而阻塞时，对已获得的资源保持不放 环形等待（circularwait），若干进程之间形成一种首尾相接的循环等待资源关系。 对待死锁的策略主要有： 死锁预防：破坏导致死锁必要条件中的任意一个就可以预防死锁。例如，要求用户申请资源时一次性申请所需要的全部资源，这就破坏了保持和等待条件；将资源分层，得到上一层资源后，才能够申请下一层资源，它破坏了环路等待条件。预防通常会降低系统的效率。 死锁避免：避免是指进程在每次申请资源时判断这些操作是否安全，例如，使用银行家算法。死锁避免算法的执行会增加系统的开销。 死锁检测：死锁预防和避免都是事前措施，而死锁的检测则是判断系统是否处于死锁状态，如果是，则执行死锁解除策略。 死锁解除：这是与死锁检测结合使用的，它使用的方式就是剥夺。即将某进程所拥有的资源强行收回，分配给其他的进程。 避免死锁的几个常见方法 避免一个线程同时获取多个锁 避免一个线程在锁内同时占用多个资源，尽量保证每个锁只占用一个资源。 尝试使用定时锁，使用lock.tryLock(timeout)来代替使用内部锁机制。 对于数据库锁，加锁和解锁必须在一个数据库连接里，否则会出现解锁失败的情况。 Java虚拟机虚拟机的几大问题 运行时数据区域 垃圾收集 对象可达 引用类型 GC Roots 算法 收集器 内存分配与回收策略（回收主要是老年代的触发条件） 类加载机制 新生代分为几个区？使用什么算法进行垃圾回收？为什么使用这个算法？新生代有三个区，一个较大的Eden区，两个小的Survivor区。 使用复制算法。（也有标记过程，标记-复制） 一方面，针对算法本身，相对于标记-清除算法，不会有内存碎片的问题；相对于标记-整理算法，处理效率高很多（在整理时，还未进行对象清理，移动存活对象时需要将存活对象插入到待清理对象之前，有大量的移动操作，时间复杂度很高）。 复制算法主要问题在于内存利用率，而HotSpot的Eden和Survivor的默认比例是8:1，保证内存利用率达到了90%，所以影响也不是太大。 另一方面，新生代minor gc比较频繁，对gc效率有比较高的要求；对象生命周期比较短，小的survivor空间即可容纳大部分情况下的存活对象。 引申：jvm的几个知识点，算法，判断对象存活，GC roots有哪些，内存分配与回收策略，类加载机制 垃圾收集算法【见Java虚拟机】 标记-清除 标记-整理 复制 分代收集 新生代：复制算法 老年代：标记-清除 or 标记整理 垃圾收集器与内存分配策略【祥见JVM的几个大知识点】垃圾收集器 内存分配策略 Minor GC 和 Full GC Minor GC:回收新生代，因为新生代对象存活时间很短，因此 Minor GC 会频繁执行，执行的速度一般也会比 较快。 Full GC:回收老年代和新生代，老年代对象其存活时间长，因此 Full GC 很少执行，执行速度会比 Minor GC 慢很多。 分配策略 对象优先在 Eden 分配 大多数情况下，对象在新生代 Eden 上分配，当 Eden 空间不够时，发起 Minor GC。 大对象直接进入老年代 大对象是指需要连续内存空间的对象，最典型的大对象是那种很长的字符串以及数组。 经常出现大对象会提前触发垃圾收集以获取足够的连续空间分配给大对象。 -XX:PretenureSizeThreshold，大于此值的对象直接在老年代分配，避免在 Eden 和 Survivor 之间的大量内存复制。 长期存活的对象进入老年代 为对象定义年龄计数器，对象在 Eden 出生并经过 Minor GC 依然存活，将移动到 Survivor 中，年龄就增加 1 岁， 增加到一定年龄则移动到老年代中。 -XX:MaxTenuringThreshold 用来定义年龄的阈值。 动态对象年龄判定 虚拟机并不是永远要求对象的年龄必须达到 MaxTenuringThreshold 才能晋升老年代，如果在 Survivor 中相同年龄 所有对象大小的总和大于 Survivor 空间的一半，则年龄大于或等于该年龄的对象可以直接进入老年代，无需等到 MaxTenuringThreshold 中要求的年龄。 空间分配担保 在发生 Minor GC 之前，虚拟机先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果条件成立的话，那么 Minor GC 可以确认是安全的。 如果不成立的话虚拟机会查看 HandlePromotionFailure 的值是否允许担保失败，如果允许那么就会继续检查老年代 最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次 Minor GC;如果小 于，或者 HandlePromotionFailure 的值不允许冒险，那么就要进行一次 Full GC。 Full GC 的触发条件 对于 Minor GC，其触发条件非常简单，当 Eden 空间满时，就将触发一次 Minor GC。而 Full GC 则相对复杂，有以下条件: 调用 System.gc()只是建议虚拟机执行 Full GC，但是虚拟机不一定真正去执行。不建议使用这种方式，而是让虚拟机管理内存。 老年代空间不足 老年代空间不足的常见场景为前文所讲的大对象直接进入老年代、长期存活的对象进入老年代等。 为了避免以上原因引起的 Full GC，应当尽量不要创建过大的对象以及数组。除此之外，可以通过 -Xmn 虚拟机参数 调大新生代的大小，让对象尽量在新生代被回收掉，不进入老年代。还可以通过 -XX:MaxTenuringThreshold 调大对 象进入老年代的年龄，让对象在新生代多存活一段时间。 空间分配担保失败使用复制算法的 Minor GC 需要老年代的内存空间作担保，如果担保失败会执行一次 Full GC。 JDK 1.7 及以前的永久代空间不足 在 JDK 1.7 及以前，HotSpot 虚拟机中的方法区是用永久代实现的，永久代中存放的为一些 Class 的信息、常量、静 态变量等数据。 当系统中要加载的类、反射的类和调用的方法较多时，永久代可能会被占满，在未配置为采用 CMS GC 的情况下也 会执行 Full GC。如果经过 Full GC 仍然回收不了，那么虚拟机会抛出 java.lang.OutOfMemoryError。 为避免以上原因引起的 Full GC，可采用的方法为增大永久代空间或转为使用 CMS GC。 Concurrent Mode Failure 执行 CMS GC 的过程中同时有对象要放入老年代，而此时老年代空间不足(可能是 GC 过程中浮动垃圾过多导致暂时 性的空间不足)，便会报 Concurrent Mode Failure 错误，并触发 Full GC。 ClassLoader原理和应用 ClassLoader的作用 加载class字节码文件到jvm 确认每个类应由那个类加载器加载，这也影响到两个类是否相等的判断，影响的方法有equals()、isAssignableFrom()、isInstance()以及instanceof关键字 加载的类存放在哪里？ jdk8之前在方法区，8之后在元数据区。 什么时候触发类加载？ 隐式加载 遇到new、getstatic、putstatic、invokestatic4条字节码指令时 对类进行反射调用时 当初始化一个类时，如果父类还没初始化，优先加载父类并初始化 虚拟机启动时，需指定一个包含main函数的主类，优先加载并初始化这个主类 显式加载 通过ClassLoader的loadClass方法 通过Class.forName 通过ClassLoader的findClass方法 有哪些类加载器ClassLoader？ Bootstrap ClassLoader：加载JVM自身工作需要的类，由JVM自己实现。加载JAVA_HOME/jre/lib下的文件 ExtClassLoader：是JVM的一部分，由sun.misc.Launcher$ExtClassLoader实现，会加载JAVA_HOME/jre/lib/ext下的文件，或由System.getProperty(&quot;java.ext.dirs&quot;)指定的目录下的文件 AppClassLoader：应用类加载器，由sun.misn.Launcher$AppClassLoader实现，加载System.getProperty(&quot;java.class.path&quot;)目录下的文件，也就是classpath路径。 双亲委派模型 原理：当一个类加载器收到类加载请求时，如果存在父类加载器，会先由父类加载器进行加载，当父类加载器找不到这个类时（根据类的全限定名称。找不到是由于，每个类有自己的加载路径。），当前类加载器才会尝试自己去加载。 为什么使用双亲委派模型？它可以解决什么问题？ 双亲委派模型能够保证类在内存中的唯一性。 假如没有双亲委派模型，用户自己写了个全限定名为java.lang.Object的类，并用自己的类加载器去加载，同时BootstrapClassLoader加载了rt.jar包中的jdk本身的java.lang.Object，这样内存中就存在两份Object类了，会出现很多问题，例如根据全限定名无法定位到具体的类。 高吞吐量的话用哪种gc算法高吞吐量，如果指cpu多用于用户程序，需要停顿时间比较短的收集器，新生代在服务端一般用Parallel Scavenge，算法也是复制算法。 复制算法的性能比较高。 jvm参数调优详细过程，到为什么这么设置，好处，一些gc场景，如何去分析gc日志jvm调优的基本原则： 大多数Java应用不需要进行JVM优化 大多数导致GC频繁、内存使用率高的问题的原因是代码层面的问题（代码层面） 上线前应考虑将JVM参数设置最优 减少创建对象的数量（代码层面） 较少使用全局变量和大对象（代码层面） 优先架构调优和代码调优，JVM优化是不得已的手段，或者说是发现问题 分析gc情况优化代码比优化JVM参数更好（代码层面） https://juejin.im/post/5dea4cb46fb9a01626644c36 新生代配置原则： 1.追求响应时间优先 这种需求下，新生代尽可能设置大一些，并通过实际情况调整新生代大小，直至接近系统的最小响应时间。因为新生代比较大，发生垃圾回收的频率会比较低，响应时间快速。 2.追求吞吐量优先 吞吐量优先的应用，在新生代中的大部分对象都会被回收，所以，新生代尽可能设置大。此时不追求响应时间，垃圾回收可以并行进行。 3.避免设置过小新生代 设置过小，YGC会很频繁，同时，很可能导致对象直接进入老年代中，老年代空间不足发生FullGC。 老年代配置原则： 1.追求响应时间优先 这种情况下，可以使用CMS收集器，以获取最短回收停顿时间，但是其内存分配需要注意，如果设置小了会造成回收频繁并且碎片变多；如果设置大了，回收的时间会很长。所以，最优的方案是根据GClog分析垃圾回收信息，调整内存大小。 2.追求吞吐量优先 吞吐量优先通常需要分配一个大新生代、小老年代，将短期存活的对象在新生代回收掉。 JVM性能调优的监控工具了解那些？jps jstack jmap jps [option] 输出Java进程信息 123jps -ml111957 org.apache.catalina.startup.Bootstrap -config /export/Domains/testenv.jd.local/server1/conf/server.xml start136044 sun.tools.jps.Jps -ml jstack [option] pid 输出某个进行内的线程栈信息 123jstack 111957 | grep 1b6d0\"System_Clock\" #307 daemon prio=5 os_prio=0 tid=0x00007f71b53f3800 nid=0x1b6d0 runnable [0x00007f72606d9000] 12-l long listings，会打印出额外的锁信息，在发生死锁时可以用&lt;strong&gt;jstack -l pid&lt;/strong&gt;来观察锁持有情况 -m mixed mode，不仅会输出Java堆栈信息，还会输出C/C++堆栈信息（比如Native方法） jmap [option] pid 输出某个进程内的堆信息：JVM版本、使用的GC算法、堆配置、堆内存使用情况 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647jmap -heap 111957Attaching to process ID 111957, please wait...Debugger attached successfully.Server compiler detected.JVM version is 25.20-b23using thread-local object allocation.Parallel GC with 43 thread(s)Heap Configuration: MinHeapFreeRatio = 0 MaxHeapFreeRatio = 100 MaxHeapSize = 2147483648 (2048.0MB) NewSize = 357564416 (341.0MB) MaxNewSize = 715653120 (682.5MB) OldSize = 716177408 (683.0MB) NewRatio = 2 SurvivorRatio = 8 MetaspaceSize = 21807104 (20.796875MB) CompressedClassSpaceSize = 1073741824 (1024.0MB) MaxMetaspaceSize = 17592186044415 MB G1HeapRegionSize = 0 (0.0MB)Heap Usage:PS Young GenerationEden Space: capacity = 353370112 (337.0MB) used = 28186432 (26.88067626953125MB) free = 325183680 (310.11932373046875MB) 7.976461801047849% usedFrom Space: capacity = 2097152 (2.0MB) used = 1736768 (1.65631103515625MB) free = 360384 (0.34368896484375MB) 82.8155517578125% usedTo Space: capacity = 2097152 (2.0MB) used = 0 (0.0MB) free = 2097152 (2.0MB) 0.0% usedPS Old Generation capacity = 869793792 (829.5MB) used = 160875768 (153.42308807373047MB) free = 708918024 (676.0769119262695MB) 18.495851485681793% used36932 interned Strings occupying 3347024 bytes. 输出堆内存中对象个数、大小统计直方图 1jmap -histo:live 111957 | less 123456789B byte C char D double F float I int J long Z boolean [ 数组，如[I表示int[] [L+类名 其他对象 dump出堆信息，再使用jhat或其他工具分析 123jmap -dump:format=b,file=dump.dat 111957jhat -port 8888 dump.dat# 浏览器输入 ip:port可访问 jstat [ generalOption | outputOptions vmid [interval[s|ms] [count]] ] jvm统计信息 vmid是Java虚拟机ID，在Linux/Unix系统上一般就是进程ID。interval是采样时间间隔。count是采样数目。 123456789101112131415jstat -gc 111957 250 6 # gc信息 S0C S1C S0U S1U EC EU OC OU MC MU CCSC CCSU YGC YGCT FGC FGCT GCT2048.0 2048.0 0.0 0.0 345088.0 148448.5 1397248.0 119834.5 80640.0 78303.8 8448.0 7919.0 133 8.230 5 11.160 19.3902048.0 2048.0 0.0 0.0 345088.0 148457.6 1397248.0 119834.5 80640.0 78303.8 8448.0 7919.0 133 8.230 5 11.160 19.3902048.0 2048.0 0.0 0.0 345088.0 148457.6 1397248.0 119834.5 80640.0 78303.8 8448.0 7919.0 133 8.230 5 11.160 19.3902048.0 2048.0 0.0 0.0 345088.0 150425.8 1397248.0 119834.5 80640.0 78303.8 8448.0 7919.0 133 8.230 5 11.160 19.3902048.0 2048.0 0.0 0.0 345088.0 150425.8 1397248.0 119834.5 80640.0 78303.8 8448.0 7919.0 133 8.230 5 11.160 19.3902048.0 2048.0 0.0 0.0 345088.0 150427.8 1397248.0 119834.5 80640.0 78303.8 8448.0 7919.0 133 8.230 5 11.160 19.390 1234567S0C、S1C、S0U、S1U：Survivor 0/1区容量（Capacity）和使用量（Used） EC、EU：Eden区容量和使用量 OC、OU：年老代容量和使用量 PC、PU：永久代容量和使用量 YGC、YGT：年轻代GC次数和GC耗时 FGC、FGCT：Full GC次数和Full GC耗时 GCT：GC总耗时 Java IOJava中的NIO，BIO，AIO分别是什么 BIO:同步并阻塞，服务器实现模式为一个连接一个线程，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销，当然可以通过线程池机制改善。BIO方式适用于连接数目比较小且固定的架构，这种方式对服务器资源要求比较高，并发局限于应用中，JDK1.4以前的唯一选择，但程序直观简单易理解。 NIO:同步非阻塞，服务器实现模式为一个请求一个线程，即客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接有I/O请求时才启动一个线程进行处理。NIO方式适用于连接数目多且连接比较短（轻操作）的架构，比如聊天服务器，并发局限于应用中，编程比较复杂，JDK1.4开始支持。 AIO:异步非阻塞，服务器实现模式为一个有效请求一个线程，客户端的I/O请求都是由OS先完成了再通知服务器应用去启动线程进行处理.AIO方式使用于连接数目多且连接比较长（重操作）的架构，比如相册服务器，充分调用OS参与并发操作，编程比较复杂，JDK7开始支持。 网络计算机网络TCP/UDP的区别 UDP：用户数据报协议 UDP(User Datagram Protocol)是无连接的，尽最大可能交付，没有拥塞控制，面向报文 (对于应用程序传下来的报文不合并也不拆分，只是添加 UDP 首部)，支持一对一、一对多、多对一和多对多 的交互通信。 TCP：传输控制协议 TCP(Transmission Control Protocol)是面向连接的，提供可靠交付，有流量控制，拥塞控 制，提供全双工通信，面向字节流(把应用层传下来的报文看成字节流，把字节流组织成大小不等的数据 块)，每一条 TCP 连接只能是点对点的(一对一)。 UDP首部格式 首部字段只有 8 个字节，包括源端口、目的端口、长度、检验和。12 字节的伪首部是为了计算检验和临时添加的。 TCP首部格式 序号 :用于对字节流进行编号，例如序号为 301，表示第一个字节的编号为 301，如果携带的数据长度为 100 字节，那么下一个报文段的序号应为 401。 确认号 :期望收到的下一个报文段的序号。例如 B 正确收到 A 发送来的一个报文段，序号为 501，携带的数据 长度为 200 字节，因此 B 期望下一个报文段的序号为 701，B 发送给 A 的确认报文段中确认号就为 701。 数据偏移 :指的是数据部分距离报文段起始处的偏移量，实际上指的是首部的长度。 确认 ACK :当 ACK=1 时确认号字段有效，否则无效。TCP 规定，在连接建立后所有传送的报文段都必须把 ACK 置 1。 同步 SYN :在连接建立时用来同步序号。当 SYN=1，ACK=0 时表示这是一个连接请求报文段。若对方同意建 立连接，则响应报文中 SYN=1，ACK=1。 终止 FIN :用来释放一个连接，当 FIN=1 时，表示此报文段的发送方的数据已发送完毕，并要求释放连接。 窗口 :窗口值作为接收方让发送方设置其发送窗口的依据。之所以要有这个限制，是因为接收方的数据缓存空 间是有限的。 TCP如何保证传输的有效性。使用超时重传来实现可靠传输：如果一个已经发送的报文段在超时时间内没有收到确认，那么就重传这个报文段。 TCP滑动窗口 暂时存放字节流。发送方和接收方各有一个窗口，接收方通过TCP报文段中的窗口字段告诉发送方自己的窗口大小，发送方根据这个值和其他信息设置自己的窗口大小。 发送窗口内的字节都允许被发送，接收窗口内的字节都允许被接收。如果发送窗口左部的字节已经发送并且收到了确认，那么就将发送窗口向右滑动一定距离，直到左部第一个字节不是已发送并且已确认的状态;接收窗口的滑动类似，接收窗口左部字节已经发送确认并交付主机，就向右滑动接收窗口。 接收窗口只会对窗口内最后一个按序到达的字节进行确认，例如接收窗口已经收到的字节为 {31, 34, 35}，其中 {31} 按序到达，而 {34, 35} 就不是，因此只对字节 31 进行确认。发送方得到一个字节的确认之后，就知道这个字节之前 的所有字节都已经被接收。 TCP的拥塞控制 与流量控制的区别： 流量控制是上一题里窗口，接收方发送窗口值来控制发送方的窗口大小，从而影响发送方的发送速率。将窗口值设置为0，则发送方不能发送数据。 控制发送方的发送速率，保证接收方来得及接收。 拥塞控制 是为了降低整个网络的拥塞程度 主要通过四个算法进行拥塞控制：慢开始、拥塞避免、快重传、快恢复。 发送方需要维护一个叫做拥塞窗口(cwnd)的状态变量（只是一个状态变量，不是发送方窗口。再区别一下，拥塞窗口讨论的是报文段数量，发送窗口讨论的是字节数量） 慢开始与拥塞避免 发送的最初是慢开始，cwnd=1，发送方只能发送一个报文段；接收到确认后，将cwnd加倍，之后能发送的报文段数量是2、4、8.. ssthresh是慢开始门限（初始值自己定），当cwnd &gt;= ssthresh 时，进入拥塞避免，每个轮 次只将 cwnd 加 1。 如果出现超时，则另ssthresh = cwnd / 2，并重新执行慢开始。 见图1、2、3 快重传与快恢复 【在接收方，要求每次接收到报文段都应该对最后一个已收到的有序报文段进行确认。例如已经接收到 M1 和 M2，此时收到 M4，应当发送对 M2 的确认。】 在发送方，如果收到三个重复确认，那么可以知道下一个报文段丢失，此时执行快重传，立即重传下一个报文段。【例如收到三个 M2，则 M3 丢失，立即重传 M3。】 同时执行快恢复，令 ssthresh = cwnd / 2 ，cwnd = ssthresh，并直接进入拥塞避免。 见上图4、5 TCP建立连接的三次握手假设A为客户端，B为服务端 首先B处于监听（listen）状态，等待客户的连接请求 A向B发送连接（SYN，同步）请求报文，SYN=1，ACK=0，seq=x（选择一个初始的序号x） B收到连接请求报文，如果同意建立连接，则向A发送连接确认报文，SYN=1，ACK=1，ack=x+1（确认号为x+1），seq=y（同时也选择一个初始的序号y） A收到B的连接确认报文后，还要向B发出确认，seq=x+1（序号为x+1），ack=y+1（确认号为y+1） 为什么要三次握手？ 三次握手是为了防止失效的连接请求到达服务器，让服务器错误打开连接。 客户端发送的连接请求如果在网络中滞留，那么隔很长时间才能收到服务器发回的连接确认，在这段时间内，客户端等待一个超时重传时间后，就会重新发送连接请求。同时滞留的连接请求最后还是会到达服务器，如果只是两次握手，那么服务器会打开两个连接。如果有第三次握手，客户端会忽略服务器之后发送的对滞留连接请求的连接确认，不进行第三次握手，因此就不会再次打开连接。 TCP四次挥手断开连接ack都为1. A 发送连接释放报文，FIN=1。 B 收到之后发出确认，此时 TCP 属于半关闭状态，B 能向 A 发送数据但是 A 不能向 B 发送数据。 当 B 不再需要连接时，发送连接释放报文，FIN=1。 A 收到后发出确认，进入 TIME-WAIT 状态，等待 2 MSL(最大报文存活时间)后释放连接。 B 收到 A 的确认后释放连接。 四次挥手的原因 客户端发送FIN连接释放报文后，服务器收到这个报文就进入CLOSE_WAIT状态，这个状态是为了让服务器端发送未传送完毕的数据，发完后服务器就会发送FIN连接释放报文。 TIME_WAIT 客户端收到服务端的FIN报文后进入此状态，并不是直接进入CLOSED状态，还需要等待一个时间计时器设置的时间2MSL。有两个理由： 确保最后一个确认报文能够到达。如果 B 没收到 A 发送来的确认报文，那么就会重新发送连接释放请求报文， A 等待一段时间就是为了处理这种情况的发生。 等待一段时间是为了让本次连接持续时间内所产生的所有报文都从网络中消失，使得下一个新的连接不会出现旧的连接请求报文。 哪些典型的应用用的是udpdns: Domain Name System，域名系统 域名解析 TFTP: Trivial File Transfer Protocol,简单文件传输协议 1.包总量较少的通信（DNS、SNMP等） 2.视频、音频等多媒体通信（即时通信） 3.限定于 LAN 等特定网络中的应用通信 4.广播通信（广播、多播） HTTPhttps和http区别，有没有用过其他安全传输手段？区别： http明文传输，安全性低；HTTPS数据加密传输，安全性高 使用https协议需要到CA（Certificate Authority，数字证书认证机构）申请证书 http的响应速度比HTTPS快，因为HTTPS除了http三次握手的包，还要加上ssl的交互–具体是？ 端口不同，http80端口，https443端口 https本质是构建在ssl/tls之上的http协议 HTTP 与 HTTPS 的区别 Http协议 基础概念 URI：uniform resource identifier 统一资源标识符 URL：uniform resource locator 统一资源定位符 URN：uniform resource name 统一资源名称 URI包括URL和URN 请求报文的格式 request line 请求行：请求方法，URL，协议 request headers 请求头：各种header 请求行和请求头合称为请求消息头 空行分隔开请求头和请求消息体 request message body 请求消息体：key-value形式或者raw格式等等 响应报文的格式 status line 状态行：协议，状态码 response headers 响应头 状态行和响应头合称为响应消息头 空行分隔开消息头和消息体 response message body 响应消息体 HTTP方法 get 主要用来获取资源 head 获取报文首部，主要用于确认 URL 的有效性以及资源更新的日期时间等。 post 主要用来传输数据 put 上传文件，不带验证机制存在安全问题，一般不使用 patch 对资源进行部分修改 – 也不常用 delete 删除文件，与put功能相反，同样不带验证机制 options 查询支持的方法，会返回Allow: GET, POST, HEAD, OPTIONS这样的内容 connect 要求在与代理服务器通信时建立隧道。使用 SSL(Secure Sockets Layer，安全套接层)和 TLS(Transport Layer Security，传输层安全)协议把通信内容 加密后经网络隧道传输。 trace 追踪路径，一般也不用… HTTP状态码 简要记一下 1XX 信息性状态码，接收的请求正在处理 2XX 请求正常处理完毕 3XX 重定向 4XX 客户端错误 5XX 服务端错误 再关注下前面的http和HTTPS的比较 其他安全传输手段：SSH SSH 协议原理、组成、认证方式和过程 延伸 https的特性：加密保证安全性防窃听、认证防伪装、完整性防篡改 加密方式：混合加密，用非对称加密传输对称秘钥，用对称秘钥进行要传输的数据的加解密 认证：使用证书来对通信双方认证。 完整性：ssl提供报文摘要功能来进行完整性保护。 http也可以通过md5验证完整性，但数据篡改后也可重新生成md5，因为是明文的。https是通过ssl的报文摘要来保证完整性的，结合了加密与认证，即使加密后数据被篡改，也很难再生成报文摘要，因为不知道明文是什么。 cookie session介绍一下 cookie 是服务器发送到用户浏览器并保持在本地的一小块数据，会在浏览器向同一服务器再次发起请求时被带上。 用途： 会话状态管理（比如用户登录状态、购物车等） 个性化设置（比如用户自定义设置、主题等） 浏览器行为分析 生成方式 服务器发送Set-Cookie: yummy_cookie=choco这样的header，客户端得到响应报文后把cookie存在浏览器 浏览器通过document.cookie属性可创建新的cookie HttpOnly 标记为 HttpOnly 的 Cookie 不能被 JavaScript 脚本调用。 Secure 标记为 Secure 的 Cookie 只能通过被 HTTPS 协议加密过的请求发送给服务端。但即便设置了 Secure 标记，敏感信 息也不应该通过 Cookie 传输，因为 Cookie 有其固有的不安全性，Secure 标记也无法提供确实的安全保障。 session 存储在服务端，可以存储在服务器上的文件、数据库或者内存中。也可以将 Session 存储在 Redis 这种内存型数据库中 使用 Session 维护用户登录状态的过程如下: 用户进行登录时，用户提交包含用户名和密码的表单，放入 HTTP 请求报文中; 服务器验证该用户名和密码，如果正确则把用户信息存储到 Redis 中，它在 Redis 中的 Key 称为 Session ID; 服务器返回的响应报文的 Set-Cookie 首部字段包含了这个 Session ID，客户端收到响应报文之后将该 Cookie 值存入浏览器中; 客户端之后对同一个服务器进行请求时会包含该 Cookie 值，服务器收到之后提取出 Session ID，从 Redis 中取 出用户信息，继续之前的业务操作。 cookie和session的选择 cookie只能存储ASCII码字符串，session可以存储任何类型的数据 cookie存储在浏览器中，安全性较低 对于大型网址，如果所有用户信息都存储在session中，开销比较大 – 【感觉不是个问题…】 session表结构怎么设计，储存在哪里？ 我们项目里没有直接使用session，用的是商城统一单点登录 如果我设计 首先一个用户请求过来，如果没有带session id，先重定向到登录页 收到登录请求，身份验证通过后，生成一个session，key为唯一ID，即session id，value为需要存储的信息，比如用户名、生成时间等，将session id作为cookie响应发回浏览器 众多的session是key-value结构，session本身也是key-value结构 存储在Redis 你们的session cookie在项目里运用到哪里？ session是SSO用的，cookie也主要是SSO用的 偶尔用的cookie是虚拟登录这样的场景 比如超级账号：员工的erp账号以只读的形式登录到用户账号，主要用于排查问题 比如账号管家：系统中，账号体系中的主账号可以登录到子账号上，一般也只读 再如虚拟登录，业务范畴上，两个账号建立授权关系，B账号可以虚拟登录到A账号上，代为操作系统 实现：被登录人一般是sso中的session对应的用户，属于资源所属者；操作者是erp账号、主账号、虚拟登录账号等，会有登录类型区分，这些信息会先加密，再存入cookie中（还会有不同的拦截器，进行身份和权限验证） 单点登录的实现 CAS TGT：Ticket Granted Ticket（俗称大令牌，或者说票根，他可以签发ST）。【类似session】 TGC：Ticket Granted Cookie（cookie中的value），存在Cookie中，根据他可以找到TGT。【类似session id】 ST：Service Ticket （小令牌），是TGT生成的，默认是用一次就生效了。也就是上面的ticket值。 ps: 未登录状态下，访问app1时，展示登录页，浏览器会写入cas服务器的TGC；第二次访问app2，（因为app2本身校验当前请求未登录）重定向到cas服务器时，会带上TGC，cas服务器根据TGC判断用户已登录，签发新的ST再重定向到app2，这时候app2用ST校验通过，记录下自己的session cookie，提供请求内容。 OAuth 【不看了不看了！】 https://juejin.im/post/5cc81d5451882524f72cd32c https://juejin.im/post/5b3b3b61f265da0f955ca780 操作系统冯诺依曼计算机的结构运算器（算术逻辑单元，处理寄存器） 控制器（指令寄存器，程序计数器） 存储器（存储数据和指令） 输入设备 输出设备 Linux怎么查看系统负载情况？ uptime w top 查看linux系统负载情况 线上服务器cpu飙高，如何处理这个问题 定位进程：top 查看cpu占用情况 定位线程：如果是Java应用，top -Hp pid 定位代码` printf %x tid 打印出线程ID对应的16进制数 0xtid jstack pid |grep -A 200 0xtid 内核态 和 用户态、cas 和 sout 哪个用到了内核态和用户态的切换sout用到了切换 进程的调度进程间的通讯方式线程间的同步方式进程和线程的区别框架Spring&amp;SpringMVC请详细描述springmvc处理请求全流程？ 通用的流程： 客户端提交请求到DispatcherServlet DispatcherServlet寻找Handler（HandlerExecutionChain）(包括handler , common interceptors和MappedInterceptor) DispatcherServlet调用controller controller调用业务逻辑，返回ModelAndView DispatcherServlet寻找ViewResolver，找到对应视图 渲染视图显示到客户端 restful的一些细节（上述2、3、4过程的细化，restful的mav一般是空的）： getHandler取到一个HandlerExecutionChain mappedHandler，包含URL对应的controller方法HandlerMethod，和一些interceptors HandlerMethod取到对应的handlerAdapter，数据绑定就再这个ha中做的 mappedHandler执行拦截器的preHandle handlerAdapter执行controller方法，包含请求前的数据绑定（数据转换），和请求后的数据转换（转换后将数据按需要的格式写入response） mappedHandler执行拦截器的postHandle 以上过程如果有抛出异常，由全局异常处理器来处理 mappedHandler触发拦截器的afterCompletion 讲一讲AtomicInteger，为什么要用CAS而不是synchronized？ ioc原理、aop原理和应用 ioc原理 控制反转（依赖注入） 本质是，spring维护了一个实例的容器，在需要使用某个实例的地方，自动注入这个实例 主要运用了反射机制，通过反射来创建约定的实例，并维护在容器中 aop原理 面向切面编程 AOP原理 原理是动态代理。代理模式的定义：给某一个对象提供一个代理，并由代理对象控制对原对象的引用。实现方式： 首先有接口A，类a实现接口A 接着创建一个bInvocationHandler类，实现InvocationHandler接口，持有一个被代理对象的实例target，invoke方法中触发method 12345678910/** * proxy: 代表动态代理对象，编译时候生成的 * method：代表正在执行的方法 * args：代表调用目标方法时传入的实参 */public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println(\"代理执行\" +method.getName() + \"方法\"); Object result = method.invoke(target, args); return result;&#125; 创建代理对象 1A a = (A) Proxy.newProxyInstance(A.class.getClassLoader(), new Class&lt;?&gt;[]&#123;A.class&#125;, handler) 比如日志、监控等公共行为可以通过AOP来实现，避免大量重复代码 元素 切面：拦截器类，定义切点以及通知 切点：具体拦截的某个业务点 通知：切面当中的方法，声明通知方法在目标业务层的执行位置，通知类型如下： 前置通知：@Before 在目标业务方法执行之前执行 后置通知：@After 在目标业务方法执行之后执行 返回通知：@AfterReturning 在目标业务方法返回结果之后执行 异常通知：@AfterThrowing 在目标业务方法抛出异常之后 环绕通知：@Around 功能强大，可代替以上四种通知，还可以控制目标业务方法是否执行以及何时执行 aspectj切面扫描的细节再看下 spring 事务实现Spring事务的底层依赖MySQL的事务，代码层面上利用AOP实现。 常用的是@Transactional注解，会被解析生成一个代理服务，TransactionInterceptor对它进行拦截处理，进行事务开启、 commit或者rollback的操作。 另外，spring还定义了事务传播行为，有7种类型，项目中常见的是PROPAGATION_REQUIRED。如果没有事务就新建事务，如果存在事务，就加入这个事务。 执行事务的时候使用TransactionInterceptor进行拦截，然后处理 事务传播行为 事务传播行为类型 说明 PROPAGATION_REQUIRED 如果当前没有事务，就新建一个事务，如果已经存在一个事务中，加入到这个事务中。这是最常见的选择。（如果父方法有事务，加入父方法的事务；父方法没有事务，则自己新建一个事务） PROPAGATION_SUPPORTS 支持当前事务，如果当前没有事务，就以非事务方式执行。（如果父方法有事务，加入父方法的事务；父方法没有事务，则以非事务执行） PROPAGATION_MANDATORY 使用当前的事务，如果当前没有事务，就抛出异常。（依赖父方法事务） PROPAGATION_REQUIRES_NEW 新建事务，如果当前存在事务，把当前事务挂起。（如果父方法有事务，把父方法事务挂起，自己新建事务；父方法没有事务，则自己新建一个事务） PROPAGATION_NOT_SUPPORTED 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。（如果父方法有事务，把父方法事务挂起，以非事务执行自己的操作；父方法没有事务，则以非事务执行）（总是以非事务执行，不报错） PROPAGATION_NEVER 以非事务方式执行，如果当前存在事务，则抛出异常。（总是以非事务执行，如果父方法存在事务，抛异常） PROPAGATION_NESTED 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与PROPAGATION_REQUIRED类似的操作。 REQUIRED、REQUIRES_NEW、NESTED的对比 REQUIRED共用一个事务。 REQUIRES_NEW 有独立的子事务，子事务异常不会导致父事务回滚，父事务异常也不会导致子事务回滚，相互独立。 NESTED 子事务嵌套在父事务中，父事务回滚会引起子事务回滚；父事务正常、子事务异常，子事务可以单独回滚。 源码详解 txNamespaceHandle注册的InfrastructureAdvisorAutoProxyCreator是一个BeanPostProcessor，主要是为了创建动态代理（wrapIfNecessary） 这几个类是可以自动创建代理的 在创建代理的时候，获取切面 txNamespaceHandler注册了一个Advisor（BeanFactoryTransactionAttributeSourceAdvisor），再在这个advisor中判断是否当前bean符合这个切面（主要实现就是看有没有@Transactional注解） TransactionInterceptor是advice，增强，执行切面工作 摘录：https://my.oschina.net/fifadxj/blog/785621 spring-jdb的事务流程： 1234567891011121314DefaultTransactionDefinition def = new DefaultTransactionDefinition();PlatformTransactionManager txManager = new DataSourceTransactionManager(dataSource);TransactionStatus status = txManager.getTransaction(def);try &#123; //get jdbc connection... //execute sql... txManager.commit(status);&#125;catch (Exception e) &#123; txManager.rollback(status); throw e;&#125; PlatformTransactionManager的getTransaction(), rollback(), commit()是spring处理事务的核心api，分别对应事务的开始，提交和回滚。 TransactionSynchronizationManager负责从ThreadLocal中存取jdbc connection 创建事务的时候会通过dataSource.getConnection()获取一个新的jdbc connection，然后绑定到ThreadLocal 在业务代码中执行sql时，通过DataSourceUtils.getConnection()从ThreadLocal中获取当前事务的jdbc connection, 然后在该jdbc connection上执行sql commit和rollback事务时，从ThreadLocal中获取当前事务的jdbc connection，然后对该jdbc connection进行commit和rollback mybatis-spring的事务流程： 配置 1234567891011121314&lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\" /&gt;&lt;/bean&gt;&lt;bean id=\"sqlSessionFactory\" class=\"org.mybatis.spring.SqlSessionFactoryBean\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\" /&gt; &lt;property name=\"transactionFactory\"&gt; &lt;bean class=\"org.apache.ibatis.spring.transaction.SpringManagedTransactionFactory\" /&gt; &lt;/property&gt; &lt;/bean&gt;&lt;bean id=\"sqlSession\" class=\"org.mybatis.spring.SqlSessionTemplate\"&gt; &lt;constructor-arg index=\"0\" ref=\"sqlSessionFactory\" /&gt;&lt;/bean&gt; mybatis-spring依赖DataSourceTransactionManager来处理事务，并没有创建自己的PlatformTransactionManager实现。 mybatis通过SqlSessionFactoryBuilder创建SqlSessionFactory，而mybatis-spring通过SqlSessionFactoryBean创建SqlSessionFactory。 配置使用SpringManagedTransactionFactory来创建MyBatis的Transaction实现SpringManagedTransaction 配置使用SqlSessionTemplate代替通过SqlSessionFactory.openSession()获取SqlSession 调用过程 可以看到mybatis-spring处理事务的主要流程和spring jdbc处理事务并没有什么区别，都是通过DataSourceTransactionManager的getTransaction(), rollback(), commit()完成事务的生命周期管理，而且jdbc connection的创建也是通过DataSourceTransactionManager.getTransaction()完成，mybatis并没有参与其中，mybatis只是在执行sql时通过DataSourceUtils.getConnection()获得当前thread的jdbc connection，然后在其上执行sql。 sqlSessionTemplate是DefaultSqlSession的一个代理类，它通过SqlSessionUtils.getSqlSession()试图从ThreadLocal获取当前事务所使用的SqlSession。如果是第一次获取时会调用SqlSessionFactory.openSession()创建一个SqlSession并绑定到ThreadLocal，同时还会通过TransactionSynchronizationManager注册一个SqlSessionSynchronization。 SqlSessionSynchronization是一个事务生命周期的callback接口，mybatis-spring通过SqlSessionSynchronization在事务提交和回滚前分别调用DefaultSqlSession.commit()和DefaultSqlSession.rollback() 这里的DefaultSqlSession只会进行一些自身缓存的清理工作，并不会真正提交事务给数据库，原因是这里的DefaultSqlSession使用的Transaction实现为SpringManagedTransaction，SpringManagedTransaction在提交事务前会检查当前事务是否应该由spring控制，如果是，则不会自己提交事务，而将提交事务的任务交给spring，所以DefaultSqlSession并不会自己处理事务。 DefaultSqlSession执行sql时，会通过SpringManagedTransaction调用DataSourceUtils.getConnection()从ThreadLocal中获取jdbc connection并在其上执行sql。 mybatis-spring做的最主要的事情是： 在SqlSession执行sql时通过用SpringManagedTransaction代替mybatis的JdbcTransaction，让SqlSession从spring的ThreadLocal中获取jdbc connection。 通过注册事务生命周期callback接口SqlSessionSynchronization，让SqlSession有机会在spring管理的事务提交或回滚时清理自己的内部缓存。 spring的循环依赖如何解决？为什么要三级缓存？https://juejin.im/post/5c98a7b4f265da60ee12e9b2 https://juejin.im/post/5e927e27f265da47c8012ed9 spring对循环依赖的处理有三种情况： 构造器的循环依赖：这种依赖spring是处理不了的，直 接抛出BeanCurrentlylnCreationException异常。 单例模式下的setter循环依赖：通过“三级缓存”处理循环依赖。 非单例循环依赖：无法处理。 如何解决的？ 只能解决单例的属性循环依赖的情况。本质上是通过将创建好的、或正在创建中的bean缓存起来。比如A和B循环依赖，创建A时先将A的实例放入缓存，自动注入属性B时，发现缓存中没有B，那么来创建B的实例，将B实例化放入缓存，注入属性A，发现A在缓存中，取出来赋值给A。bean B创建完成返回，赋值给A的属性B。这时候A和B的bean就都创建好了。 为什么要三级？看起来一级就可以实现呀？ 为什么要三级缓存：循环依赖的关键点：提前暴露绑定A原始引用的工厂类到工厂缓存。等需要时触发后续操作处理A的早期引用，将处理结果放入二级缓存 只有一级singeltonObjects肯定是不行的，需要一个放半成品的地方 实际上二级就够了，可以解决循环依赖的问题 考虑到代理的情况，就需要objectFactories这个三级缓存了，因为代理的创建是在第三步，这时候动态代理还没产生，注入了也不是最终的实例。放入三级缓存时，重写了getObject方法，会调用BeanPostProcessor的getEarlyBeanReference，这时候取到的就会是动态代理后的。 Zookeeperzk挂了怎么办？ todo 指zk集群挂了其中一台机器？ – 集群自己可以处理 挂的是master 挂的是follower 挂的是.. 集群全挂了？—那就是全挂了啊 趁早加入监控和降级策略 Dubbo&amp;Netty&amp;RPChttps://juejin.im/post/5e215783f265da3e097e9679 RPC remote procedure call 远程过程调用，是一种进程间的通信方式，是一种技术思想，而不是规范 一次完整的rpc调用流程。RPC的目标是把2-8封装起来，对用户透明。 (1):服务消费方(client)以本地调用方式调用服务。 (2):client stub接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体。 (3):client stub找到服务地址，并将消息发送到服务端。 (4):server stub收到消息后进行解码。 (5):server stub根据解码结果调用本地的服务。 (6):本地服务执行并将结果返回给server stub。 (7):server stub将返回结果打包成消息并发送至消费方。 (9):client stub接收到消息，并进行解码。 (9):服务消费方得到最终结果。 决定rpc效率的两个重要因素：通信效率，序列化和反序列化效率 常见rpc框架：dubbo、gRPC、Thrift、HSF（high speed service framework） netty 理解netty netty是一个异步事件驱动的网络应用程序框架，是基于NIO的多路复用模型实现的。 传统HTTP服务 【HTTP服务器之所以称为HTTP服务器，是因为编码解码协议是HTTP协议，如果协议是Redis协议，那它就成了Redis服务器，如果协议是WebSocket，那它就成了WebSocket服务器，等等。 使用Netty可以定制编解码协议，实现自己的特定协议的服务器。】 创建一个ServerSocket，监听并绑定一个端口 一系列客户端来请求这个端口 服务器使用Accept，获得一个来自客户端的Socket连接对象 启动一个新线程处理连接 读Socket，得到字节流 解码协议，得到HTTP请求对象 处理HTTP请求，得到一个结果，封装成一个HTTPResponse对象 编码协议，将结果序列化字节流写入Socket，发给客户端 循环步骤3 NIO 不是Java独有的概念，NIO代表IO多路复用。 由操作系统提供的功能，早期select，后期linux-epoll/max-kqueue。一般就说是epoll（没人用mac当服务器） Netty基于Java NIO进行了封装，提供易于操作的使用模式和接口。 BIO (Blocking IO)，如何理解blocking 服务端监听时，accept是阻塞的，只有新连接来了，accept才会返回，主线程才能继续 读写Socket时，read是阻塞的，只有请求消息来了（需要读完吗？），read才能返回，子线程才能继续处理 读写Socket时，write是阻塞的，只有客户端把消息接收了（客户端把消息接收了是什么表现？），write才能返回，子线程才能继续 NIO利用事件机制（=事件驱动机制）实现非阻塞。【可以用一个线程把Accept，读写操作，请求处理的逻辑全干了。如果什么事都没得做，它也不会死循环，它会将线程休眠起来，直到下一个事件来了再继续干活，这样的一个线程称之为NIO线程。】 伪代码 123456789101112131415while true &#123; events = takeEvents(fds) // 获取事件，如果没有事件，线程就休眠 for event in events &#123; if event.isAcceptable &#123; doAccept() // 新链接来了 &#125; elif event.isReadable &#123; request = doRead() // 读消息 if request.isComplete() &#123; doProcess() &#125; &#125; elif event.isWriteable &#123; doWrite() // 写消息 &#125; &#125;&#125; Reactor（基于事件驱动）线程模型 【netty可以基于以下模型灵活配置，比较常见的是用第三种。】 【在Netty里面，Accept连接可以使用单独的线程池去处理，读写操作又是另外的线程池来处理。】 【Accept连接和读写操作也可以使用同一个线程池来进行处理。请求处理逻辑既可以使用单独的线程池进行处理，也可以跟读写线程放在一块处理。】 【线程池中的每一个线程都是NIO线程。用户可以根据实际情况进行组装，构造出满足系统需求的高性能并发模型。】 Reactor单线程模型。一个NIO线程+一个accept线程。reactor线程负责分发，read、decode等操作都由其他线程处理。就和上面的伪代码差不多。 Reactor多线程模型。相比上一种，【其他线程】由线程池来托管。 Reactor主从模型。多个acceptor的NIO线程池用于接收客户端的连接。 TCP粘包拆包 现象 假设使用netty在客户端重复写100次数据”你好，我的名字是xxx!”给服务端，用ByteBuf存放这个数据 服务端接收后输出，一般存在三种情况 完整的一个字符串 字符串多了 字符串少了 原因：尽管client按照ByteBuf为单位发送数据，server按照ByteBuf读取，但操作系统底层是tcp协议，按照字节发送和接收数据，在netty应用层，重新拼装成的ByteBuf与客户端发送过来的ByteBuf可能不是对等的。 因此，我们需要自定义协议来封装和解封应用层的数据包。 netty中定义好的拆包器 固定长度的拆包器 FixedLengthFrameDecoder 行拆包器 LineBasedFrameDecoder 分隔符拆包器 DelimiterBasedFrameDecoder （行拆包器的通用版本，可自定义分隔符） 长度域拆包器 LengthFieldBasedFrameDecoder （最通用，在协议中包含长度域字段） 零拷贝 传统方式的拷贝 File.read(bytes)Socket.send(bytes) 需要四次数据拷贝和四次上下文切换 数据从磁盘读取到内核的read buffer 数据从内核缓冲区拷贝到用户缓冲区 数据从用户缓冲区拷贝到内核的socket buffer 数据从内核的socket buffer拷贝到网卡接口（硬件）的缓冲区 零拷贝的概念 上面的第二步和第三步是没有必要的，通过java的FileChannel.transferTo方法，可以避免上面两次多余的拷贝（需要操作系统支持） 调用transferTo,数据从文件由DMA引擎拷贝到内核read buffer 接着DMA从内核read buffer将数据拷贝到网卡接口buffer 上面的两次操作都不需要CPU参与，达到了零拷贝。 Netty中的零拷贝 体现在三个方面： bytefuffer Netty发送和接收消息主要使用bytebuffer，bytebuffer使用直接内存（DirectMemory）直接进行Socket读写。 原因：如果使用传统的堆内存进行Socket读写，JVM会将堆内存buffer拷贝一份到直接内存中然后再写入socket，多了一次缓冲区的内存拷贝。DirectMemory中可以直接通过DMA发送到网卡接口 Composite Buffers 传统的ByteBuffer，如果需要将两个ByteBuffer中的数据组合到一起，需要先创建一个size=size1+size2大小的新的数组，再将两个数组中的数据拷贝到新的数组中。 使用Netty提供的组合ByteBuf，就可以避免这样的操作。CompositeByteBuf并没有真正将多个Buffer组合起来，而是保存了它们的引用，从而避免了数据的拷贝，实现了零拷贝。 对FileChannel.transferTo的使用 Netty中使用了FileChannel的transferTo方法，该方法依赖于操作系统实现零拷贝。 dubbo 简介与特性：dubbo是一款高性能、轻量级的开元Java RPC框架，提供三大核心能力：面向接口的远程方法调用、智能容错和负载均衡、服务自动注册和发现。 【以下几点是官网上的特性介绍…】 面向接口的远程方法调用：提供高性能的基于代理的远程调用能力，服务以接口为粒度，为开发者屏蔽远程调用底层细节。 智能负载均衡：内置多种负载均衡策略（有哪些？），感知下游节点的健康状况，显著减少调用延迟，提高系统吞吐量。 服务自动注册于发现：支持多种注册中心服务（有哪些？），服务实例上下线实时感知（具体实现是什么？）。 高度可扩展能力：遵循微内核+插件的设计原则，所有核心能力如Protocol、Transport、Serialization被设计为可扩展点，平等的对待内置实现和第三方实现。（SPI设计模式？） 运行期流量调度：内置条件、脚本等路由策略，通过配置不同的路由规则，实现灰度发布、同机房优先等功能。 可视化的服务治理与运维：提供丰富服务治理、运维工具：随时查看服务元数据、服务健康状态以及调用统计，实时下发路由策略、调度配置参数。 dubbo架构 以上两张图说明dubbo执行流程： dubbo容器启动后，provider将自己提供的服务注册到注册中心（注册中心便知道有哪些服务上线了） consumer启动后，从注册中心订阅需要的服务。 注册中心以长连接的方式向consumer发送服务变更通知。 consumer同步调用provider的服务（如果服务有多个节点，可通过负载均衡算法选择一个节点进行调用） consumer和provider会定期将调用信息（调用时间、调用服务信息）发送给监控中心 Dubbo容器启动、服务生产者注册自己的服务、服务消费者从注册中心中订阅服务是在Dubbo应用启动时完成的；consumer调用provider是同步过程；注册中心向consumer发送服务变更通知是异步的；consumer和provider向监控中心发送信息是异步的。 调用链整体展开： 下面这张图看起来有点复杂了.. Dubbo配置的覆盖关系 (1):方法级优先、接口级次之，全局配置优先级最低。 (2):如果级别一样，则消费者优先，提供方次之。 dobbo高可用 注册中心Zookeeper宕机，还可以消费Dubbo暴露的服务。 Dubbo的监控中心宕机，不会影响Dubbo的正常使用，只是丢失了部分采样数据。 数据库宕机后，注册中心仍然可以通过缓存提供服务列表查询，但是不能注册新的服务。 注册中心集群的任意一个节点宕机，将自动切换到另外一台。 注册中心全部宕机，服务提供者和消费者可以通过本地缓存通讯。 服务提供者无状态，任意一台宕机后，不影响使用。 服务提供者全部宕机，服务消费者应用将无法使用，并且会无限次重连等待服务提供者恢复。 负载均衡策略 【默认为随机】 基于权重的随机负载均衡：Random LoadBalance，比如orderService想要远程调用userService，而userService分别在三台机器上，我们可以给每台机器设置权重，比如三台机器的权重依次为100、200、50，则总权重为350，则选择第一台的概率就是100/350. 基于权重的轮询负载均衡：RoundRobin LoadBalance（可以理解为按照权重占比进行轮询。占比少的，当权重比较低时就不会再去权重低的机器上请求。如果某台机器性能一般，但权重占比高，就很可能卡在这里） 最少活跃数负载均衡：LeastActive LoadBalance，比如三台服务器上一次处理请求所花费的时间分别为100ms、1000ms、300ms，则这一次请求回去上一次处理请求时间最短的机器，所以这次一号服务器处理这次请求。 一致性Hash负载均衡：ConsistentHash LoadBalance 原文：https://blog.csdn.net/revivedsun/java/article/details/71022871 一致性Hash负载均衡涉及到两个主要的配置参数为hash.arguments 与hash.nodes。 hash.arguments ： 当进行调用时候根据调用方法的哪几个参数生成key，并根据key来通过一致性hash算法来选择调用结点。例如调用方法invoke(String s1,String s2); 若hash.arguments为1(默认值)，则仅取invoke的参数1（s1）来生成hashCode。 hash.nodes： 为结点的副本数。 12345缺省只对第一个参数Hash，如果要修改，请配置&lt;dubbo:parameter key=\"hash.arguments\" value=\"0,1\" /&gt;缺省用160份虚拟节点，如果要修改，请配置&lt;dubbo:parameter key=\"hash.nodes\" value=\"320\" /&gt; 降级服务 当服务器压力剧增的情况下，根据实际业务及流量，对一些服务和页面有策略地不处理或者换种简单的方式处理，从而释放服务器资源以保证核心交易正常或高效运行。 mock=force:return+null:表示消费方对该服务的方法都返回null值，不发起远程调用。用来屏蔽不重要的服务不可用时对调用方的影响，可以直接在Dubbo客户端(localhost:7001)对服务消费者设置，屏蔽掉即可。 mock=fall:return+null:表示消费方对该服务的方法调用在失败后，再返回null，不抛出异常。用来容忍不重要服务不稳定时对调用方的影响，可以直接在Dubbo客户端(localhost:7001)对服务消费者设置，容错掉即可。 集群容错 Failover Cluster:失败自动切换，当出现失败，重试其他服务器。通常用于读操作，但重试会带来更长延迟。可通过retries=n来设置重试次数。 Failfast Cluster:快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增操作。 Forking Cluster:并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作，但需要浪费更多的服务资源。通过过fork=n设置最大并行数。 Broadcast Cluster:广播调用所有提供者，逐个调用，任意一台报错则报错，通常用于通知所有服务提供者更新缓存或日志等本地资源信息。 消息队列作用 解耦 异步 削峰/限流 原理介绍 todo如何保证RocketMQ 消息的顺序性，如何解决重复消费问题针对kafka来说 如何保证消息的顺序性： 一个分区内的消息是顺序的 一个主题的不同分区之间，消息不能保证有序 – 对同一类消息指定相同的key，相同的key会哈希到同一个分区，这样可以保证这部分消息的有序性 https://www.cnblogs.com/756623607-zhang/p/10506909.html 如何解决重复消费： kafka自带的消费机制 consumer消费后，会定期将消费过的offset偏移量提交给broker。如果consumer重启，会继续上次的offset开始消费。 业务上保证幂等性 如果进程挂了或机器宕机，没来得及提交offset，需要业务上进行幂等。 比如建立一张消息表。 生产者，发送消息前判断库中是否有记录（有记录说明已发送），没有记录，先入库，状态为待消费，然后发送消息并把主键id带上。 消费者，接收消息，通过主键ID查询记录表，判断消息状态是否已消费。若没消费过，则处理消息，处理完后，更新消息记录的状态为已消费。 MyBatisMyBatis，Mybatis与SpringMyBatis 消除了大部分 JDBC 的样板代码、手动设置参数以及检索结果。通过简洁的设计最大限度地简化开发和提升性能。 解除SQL与程序代码的耦合，通过提供dao层，将业务逻辑和数据访问逻辑分离开。设计更清晰，更易维护。 MyBatis整体架构 MyBatis层级结构 裸用sqlSession是上面的红框 spring用mapper/dao接口代理，本质上是一个MapperProxy，从下面的红框开始执行 spring事务是在哪个环节起作用？ https://mybatis.org/spring/zh/transactions.html 一个使用 MyBatis-Spring 的其中一个主要原因是它允许 MyBatis 参与到 Spring 的事务管理中。而不是给 MyBatis 创建一个新的专用事务管理器，MyBatis-Spring 借助了 Spring 中的 DataSourceTransactionManager 来实现事务管理。 一旦配置好了 Spring 的事务管理器，你就可以在 Spring 中按你平时的方式来配置事务。并且支持 @Transactional 注解和 AOP 风格的配置。在事务处理期间，一个单独的 SqlSession 对象将会被创建和使用。当事务完成时，这个 session 会以合适的方式提交或回滚。 事务配置好了以后，MyBatis-Spring 将会透明地管理事务。 所以，最外层是事务，每个事务会起一个SqlSession。 几篇文章： 入门，裸用mybatis：https://juejin.im/post/5aa5c6fb5188255587232e5a#heading-0 mybatis执行，包括整合spring后的流程：https://juejin.im/post/5e350d895188254dfd43def5#heading-9 关于JDBC：https://juejin.im/post/5c75e6666fb9a049cd54dc88 Mybatis和spring整合的使用：https://juejin.im/post/5cdfed6ef265da1b6720dcaf mybatis框架说明： 整体执行流程说明： sqlSession执行流程说明： 关键流程（以下整个可以看成裸用MyBatis的执行流程） config文件加载：解析xml文件配置项 mapper文件加载：上一个流程中的一个环节，解析完后封装成MappedStatement，存入configuration SqlSource创建流程：上一流程的一个环节，SqlSource是MappedStatement的一部分，主要存放sql和占位的参数名称 – 解析环节结束 SqlSession执行流程：sqlSessionFactory.openSession主要是建立了一个和数据库的连接connection 获取BoundSql流程：sqlSession.xx方法执行时，需要获取BoundSql，BoundSql本质上是SqlSource和执行请求的入参的一个组合 参数映射流程：根据顺序，或者根据名称（只是大略看了一眼） 结果集映射流程：根据名称（只是大略看了一眼） mybatis的openSession默认开启事务，autocommit为false，隔离级别为null mybatis的JdbcTransaction 整合spring的几个组件 org.mybatis.spring.SqlSessionFactoryBean 注入sqlSessionFactory org.mybatis.spring.mapper.MapperScannerConfigurer扫描指定包 将包下class文件加入到beanDefinition中，bean类型指定为MapperFactoryBean SqlSessionFactoryBean构建sqlSessionFactory时，扫描mapper xml文件，根据namespace在MapperRegistry中注入对应mapper接口的MapperProxyFactory MapperFactoryBean-&gt;getObject中生成mapper的代理类MapperProxy（通过MapperFactoryBean中的interface，即mapper的namespace找到MapperProxyFactory，再生产出代理类） 以下大概知道了 现在差一个中间环节，mapper的beanDefinition怎么变成MapperProxy..以及MapperFactoryBean的作用 还有个SqlSessionTemplate：https://juejin.im/post/5cea1f386fb9a07ea803a70e 还有MapperProxyFactory – 来创建MapperProxy Java动态代理：https://juejin.im/post/5c1ca8df6fb9a049b347f55c MapperFactoryBean MapperProxy MapperMethod – 到这里之后，流程就转到sqlSession.selectOne之类的了 Mybatis缓存 https://juejin.im/post/5e81fb126fb9a03c546c22bb MyBatis 系统中默认定义了两级缓存：一级缓存和二级缓存 默认情况下，只有一级缓存开启。（SqlSession级别的缓存，也称为本地缓存） 二级缓存需要手动开启和配置，它是基于 namespace 级别的缓存，缓存只作用于 cache 标签所在的映射文件中的语句。 系统设计分布式谈谈分布式锁、以及分布式全局唯一ID的实现比较？分布式锁实现方式及比较 为什么需要分布式锁？ 效率：避免不同节点重复相同的工作，这些工作会浪费资源。比如针对一个操作发多封邮件。 正确性：避免破坏正确性的发生，比如多个节点操作了同一条数据，其中一个操作结果被覆盖了，造成数据丢失。 常见实现方式 数据库 表的一行数据表示一个资源，select..for update来加锁，可同时存节点信息，支持重入。 理解简单，但需要自己实现，以及维护超时、事务和异常处理等，性能局限于数据库，性能相对比较低，不适合高并发场景。 zookeeper curator封装 – 具体怎么用还没看过呢 Zk 排他锁和共享锁有区别。排他锁，利用zk有序节点，序号最小的节点表示获取到锁，其他未获取到锁的节点监听自己的前一个节点。 （共享锁，能获取到资源都算？回家再看看。）还有个读写锁，一个节点获取读锁，只要序号小于他的都为读锁，就表示获取到读锁；一个节点获取写锁，需要自己的序号最小，才表示获取到写锁。可重入锁之类的，zk节点写值吧，原理和Java的reentrantLock类似，获取多少次，state自加多少次，解锁再一次次自减，直到state为0表示完全释放。 （文章里说zk分布式锁和数据库mysql差不多。。真的么） redis setNX 自己参照一篇文章实现的比较简单，主要利用setNX，不支持重入，非公平。有超时释放，有加锁身份，解锁原子性 下面的文章里介绍的Redission。不太了解，加锁原子性lua脚本，用了hset，hashmap的结构，key是资源，value是锁定次数，可重入。。还有公平锁的实现。。。 分布式锁 分布式全局唯一ID/分布式ID生成器 实现方式及比较 uuid 缺点：长，占用空间大，间接导致数据库性能下降 缺点：不是有序的，导致索引在写的时候有过多的随机写 数据库自增主键 缺点：完全依赖于数据库，有性能瓶颈 缺点：不易扩展 snowflake Twitter，Scala实现的… 雪花算法，带有时间戳的全局唯一ID生成算法 固定ID格式： 12341位的时间戳（精确到毫秒，41位的长度可使用69年）10位的机器ID（10位长度最多支持1024个服务器节点部署）12位的计数序列号（12位支持每节点每毫秒最多生成4096个序列号） 分布式锁的实现方式，zk实现和Redis实现的比较实现方式：CAP的应用 MySQL唯一索引 实现：锁名称建立唯一索引，先插入数据的线程获得锁 缺点：完全依赖数据库的可用性（单点问题，无主从切换）和性能 Redis 实现：setnx key value expire_time 优缺点：为解决无主从切换的问题，可以使用Redis集群，或者sentinel哨兵模型。当master节点出现故障，哨兵从slave中选取节点称为新master节点。文章说，Redis集群的复制是AP模式，可能存在数据不一致，导致存在两个线程获得到锁的情况。（一个线程在原master获得锁，另一个线程在新master获得锁） 对数据一致性非常敏感的场景，建议使用CP模型（比如zk） zk 实现： 线程向zk的锁目录，申请创建有序的临时节点 如果建成的节点序号最小，表明获得到锁 如果序号非最小，监听自己的前一个节点 删除节点表示释放锁；当获取锁的客户端异常、无心跳，临时节点会被删除，也表示释放锁 优缺点：CP模式，zk的分布式锁比Redis的可靠，但Redis的性能更高。要根据自己的业务场景，再选型。 对一致性哈希的理解一致性哈希算法 求出各节点的哈希值，将其配置到0~2^32的圆（continuum）上 用同样方法求出存储数据的哈希值，映射到相同的圆上 从数据映射到的位置开始顺时针查找，将数据保存到找到的第一个节点上。如果超过2^32仍然找不到节点，就保存到第一个节点上 【如果添加一个节点node5，只会影响该节点的逆时针方向的第一个节点node4会受到影响（原来在node4上的数据要重新分配一些到node5上）】 一致性哈希算法对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性 在服务节点太少时，容易因节点分部不均匀而造成数据倾斜问题，可引入虚拟节点机制，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。同时数据定位算法不变，只是多了一步虚拟节点到实际节点的映射。 对分布式数据一致性的理解https://juejin.im/post/5ce7b325e51d45772a49ac9d 数据不一致性的情形 主库、从库和缓存的数据一致性：相同数据冗余。为保证数据库的高可用和高性能，会采用主从（备）架构并引入缓存。数据不一致存在于数据冗余的时间窗口内。 多副本数据之间的数据一致性：相同数据副本。一份数据有多个副本存储到不同节点上，客户端可以访问任一节点进行读写。常用协议包括Paxos、ZAB、Raft、Quorum、Gossip等。 分布式服务之间的数据一致性：微服务架构下，不同服务操作不同的库表，要求库表间要保持一致（等价于分布式事务） – 【感觉题目问的是这个】 对CAP理论的理解 https://www.zhihu.com/question/54105974/answer/139037688 C代表一致性，A代表可用性（在一定时间内，用户的请求都会得到应答），P代表分区容忍性。 一个分布式系统里面，节点组成的网络本来应该是连通的。然而可能因为一些故障，使得有些节点之间不连通了，整个网络就分成了几块区域。数据就散布在了这些不连通的区域中。这就叫分区。 当你一个数据项只在一个节点中保存，那么分区出现后，和这个节点不连通的部分就访问不到这个数据了。这时分区就是无法容忍的。 提高分区容忍性的办法就是一个数据项复制到多个节点上，那么出现分区之后，这一数据项就可能分布到各个区里。容忍性就提高了。 然而，要把数据复制到多个节点，就会带来一致性的问题，就是多个节点上面的数据可能是不一致的。要保证一致，每次写操作就都要等待全部节点写成功，而这等待又会带来可用性的问题。 总的来说就是，数据存在的节点越多，分区容忍性越高，但要复制更新的数据就越多，一致性就越难保证。为了保证一致性，更新所有节点数据所需要的时间就越长，可用性就会降低。 理解数据库本地事务 分布式事务 ACID 原子性 atomicity：一个事务(transaction)中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。 一致性 consistency：事务的执行前后，是从一个一致性状态转移到另一个一致性状态。【是通过原子性和隔离性保证的。】 隔离性 isolation：事务并发执行时，每个事务有各自完整的数据空间。有不同的隔离级别，大部分通过锁实现。 持久性 durability：事务只要成功执行，对数据库所做的更新会永久保存下来。 隔离级别 innodb实现原理：主要通过锁和日志来保证ACID 通过锁机制和mvcc实现隔离性 redo log（重做日志）实现持久性 undo log实现原子性和一致性【可以回滚】 分布式事务 – 主要是要保证原子性 分布式事务 指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于不同的分布式系统的不同节点之上。 一次大操作由不同的小操作组成，小操作分布在不同的服务器上，且属于不同的应用，分布式服务需要保证这些小操作要么全部成功，要么全部失败。 分布式事务的场景 Service多个节点 – 指微服务等，比如一个交易平台，订单、库存、余额等在不同的服务下，一次交易需要原子性得更新。 resource多个节点 – 指分库分表了，比如转账双方的余额在不同的表里，一次转账双方都要正确更新。 理论 CAP BASE 解决方案 2PC 第一阶段：预提交，并反映是否可以提交。【事务管理器要求每个涉及到事务的数据库预提交(precommit)此操作，并反映是否可以提交.】 第二阶段：提交，或者回滚。【事务协调器要求每个数据库提交数据，或者回滚数据。】 优点：实现成本低 缺点：单点问题（事务管理器单点，可能引起资源管理器一直阻塞），同步阻塞（precommit后，资源管理器一直处于阻塞中，直到提交、释放资源），可能存在数据不一致（比如协调者发出了commit通知，但只有部分参与者收到通知并执行了commit，其余参与者则没收到通知处于阻塞状态，就产生不一致了） TCC try - confirm - cancel 协调者变成多点，引入进群 引入超时，超时后进行补偿，并且不会锁定整个资源，缓解同步阻塞 数据一致性：通过补偿机制，由业务活动管理器控制一致性 本地消息表：核心是将需要分布式处理的任务通过消息日志的方式来异步执行。消息日志可以存储到本地文本、数据库或消息队列，再通过业务规则自动或人工发起重试。 MQ事务 SAGA seata 分布式事务描述分布式事务之TCC服务设计？ 不了解 TCC分布式事务 try - confirm - cancel 高并发&amp;高性能&amp;高可用系统设计高并发&amp;高性能&amp;高可用 高并发：系统能够同时、并行处理很多请求，常用指标响应时间、吞吐量、并发用户数、QPS 如何提高： 垂直扩展 增加单机硬件性能，比如CPU增加核数、升级更好的网卡，换好的硬盘，扩充内存 提升单机架构性能。比如使用缓存，使用异步增加单服务吞吐量 水平扩展 增加服务器数量 高性能：程序处理速度快，占用内存少，CPU占用率低 高并发和高性能紧密相关，提供性能大部分可以提高并发 增加服务器资源（内存、CPU、服务器数量）绝大部分能提高并发和性能 注意事项： 避免因为io阻塞让CPU闲置，浪费CPU 避免频繁创建、销毁线程，导致浪费资源在调度上 高可用：减少停工时间，保持服务的高度可用性（一直能用） 全年停机不超过31.5秒 6个9：一直能用的概率为99.9999% 注意事项： 避免单点 使用集群 心跳机制，监控服务器状态，挂了就进行故障修复 限流算法缓存：提升系统访问速度和增大系统处理容量 降级：当服务器压力剧增的情况下，根据当前业务情况对一些服务和页面有策略地降级，以此释放服务器资源以保证核心任务的正常运行 限流：限流的目的是通过对并发访问/请求进行限速，或者对一个时间窗口内的请求进行限速来保护系统，一旦达到限制速率则可以拒绝服务、排队或等待、降级等处理 限流算法： 固定窗口法 实现：固定时间内限定个数，比如限定每分钟100个请求 缺点：无法应对两个时间边界内的突发流量。比如在计数器清零的前一秒和后一秒都进来100个请求，那么系统短时间内就收到了两倍（200个）请求，有可能超负荷。 原因：统计精度不够 滑动窗口法 实现：简单来说就是随着时间的推移，时间窗口也会持续移动，有一个计数器不断维护着窗口内的请求数量，这样就可以保证任意时间段内，都不会超过最大允许的请求数。例如当前时间窗口是0s~60s，请求数是40，10s后时间窗口就变成了10s~70s，请求数是60。 可以用Redis有序集合实现.. 缺点：还是不能解决细粒度请求过于集中的问题，比如限制一分钟60个请求，但在59s时发送了60个请求过来。 漏桶算法 算法思想：与令牌桶算法有点相反。不限制流入速率，但以固定的速度将水流出。如果流入速度太大会导致水满溢出，溢出的请求被丢弃。 实现一：基于queue。queue的大小表示桶的大小，queue满了请求会被拒绝；另维护一个定时器，根据设定的出水速度去queue中取一个任务，比如限定一秒钟5个请求，就200ms去取一个任务，取到就执行，取不到就轮空。 实现二：基于meter，计数器。【…写的不是很清楚，看起来和固定窗口法很像了，没有体现固定的出水，只表示时间粒度比较细】 令牌桶法 算法思路：以固定的速率生成令牌，把令牌放到固定容量的桶里，超过桶容量的令牌则丢弃。每来一个请求获取一次令牌，只有获得令牌的请求才能放行，没有获得令牌的请求丢弃。 【令牌是匀速生成的，如果请求超高频，则完全被限制成令牌的生成速率；如果请求突发，也最多只允许令牌数的上限。】 Guava RateLimiter 令牌桶与漏桶的比较 漏桶能够强行限制数据的传输速率 令牌桶限制数据的平均传输速率，允许某种程度的突发传输 【看起来比较喜欢令牌桶】 两次点击，怎么防止重复下订单？两次点击的场景 没有刷新和前端控制，同一个按钮点了两次 网络问题以为失败（其实成功了）又提交了一次 rpc等重试服务 刷新前后各点一次（或者表单刷新又提交了一次） 点了后退按钮，再前进 处理方案： 前端 弹出确认界面，或disable入口并倒计时等 后端 约定【所谓重复订单，需要定义这是相同的订单】，需要和客户端配合实现 比如支付可以用订单ID作判断 如果是下单，可以用uuid或服务端先生成一个全局唯一的订单ID，客户端如果未接收到下单成功的响应，多次重试都用这一个订单ID来提交。（如果是刷新，需要客户端去服务端请求最新购物车数据，已成功下单的商品已被移除；如果是未刷新页面的重试，则使用同一个订单ID；或者提示用户刷新、提示是否重试） 后端的去重判断方式 https://www.cnblogs.com/jett010/articles/9056567.html – 本质上分布式锁的应用 基于数据库中对应订单ID的状态做判断，ID已存在（下单），或者状态已变更（修改订单，比如取消、退款等）。如果查询和更新是分开的两个操作，会存在时间差，比如查询完后状态被别的线程修改了，可以用加数据库锁的方式解决这个问题（悲观锁或乐观锁） 利用数据库唯一性索引，性能比较低 Redis分布式锁，key是订单ID，要点是加锁和解锁的原子性 ps redis的计数器是原子操作 https://redis.io/commands/incr 秒杀场景设计，应付突然的爆发流量一个秒杀系统的设计思考 两个核心问题：并发读、并发写 对应到架构设计，就是高可用、一致性和高性能的要求 高性能：涉及高读和高写。 核心理念：高读-&gt;尽量“少读”或“读少”，高写-&gt;数据拆分 动静分离：将页面拆分，静态部分做CDN缓存（秒级失效，若干CDN节点），动态部分异步请求。 数据拆分、静态缓存、数据整合（指动态数据、静态数据怎么整合在一起，一种服务端将动态数据插入到静态页面，另一种前端异步调用） 热点优化 热点操作：零点刷新、零点下单、零点加购物车等，属于用户行为不好改变，但可以做一些限制，比如用户频繁刷新页面时进行提示阻断。 热点数据： 热点识别：分为静态热点（可以提前预测的。大促前夕，可以根据大促的行业特点、活动商家等纬度信息分析出热点商品，或者通过卖家报名的方式提前筛选；另外，还可以通过技术手段提前预测，例如对买家每天访问的商品进行大数据计算，然后统计出 TOP N 的商品，即可视为热点商品）和动态热点（无法提前预测的。比如直播临时做了个广告可能导致一件商品短期内被大量购买）。 动态热点的识别实现思路：1. 异步采集交易链路各个环节的热点key信息，比如nginx采集访问url或agent采集热点日志（一些中间件本身已具备热点发现能力），提前识别潜在的热点数据。2. 聚合分析热点数据，达到一定规则的热点数据，通过订阅分发推送到链路系统，各系统根据自身需求决定如何处理热点数据，或限流或缓存，从而实现热点保护 最好做到秒级实时，这样动态发现才有意义。实际上也是对核心节点的数据采集和分析能力提出了较高的要求。 热点隔离：将热点数据隔离出来，不影响非热点数据的访问。 – 【我怎么觉得参与秒杀的商品都可以直接作为热点呢？？】 业务隔离。秒杀作为一种营销活动，卖家需要单独报名，从技术上来说，系统可以提前对已知热点做缓存预热 – 【静态热点吧..】 系统隔离。系统隔离是运行时隔离，通过分组部署和另外 99% 进行分离，另外秒杀也可以申请单独的域名，入口层就让请求落到不同的集群中 – 【也是静态热点吧..】 热点数据，可以启用单独的缓存集群或者DB服务组，从而更好的实现横向或纵向能力扩展 – 【可以是动态的，假如一个商品被动态标记为热点后】 热点优化：对这1%的数据做针对性的优化 缓存：热点缓存是最为有效的办法。 限流：流量限制更多是一种保护机制。–属于有损服务。 系统优化：提升硬件水平、调优JVM性能、代码层面优化 代码层面优化：1. 减少序列化（减少RPC调用，一种可行的方案是将多个关联性较强的应用进行 “合并部署”，从而减少不同应用之间的 RPC 调用（微服务设计规范））2. 直接输出流数据（只要涉及字符串的I/O操作，无论是磁盘 I/O 还是网络 I/O，都比较耗费 CPU 资源，因为字符需要转换成字节，而这个转换又必须查表编码。所以对于常用数据，比如静态字符串，推荐提前编码成字节并缓存，具体到代码层面就是通过 OutputStream() 类函数从而减少数据的编码转换；另外，热点方法toString()不要直接调用ReflectionToString实现，推荐直接硬编码，并且只打印DO的基础要素和核心要素– 这整段不是很懂，toString懂啊哈哈）3. 裁剪日志异常堆栈，超大流量下频繁地输出完整堆栈，会加剧系统当前负载（可以通过日志配置文件控制异常堆栈输出的深度）4. 去组件框架：极致优化要求下，可以去掉一些组件框架，比如去掉传统的 MVC 框架，直接使用 Servlet 处理请求。这样可以绕过一大堆复杂且用处不大的处理逻辑，节省毫秒级的时间，当然，需要合理评估你对框架的依赖程度 一致性：秒杀场景下的一致性问题，主要是库存扣减的准确性问题 减库存的方式： 下单减库存（用户体验好，但存在恶意下单不付款的问题） 付款减库存（用户体验差，很多人下单成功但有人不能付款） 预扣库存：缓解了以上两种方式的问题。预扣库存实际就是“下单减库存”和 “付款减库存”两种方式的结合，将两次操作进行了前后关联，下单时预扣库存，付款时释放库存。 劣势：并没有彻底解决以上问题。比如针对恶意下单的场景，虽然可以把有效付款时间设置为 10 分钟，但恶意买家完全可以在 10 分钟之后再次下单。 实际业界也多用这种方式，下单后一般都有个 “有效付款时间”，超过该时间订单自动释放，是典型的预扣库存方案。 恶意下单的解决方案主要还是结合安全和反作弊措施来制止。比如，识别频繁下单不付款的买家并进行打标，这样可以在打标买家下单时不减库存；再比如为大促商品设置单人最大购买件数，一人最多只能买 N 件商品；又或者对重复下单不付款的行为进行次数限制阻断等 超卖分为两种：1. 商家可以补货的，允许超卖；2. 不允许超卖，限定库存字段不能为负数：1）一是在通过事务来判断，即保证减后库存不能为负，否则就回滚；2）直接设置数据库字段类型为无符号整数，这样一旦库存为负就会在执行 SQL 时报错 一致性性能的优化 高并发读：“分层校验”。即在读链路时，不做一致性校验，只做不影响性能的检查（如用户是否具有秒杀资格、商品状态是否正常、用户答题是否正确、秒杀是否已经结束、是否非法请求等），在写链路的时候，才对库存做一致性检查，在数据层保证最终准确性。 不同层次尽可能过滤掉无效请求，只在“漏斗” 最末端进行有效处理，从而缩短系统瓶颈的影响路径。 高并发写 更换DB选型：换用缓存系统（带有持久化功能的缓存，如Redis，适合减库存操作逻辑单一的，无事务要求的） 优化DB性能：库存数据落地到数据库实现其实是一行存储（MySQL），因此会有大量线程来竞争 InnoDB 行锁。但并发越高，等待线程就会越多，TPS 下降，RT 上升，吞吐量会受到严重影响。 两种方法： 应用层排队。加入分布式锁，控制集群对同一行记录进行操作的并发度，也能控制单个商品占用数据库连接的数量 数据层排队。（应用层排队是有损性能的，数据层排队是最为理想的。）业界中，阿里的数据库团队开发了针对InnoDB 层上的补丁程序（patch），可以基于DB层对单行记录做并发排队，从而实现秒杀场景下的定制优化。另外阿里的数据库团队还做了很多其他方面的优化，如 COMMIT_ON_SUCCESS 和 ROLLBACK_ON_FAIL 的补丁程序，通过在 SQL 里加入提示（hint），实现事务不需要等待实时提交，而是在数据执行完最后一条 SQL 后，直接根据 TARGET_AFFECT_ROW 的结果进行提交或回滚，减少网络等待的时间（毫秒级）。目前阿里已将包含这些补丁程序的 MySQL 开源：AliSQL。 高可用 流量削峰 答题：通过提升购买的复杂度，达到两个目的：防止作弊&amp;延缓请求。本质是通过在入口层削减流量，从而让系统更好地支撑瞬时峰值。 排队：最为常见消息队列，通过把同步的直接调用转换成异步的间接推送，缓冲瞬时流量。（弊端：请求积压、用户体验）（排队本质是在业务层将一步操作转变成两步操作，从而起到缓冲的作用，但鉴于此种方式的弊端，最终还是要基于业务量级和秒杀场景做出妥协和平衡。） 过滤：过滤的核心目的是通过减少无效请求的数据IO 保障有效请求的IO性能。 读限流：对读请求做限流保护，将超出系统承载能力的请求过滤掉 读缓存：对读请求做数据缓存，将重复的请求过滤掉 写限流：对写请求做限流保护，将超出系统承载能力的请求过滤掉 写校验：对写请求做一致性校验，只保留最终的有效数据 Plan B 架构阶段：考虑系统的可扩展性和容错性，避免出现单点问题。例如多地单元化部署，即使某个IDC甚至地市出现故障，仍不会影响系统运转 编码阶段：保证代码的健壮性，例如RPC调用时，设置合理的超时退出机制，防止被其他系统拖垮，同时也要对无法预料的返回错误进行默认的处理 测试阶段：保证CI的覆盖度以及Sonar的容错率，对基础质量进行二次校验，并定期产出整体质量的趋势报告 发布阶段：系统部署最容易暴露错误，因此要有前置的checklist模版、中置的上下游周知机制以及后置的回滚机制 运行阶段：系统多数时间处于运行态，最重要的是运行时的实时监控，及时发现问题、准确报警并能提供详细数据，以便排查问题 故障发生：首要目标是及时止损，防止影响面扩大，然后定位原因、解决问题，最后恢复服务 预防：建立常态压测体系，定期对服务进行单点压测以及全链路压测，摸排水位 管控：做好线上运行的降级、限流和熔断保护。需要注意的是，无论是限流、降级还是熔断，对业务都是有损的，所以在进行操作前，一定要和上下游业务确认好再进行。就拿限流来说，哪些业务可以限、什么情况下限、限流时间多长、什么情况下进行恢复，都要和业务方反复确认 监控：建立性能基线，记录性能的变化趋势；建立报警体系，发现问题及时预警 恢复：遇到故障能够及时止损，并提供快速的数据订正工具，不一定要好，但一定要有在系统建设的整个生命周期中，每个环节中都可能犯错，甚至有些环节犯的错，后面是无法弥补的或者成本极高的。所以高可用是一个系统工程，必须放到整个生命周期中进行全面考虑。同时，考虑到服务的增长性，高可用更需要长期规划并进行体系化建设。 监控集群监控的时候，重点需要关注哪些技术指标？这些指标如何优化？ 系统指标：cpu使用率、内存使用率、机器连通性、系统负载（cpu.load） 网络指标：网络流入速度、网络流出速度、网络流入包数、网络流出包数、TCP连接数、TCP Active Opens（主动打开数）、IP接收包数、IP丢包数、TCP接收包数、TCP发送包数、TCP包传输错误数、TCP重传数 磁盘指标：磁盘使用率、iNode使用率、磁盘繁忙占比、磁盘读速度、磁盘写速度、磁盘读次数、磁盘写次数 容器指标？：线程数[容器指标]/平均到每核、进程数[容器指标]/平均到每核…. 监控知识体系 [小米监控]( 面向对象简单说一下面向对象的特征以及六大原则特征： 封装：把客观事物封装成抽象的类 抽象继承！：（实现继承or接口继承）让某个类型的对象获得另一个类型的对象的属性和方法 多态：一个类实例的相同方法在不同情形有不同的表现形式。（编译多态与运行时多态）一般指运行时多态..? 多态存在的必要条件：继承、重写、父类引用指向子类对象 原则： 单一职责：一个类的功能要单一，不能包罗万象 开放封闭：一个模块，在扩展性方面应该是开发的，在更改性方面是封闭的 里氏替换：子类应当可以替换父类，并出现在父类能够出现的任何地方 依赖倒置：高层次的模块不应该依赖于低层次的模块，他们都应该依赖于抽象；抽象不应该依赖于具体实现，具体实现应该依赖于抽象。 接口分离：模块间要通过抽象接口隔离开，而不是通过具体的类强耦合起来，即面向接口编程。 迪米特原则：一个类对自己依赖的类知道的越少越好。类间解耦，低耦合、高内聚。 亿级数据从千万的数据到亿级的数据，会面临哪些技术挑战？你的技术解决思路？https://zhuanlan.zhihu.com/p/51081227 最常见的数据库，如MySql、Oracle等，都采用行式存储，比较适合OLTP。如果用MySql等行数据库来实现OLAP，一般都会碰到两个瓶颈： 数据量瓶颈：mysql比较适合的数据量级是百万级，再多的话，查询和写入性能会明显下降。因此，一般会采用分库分表的方式，把数据规模控制在百万级。 查询效率瓶颈：mysql对于常用的条件查询，需要单独建立索引或组合索引。非索引字段的查询需要扫描全表，性能下降明显。 分表 垂直业务拆分=分库+微服务（可分库基础上再分表） https://zhuanlan.zhihu.com/p/54594681 1亿个手机号码，判断重复不允许有误差的： hash到n个小文件中 每个文件做统计 个数大于1的是重复的 允许有误差的： 布隆过滤器 算法海量数据5亿整数的大文件，怎么排？内存估算 假设每个数最多64位，8字节 5,0000,0000x8 ~ 500MBx8 = 4000MB ~ 4G 假设5亿数不重复 内存装的下： 直接快排，得算好久吧.. 5亿个整数排序 内存装不下： 读文件，数取模%4000存入4000个小文件，每个文件约1M 读每个小文件，快排 多路归并排序输出 海量数据处理思路 分治/hash映射 + hash统计 + 堆/快排/归并排序 hash分成n个小文件，满足内存要求：好处是，可以让相同的数或字符串进入同一个小文件 小文件排序或统计，或没有本步骤，输出另一份小文件 最终要求 全排序：使用多路归并 找top k：直接小顶堆（找最大top k）or大顶堆；或者每个小文件先找top k，再对比n个top k 找两文件不同：两两小文件set对比 数据结构 bitmap 可用于整数去重等 trie树 名字来源retrieval 排序常见的排序算法 冒泡排序-复杂度O(n^2)-交换排序 对所有相邻记录的关键码值进行比较，如果是逆序（L.r[1].key &gt; L.r[2].key），则将其交换，最终达到有序化。 对无序区从前向后依次将相邻记录的关键码进行比较，若逆序，则将其交换，从而使得关键码值小的记录向上“飘浮”（左移），关键码值大的记录向下“坠落”（右移）。 每经过一趟冒泡排序，都使无序区中关键码值最大的记录进入有序区，对于由n个记录组成的记录序列，最多经过n-1趟冒泡排序，就可将这n个记录重新按关键码顺序排列。可看出，若“在一趟排序过程中没有进行过交换记录的操作”，则可结束整个排序过程。 123456789101112131415161718192021/** * 冒泡排序--更像坠落排序 * * @param nums */@Overridepublic void sort(T[] nums) &#123; int len = nums.length; boolean isSorted = false; // i区分无序区和有序区 for (int i = len - 1; i &gt;= 0 &amp;&amp; !isSorted; i--) &#123; isSorted = true; // j将大元素右移 for (int j = 0; j &lt; i; i++) &#123; if (less(nums[j + 1], nums[j])) &#123; isSorted = false; swap(nums, j, j + 1); &#125; &#125; &#125;&#125; 选择排序-复杂度O(n^2)-选择排序 每一趟从待排序的记录中选出关键码最小的记录，顺序放在已排好序的子序列后面，直到全部记录排序完毕。 123456789101112131415161718/** * 选择排序 * * @param nums */@Overridepublic void sort(T[] nums) &#123; for (int i = 0; i &lt; nums.length; i++) &#123; // index指向每轮最小的数 int index = i; for (int j = i + 1; j &lt; nums.length; j++) &#123; if (less(nums[j], nums[index])) &#123; index = j; &#125; &#125; swap(nums, i, index); &#125;&#125; 插入排序-复杂度O(n^2) -插入排序 基本思想是，将待排序的记录，按其关键码的大小插入到已经排好序的有序子表中，直到全部记录插入完成为止。 12345678910111213/** * 插入排序 * * @param nums */@Overridepublic void sort(T[] nums) &#123; for (int i = 1; i &lt; nums.length; i++) &#123; for (int j = i; j &gt; 0 &amp;&amp; less(nums[j], nums[j - 1]); j--) &#123; swap(nums, j, j - 1); &#125; &#125;&#125; 归并排序-复杂度O(nlogn)-归并排序 1234567891011121314151617181920212223242526272829303132333435363738/** * 归并排序 * * @param nums */public void sort(T[] nums, Class&lt;T&gt; clazz) &#123; T[] copy = (T[]) Array.newInstance(clazz, nums.length); System.arraycopy(nums, 0, copy, 0, nums.length); sort(nums, copy, 0, nums.length);&#125; private void sort(T[] nums, T[] copy, int begin, int end) &#123; if (begin + 1 == end) &#123; return; &#125; int half = (end - begin) / 2; sort(nums, copy, begin, begin + half); sort(nums, copy, begin + half, end); merge(copy, nums, begin, begin + half, end);&#125; private void merge(T[] nums, T[] copy, int begin, int mid, int end) &#123; int i = begin, j = mid, k = begin; while (i &lt; mid &amp;&amp; j &lt; end) &#123; if (nums[i].compareTo(nums[j]) &lt; 0) &#123; copy[k] = nums[i++]; &#125; else &#123; copy[k] = nums[j++]; &#125; k++; &#125; while (i &lt; mid) &#123; copy[k++] = nums[i++]; &#125; while (j &lt; end) &#123; copy[k++] = nums[j++]; &#125;&#125; 快速排序-复杂度O(nlogn)-交换排序 1234567891011121314151617181920212223242526272829/** * 快速排序 * * @param nums */@Overridepublic void sort(T[] nums) &#123; sort(nums, 0, nums.length - 1);&#125; private void sort(T[] nums, int begin, int end) &#123; int left = begin + 1, right = end; while (left &lt; right) &#123; while (left &lt;= end &amp;&amp; less(nums[left], nums[begin])) &#123; left++; &#125; while (right &gt;= begin &amp;&amp; less(nums[begin], nums[right])) &#123; right--; &#125; if (left &lt; right) &#123; swap(nums, left, right); &#125; &#125; if (right &lt;= end &amp;&amp; right &gt;= begin) &#123; swap(nums, begin, right); sort(nums, begin, right - 1); sort(nums, right + 1, end); &#125;&#125; 堆排序-复杂度O(nlogn)-堆排序 位置 k 的节点的父节点位置 为 k/2，而它的两个子节点的位置分别为 2k 和 2k+1。 12345678910111213141516171819202122232425262728293031323334353637383940/** * 堆排序 排成最大堆 * 数组第 0 个位置不能有元素 * * @param nums */@Overridepublic void sort(T[] nums) &#123; int cnt = nums.length - 1; for (int k = cnt / 2; k &gt;= 1; k--) &#123; sink(nums, k, cnt); &#125; while (cnt &gt; 1) &#123; swap(nums, 1, cnt); cnt--; sink(nums, 1, cnt); &#125;&#125; /** * 下沉 * * @param nums * @param k */private void sink(T[] nums, int k, int len) &#123; while (k * 2 &lt;= len) &#123; int child = k * 2; // 判断child + 1未越界 if (child + 1 &lt; len &amp;&amp; less(nums[child], nums[child + 1])) &#123; child++; &#125; // 如果子节点比k小，退出循环 if (less(nums[child], nums[k])) &#123; break; &#125; swap(nums, k, child); k = child; &#125;&#125; https://book.open-falcon.org/zh_0_2/intro/) 其他最近两年遇到的最大的挫折，从挫折中学到了什么？胃炎？失眠？ 生病要看病，不要自己琢磨；看病也是要厚脸皮的，多问；运动，保持良好生活习惯。 – 这么回答..太坑了吧 最近有没有学习过新技术？zookeeper kafka json:Gson/jackson/fastjson 设计模式：尽量在工作中合适地应用 docker简单使用","tags":[]},{"title":"InnoDB存储引擎-学习笔记","date":"2020-07-20T11:17:44.000Z","path":"2020/07/20/InnoDB存储引擎-学习笔记/","text":"第2章 InnoDB存储引擎2.1 概述MySQL5.5开始默认存储引擎 特点是行锁设计、支持MVCC、支持外键、提供一致性非锁定读 2.2 InnoDB存储引擎的版本最新的是InnoDB 1.2.x 2.3 InnoDB体系架构文件+内存池+后台线程 2.3.1 后台线程主要作用 刷新内存池中的数据，保证缓存的是最近的数据 将已修改数据文件刷新到磁盘文件 保证在数据库发生异常的情况下，InnoDB能恢复到正常运行状态 Master Thread：核心后台线程 将缓冲池中的数据异步刷新到磁盘，保证数据的一致性。包括脏页的刷新、合并插入缓冲（insert buffer）、UNDO页的回收等 IO Thread：负责IO请求的回调处理 write、read、insert buffer、log io thread innodb使用AIO（异步io）来处理io请求 Purge Thread innodb 1.1引入，原本操作也在master Thread中 事务被提交后，其所使用的undolog可能不再需要，因此需要Purge Thread来回收已经使用并分配的undo页 Page Cleaner Thread innodb1.2.x引入的，脏页的刷新操作 2.3.2 内存 缓冲池 缓冲池中缓存的数据页类型有：索引页、数据页、undo页【图片里是redo log_buffer重做日志缓冲】、插入缓冲（insert buffer）、自适应哈希索引（adaptive hash index）、InnoDB存储的锁信息（lock info）、数据字典信息（data dictionary）等。 LRU List、Free List和Flush List 数据库中的缓冲池是通过LRU（Latest Recent Used，最近最少使用）算法来进行管理的。即最频繁使用的页在LRU列表的前端，而最少使用的页在LRU列表的尾端。 优化：midpoint 最新读到的页，不是直接放入LRU列表的首部，而是放入列表的midpoint位置，一般在5/8处。 在InnoDB存储引擎中，把midpoint之后的列表称为old列表，之前的列表称为new列表。可以简单地理解为new列表中的页都是最为活跃的热点数据。 为什么不直接放首部？某些SQL操作会使缓冲池中的页被刷出，影响缓冲池的效率。比如索引或数据的扫描操作，需要访问表中的许多页，甚至全部的页，（而这些页可能只在本次查询需要，并不是活跃的热点数据。）会将热点数据从缓冲池中刷出，下次需要再加载，innodb需要再次访问磁盘。 重做日志缓冲 额外的内存池 2.4 Checkpoint技术Write Ahead Log策略，即当事务提交时，先写重做日志，再修改页。 因此Checkpoint（检查点）技术的目的是解决以下几个问题： 缩短数据库的恢复时间； 缓冲池不够用时，将脏页刷新到磁盘； 重做日志不可用时，刷新脏页。 2.5 MasterThread工作方式各个版本有些微差异，比如刷新脏页的阈值有差异等，但主要工作是以下内容。 另外，刷新脏页的操作已经从master Thread分离到了单独的线程page cleaner thread. 减轻master Thread的工作，提高系统并发性。 每秒一次的操作包括： == srv_master_do_active_tasks 日志缓冲刷新到磁盘，即使这个事务还没有提交（总是） 合并插入缓冲（可能） 至多刷新100个InnoDB的缓冲池中的脏页到磁盘（可能） – page cleaner 如果当前没有用户活动，则切换到background loop（可能）。 10秒的操作，包括如下内容： == srv_master_do_idle_tasks 刷新100个脏页到磁盘（可能的情况下） – page cleaner 合并至多5个插入缓冲（总是） 将日志缓冲刷新到磁盘（总是） 删除无用的Undo页（总是）– purge thread 刷新100个或者10个脏页到磁盘（总是）。 – page cleaner 2.6 InnoDB关键特性InnoDB存储引擎的关键特性包括： 插入缓冲（Insert Buffer） 两次写（Double Write） 自适应哈希索引（Adaptive Hash Index） 异步IO（Async IO） 刷新邻接页（Flush Neighbor Page） 网上看到的特性是 1+2+3+预读，read ahead 2.6.1 插入缓冲insert buffer，和数据页一样，也是物理页的组成部分。 insert buffer的使用场景，非唯一辅助索引的插入操作。 具体实现是B+树 1.0.x引入Change Buffer，是insert buffer的升级。对DML操作——insert、delete、update都进行缓冲，分别是insert buffer、delete buffer、Purge buffer。 对一条记录进行UPDATE操作可能分为两个过程： 将记录标记为已删除； 真正将记录删除。 因此Delete Buffer对应UPDATE操作的第一个过程，即将记录标记为删除。PurgeBuffer对应UPDATE操作的第二个过程，即将记录真正的删除 2.6.2 两次写 doublewrite由两部分组成，一部分是内存中的doublewrite buffer，大小为2MB，另一部分是物理磁盘上共享表空间中连续的128个页，即2个区（extent），大小同样为2MB。 doublewrite发生在对缓冲池的脏页进行刷新的时候，不直接写磁盘，先memcpy将脏页复制到doublewrite buffer，之后doublewrite buffer每次1MB写入共享表空间的物理磁盘（马上调用fsync，避免缓冲写）（写入是顺序的）。doublewrite写完后，再将页写入各个表空间（写入是离散的）。 2.6.3 自适应哈希索引对象是索引页。 根据访问频率和模式自动为某些热点页建立哈希索引。 2.6.4 异步IO第3章 文件3.1 参数文件3.2日志文件3.2.1 错误日志错误日志文件对MySQL的启动、运行、关闭过程进行了记录。 慢查询日志 查询日志 二进制日志 问题汇总1. 三大范式？第一范式：每个列都不可以再拆分。 第二范式：在第一范式的基础上，非主键列完全依赖于主键，而不能是依赖于主键的一部分。 第三范式：在第二范式的基础上，非主键列只依赖于主键，不依赖于其他非主键。 2. 数据类型https://dev.mysql.com/doc/refman/8.0/en/integer-types.html 整数类型。 可以指定长度，不会限制值的范围，只会影响显示字符的个数 Type Storage (Bytes) Minimum Value Signed Minimum Value Unsigned Maximum Value Signed Maximum Value Unsigned TINYINT 1 -128 0 127 255 SMALLINT 2 -32768 0 32767 65535 MEDIUMINT 3 -8388608 0 8388607 16777215 INT 4 -2147483648 0 2147483647 4294967295 BIGINT 8 -263 0 263-1 264-1 https://dev.mysql.com/doc/refman/8.0/en/char.html 字符串类型，varchar有1-2位存长度，列长度小于255字节时，使用1字节表示，否则使用2字节表示。 Value CHAR(4) Storage Required VARCHAR(4) Storage Required &#39;&#39; &#39; &#39; 4 bytes &#39;&#39; 1 byte &#39;ab&#39; &#39;ab &#39; 4 bytes &#39;ab&#39; 3 bytes &#39;abcd&#39; &#39;abcd&#39; 4 bytes &#39;abcd&#39; 5 bytes &#39;abcdefgh&#39; &#39;abcd&#39; 4 bytes &#39;abcd&#39; 5 bytes https://dev.mysql.com/doc/refman/8.0/en/innodb-limits.html innodb限制 最大列数1017 innodb 加上blob text等（off-page）数据，行大小不能超过4G mysql 行大小 65535bytes字节 备查","tags":[]},{"title":"算法导论学习笔记","date":"2020-07-15T10:31:16.000Z","path":"2020/07/15/算法导论学习笔记/","text":"第12章 二叉查找树查找树search tree 操作：search、minimum、maximum、predecessor、successor、insert和delete 可以用作字典，也可以用作优先队列 二叉查找树 binary search tree 执行基本操作的时间与树的高度成正比。n个节点的完全二叉树，最坏运行时间O(lgn)；n个节点的线性链，最坏运行时间O(n)；一棵随机构造的二叉树的期望高度是O(lgn)，从而操作的平均时间为O(lgn)。 12.1 二叉查找树中序遍历（即根节点在中间） 【记忆：中序遍历的结果是递增有序的】 12345INORDER-TREE-WALK(x) if x!=NIL then INORDER-TREE-WALK(x.left) print x.key INORDER-TREE-WALK(x.right) 12.2 查询二叉查找树123456TREE-SEARCH(x,k) if x = NIL or x.key = k then return x if k &lt; x.key then return TREE-SEARCH(x.left,k) else return TREE-SEARCH(x.right,k) 最大关键字元素 TREE-MAXIMUM 沿着节点的right指针，直到NIL 最小关键字元素 TREE-MINIMUM 沿着节点的left指针，直到NIL 中序遍历下的前驱 和后继对称 如果节点x有左子树，返回左子树的TREE-MAXIMUM 如果没有左子树，向上找一个节点，该节点的右孩子是节点x的祖先 中序遍历下的后继 如果节点x有右子树，后继为这个右子树的最小关键字元素 （右子树的最左节点） 如果没有右子树，后继为节点向上寻找，找到某个节点，这个节点的左孩子是节点x的祖先 12.3 插入和删除插入 沿根节点向下.. 删除 如果z没有子女，则将它删除（修改z的父节点，使NIL成为它的子女） 如果z只有一个子女，则删除z（z.parent指向z.xChild） 如果z有两个子女，则删除其后继y（可知，y至多有一个右孩子，否则左孩子比y小，y不会是后继；删除y，y.parent指向y.rightChild），把y的值赋给z 第13章 红黑树13.1 红黑树的性质 每个节点是红色的，或是黑色的 根节点是黑色的 每个叶子节点（Nil）是黑色的 如果一个节点是红色的，则它的两个子节点是黑色的 对每个节点，从该节点到其子孙节点的所有路径上包含相同个数的黑色节点。（红节点不能有红孩子）（从该节点出发的所有下降路径，有相同的黑节点个数） 13.2 旋转 左旋 x向左下垂 1LEFT_ROTATE(T,x) 右旋 x向右上移 1RIGHT_ROTATE(T,x) 13.3 插入先按照二叉查找树插入，节点为红色，再调整颜色和旋转 调整颜色： z一直指向一个红节点，如果p[z]也是红节点 如果z的叔叔y，是红色 那么，p[z]和y着黑色，p[p[z]]着红色，z指向p[p[z]]，循环继续 如果叔叔y是黑色 z是右孩子 将z左旋，z指向z的左孩子，变成下面2的情况 z是左孩子 p[z]着黑色，p[p[z]]着红色，将p[z]右旋 此时，z红色，p[z]黑色，退出循环 13.4 删除先按照二叉查找树删除 如果删除的节点是黑色的，需要调整颜色和旋转","tags":[]},{"title":"MyBatis与MyBatis-Spring","date":"2020-07-11T14:58:21.000Z","path":"2020/07/11/MyBatis与MyBatis-Spring/","text":"MyBatis 消除了大部分 JDBC 的样板代码、手动设置参数以及检索结果。通过简洁的设计最大限度地简化开发和提升性能。 解除SQL与程序代码的耦合，通过提供dao层，将业务逻辑和数据访问逻辑分离开。设计更清晰，更易维护。 MyBatis整体架构 MyBatis层级结构 裸用sqlSession是上面的红框 spring用mapper/dao接口代理，本质上是一个MapperProxy，从下面的红框开始执行 spring事务是在哪个环节起作用？ https://mybatis.org/spring/zh/transactions.html 一个使用 MyBatis-Spring 的其中一个主要原因是它允许 MyBatis 参与到 Spring 的事务管理中。而不是给 MyBatis 创建一个新的专用事务管理器，MyBatis-Spring 借助了 Spring 中的 DataSourceTransactionManager 来实现事务管理。 一旦配置好了 Spring 的事务管理器，你就可以在 Spring 中按你平时的方式来配置事务。并且支持 @Transactional 注解和 AOP 风格的配置。在事务处理期间，一个单独的 SqlSession 对象将会被创建和使用。当事务完成时，这个 session 会以合适的方式提交或回滚。 事务配置好了以后，MyBatis-Spring 将会透明地管理事务。 所以，最外层是事务，每个事务会起一个SqlSession。 几篇文章： 入门，裸用mybatis：https://juejin.im/post/5aa5c6fb5188255587232e5a#heading-0 mybatis执行，包括整合spring后的流程：https://juejin.im/post/5e350d895188254dfd43def5#heading-9 关于JDBC：https://juejin.im/post/5c75e6666fb9a049cd54dc88 Mybatis和spring整合的使用：https://juejin.im/post/5cdfed6ef265da1b6720dcaf mybatis框架说明： 整体执行流程说明： sqlSession执行流程说明： 关键流程（以下整个可以看成裸用MyBatis的执行流程） config文件加载：解析xml文件配置项 mapper文件加载：上一个流程中的一个环节，解析完后封装成MappedStatement，存入configuration SqlSource创建流程：上一流程的一个环节，SqlSource是MappedStatement的一部分，主要存放sql和占位的参数名称 – 解析环节结束 SqlSession执行流程：sqlSessionFactory.openSession主要是建立了一个和数据库的连接connection 获取BoundSql流程：sqlSession.xx方法执行时，需要获取BoundSql，BoundSql本质上是SqlSource和执行请求的入参的一个组合 参数映射流程：根据顺序，或者根据名称（只是大略看了一眼） 结果集映射流程：根据名称（只是大略看了一眼） mybatis的openSession默认开启事务，autocommit为false，隔离级别为null mybatis的JdbcTransaction 整合spring的几个组件 org.mybatis.spring.SqlSessionFactoryBean 注入sqlSessionFactory org.mybatis.spring.mapper.MapperScannerConfigurer扫描指定包 将包下class文件加入到beanDefinition中，bean类型指定为MapperFactoryBean SqlSessionFactoryBean构建sqlSessionFactory时，扫描mapper xml文件，根据namespace在MapperRegistry中注入对应mapper接口的MapperProxyFactory MapperFactoryBean-&gt;getObject中生成mapper的代理类MapperProxy（通过MapperFactoryBean中的interface，即mapper的namespace找到MapperProxyFactory，再生产出代理类） 以下大概知道了 现在差一个中间环节，mapper的beanDefinition怎么变成MapperProxy..以及MapperFactoryBean的作用 还有个SqlSessionTemplate：https://juejin.im/post/5cea1f386fb9a07ea803a70e 还有MapperProxyFactory – 来创建MapperProxy Java动态代理：https://juejin.im/post/5c1ca8df6fb9a049b347f55c MapperFactoryBean MapperProxy MapperMethod – 到这里之后，流程就转到sqlSession.selectOne之类的了 Mybatis缓存 https://juejin.im/post/5e81fb126fb9a03c546c22bb MyBatis 系统中默认定义了两级缓存：一级缓存和二级缓存 默认情况下，只有一级缓存开启。（SqlSession级别的缓存，也称为本地缓存） 二级缓存需要手动开启和配置，它是基于 namespace 级别的缓存，缓存只作用于 cache 标签所在的映射文件中的语句。 spring 事务实现 Spring事务的底层依赖MySQL的事务，代码层面上利用AOP实现。 常用的是@Transactional注解，会被解析生成一个代理服务，TransactionInterceptor对它进行拦截处理，进行事务开启、 commit或者rollback的操作。 另外，spring还定义了事务传播行为，有7种类型，项目中常见的是PROPAGATION_REQUIRED。如果没有事务就新建事务，如果存在事务，就加入这个事务。 执行事务的时候使用TransactionInterceptor进行拦截，然后处理 事务传播行为 事务传播行为类型 说明 PROPAGATION_REQUIRED 如果当前没有事务，就新建一个事务，如果已经存在一个事务中，加入到这个事务中。这是最常见的选择。（如果父方法有事务，加入父方法的事务；父方法没有事务，则自己新建一个事务） PROPAGATION_SUPPORTS 支持当前事务，如果当前没有事务，就以非事务方式执行。（如果父方法有事务，加入父方法的事务；父方法没有事务，则以非事务执行） PROPAGATION_MANDATORY 使用当前的事务，如果当前没有事务，就抛出异常。（依赖父方法事务） PROPAGATION_REQUIRES_NEW 新建事务，如果当前存在事务，把当前事务挂起。（如果父方法有事务，把父方法事务挂起，自己新建事务；父方法没有事务，则自己新建一个事务） PROPAGATION_NOT_SUPPORTED 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。（如果父方法有事务，把父方法事务挂起，以非事务执行自己的操作；父方法没有事务，则以非事务执行）（总是以非事务执行，不报错） PROPAGATION_NEVER 以非事务方式执行，如果当前存在事务，则抛出异常。（总是以非事务执行，如果父方法存在事务，抛异常） PROPAGATION_NESTED 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与PROPAGATION_REQUIRED类似的操作。 REQUIRED、REQUIRES_NEW、NESTED的对比 REQUIRED共用一个事务。 REQUIRES_NEW 有独立的子事务，子事务异常不会导致父事务回滚，父事务异常也不会导致子事务回滚，相互独立。 NESTED 子事务嵌套在父事务中，父事务回滚会引起子事务回滚；父事务正常、子事务异常，子事务可以单独回滚。 源码详解 txNamespaceHandle注册的InfrastructureAdvisorAutoProxyCreator是一个BeanPostProcessor，主要是为了创建动态代理（wrapIfNecessary） 这几个类是可以自动创建代理的 在创建代理的时候，获取切面 txNamespaceHandler注册了一个Advisor（BeanFactoryTransactionAttributeSourceAdvisor），再在这个advisor中判断是否当前bean符合这个切面（主要实现就是看有没有@Transactional注解） TransactionInterceptor是advice，增强，执行切面工作 摘录：https://my.oschina.net/fifadxj/blog/785621 spring-jdb的事务流程： 1234567891011121314DefaultTransactionDefinition def = new DefaultTransactionDefinition();PlatformTransactionManager txManager = new DataSourceTransactionManager(dataSource);TransactionStatus status = txManager.getTransaction(def);try &#123; //get jdbc connection... //execute sql... txManager.commit(status);&#125;catch (Exception e) &#123; txManager.rollback(status); throw e;&#125; PlatformTransactionManager的getTransaction(), rollback(), commit()是spring处理事务的核心api，分别对应事务的开始，提交和回滚。 TransactionSynchronizationManager负责从ThreadLocal中存取jdbc connection 创建事务的时候会通过dataSource.getConnection()获取一个新的jdbc connection，然后绑定到ThreadLocal 在业务代码中执行sql时，通过DataSourceUtils.getConnection()从ThreadLocal中获取当前事务的jdbc connection, 然后在该jdbc connection上执行sql commit和rollback事务时，从ThreadLocal中获取当前事务的jdbc connection，然后对该jdbc connection进行commit和rollback mybatis-spring的事务流程： 配置 1234567891011121314&lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\" /&gt;&lt;/bean&gt;&lt;bean id=\"sqlSessionFactory\" class=\"org.mybatis.spring.SqlSessionFactoryBean\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\" /&gt; &lt;property name=\"transactionFactory\"&gt; &lt;bean class=\"org.apache.ibatis.spring.transaction.SpringManagedTransactionFactory\" /&gt; &lt;/property&gt; &lt;/bean&gt;&lt;bean id=\"sqlSession\" class=\"org.mybatis.spring.SqlSessionTemplate\"&gt; &lt;constructor-arg index=\"0\" ref=\"sqlSessionFactory\" /&gt;&lt;/bean&gt; mybatis-spring依赖DataSourceTransactionManager来处理事务，并没有创建自己的PlatformTransactionManager实现。 mybatis通过SqlSessionFactoryBuilder创建SqlSessionFactory，而mybatis-spring通过SqlSessionFactoryBean创建SqlSessionFactory。 配置使用SpringManagedTransactionFactory来创建MyBatis的Transaction实现SpringManagedTransaction 配置使用SqlSessionTemplate代替通过SqlSessionFactory.openSession()获取SqlSession 调用过程 可以看到mybatis-spring处理事务的主要流程和spring jdbc处理事务并没有什么区别，都是通过DataSourceTransactionManager的getTransaction(), rollback(), commit()完成事务的生命周期管理，而且jdbc connection的创建也是通过DataSourceTransactionManager.getTransaction()完成，mybatis并没有参与其中，mybatis只是在执行sql时通过DataSourceUtils.getConnection()获得当前thread的jdbc connection，然后在其上执行sql。 sqlSessionTemplate是DefaultSqlSession的一个代理类，它通过SqlSessionUtils.getSqlSession()试图从ThreadLocal获取当前事务所使用的SqlSession。如果是第一次获取时会调用SqlSessionFactory.openSession()创建一个SqlSession并绑定到ThreadLocal，同时还会通过TransactionSynchronizationManager注册一个SqlSessionSynchronization。 SqlSessionSynchronization是一个事务生命周期的callback接口，mybatis-spring通过SqlSessionSynchronization在事务提交和回滚前分别调用DefaultSqlSession.commit()和DefaultSqlSession.rollback() 这里的DefaultSqlSession只会进行一些自身缓存的清理工作，并不会真正提交事务给数据库，原因是这里的DefaultSqlSession使用的Transaction实现为SpringManagedTransaction，SpringManagedTransaction在提交事务前会检查当前事务是否应该由spring控制，如果是，则不会自己提交事务，而将提交事务的任务交给spring，所以DefaultSqlSession并不会自己处理事务。 DefaultSqlSession执行sql时，会通过SpringManagedTransaction调用DataSourceUtils.getConnection()从ThreadLocal中获取jdbc connection并在其上执行sql。 mybatis-spring做的最主要的事情是： 在SqlSession执行sql时通过用SpringManagedTransaction代替mybatis的JdbcTransaction，让SqlSession从spring的ThreadLocal中获取jdbc connection。 通过注册事务生命周期callback接口SqlSessionSynchronization，让SqlSession有机会在spring管理的事务提交或回滚时清理自己的内部缓存。","tags":[]},{"title":"算法汇总","date":"2020-07-03T19:25:39.000Z","path":"2020/07/04/算法汇总/","text":"动态规划斐波那契数列爬楼梯每次可以上一阶或者两阶，求有多少种上楼梯的方法。 递推公式：f(n)=f(n-1)+f(n-2) 强盗抢劫抢劫一排住户，但是不能抢邻近的住户，求最大抢劫金额。 nums[i] 住户的财富 递推公式：f(n)=max(f(n-2)+nums[n], f(n-1)) 强盗在环形街区抢劫递推公式和上一题一样，主要考虑，抢和不抢第一间 抢第一间住户，则不能抢n，范围[0,n-1) 不抢第一间，范围[1,n) 信件错排所有元素的编号与位置号都不相等的情况叫作错排 递推公式：f(n)=(n-1)*f(n-2)+(n-1)*f(n-1) 简单说明： f(n)表示n个信件错排的情况数 假设有n个信件，对信件n来说，放错的位置有n-1个，假设放在了位置k上（k!=n） 情况1，位置k上信件放在位置n，剩下n-2个信件有f(n-2)种错排，这n-2个信件都不会放在位置n上 情况2，位置k上的信件不放在位置n，则可以将位置n考虑成一个新的第’k’位，包扩k在内的n-1个元素的每一种错排，都等价于只有n-1个元素的错排（只是其中的第k位需要换为第n位）【这么想的话，作为新的第’k’位，信件k肯定是不会放在第’k’位上的！也就完全不会和情况1有重合情况了！】 情况1和情况2有重叠吗？——答案是没有重叠！ 母牛生产每只小母牛生下来算0岁，第1年1岁，第2年2岁，第3年3岁，3岁称为成熟母牛，会生下一只小母牛。 现在，第一年有1只2岁小母牛，第二年生下1只小母牛。问第n年，有多少只成熟母牛？ n=1, res = 0 n=2, res = 1 n=3, res = 1 n=4, res = 1 n=5, res = 2 n=6, res = 3 每一年的成熟母牛数等于前一年的成熟母牛数加上三年前的成熟母牛数（生下来的宝宝数，因为过去三年了，宝宝变成成熟母牛了）。 f(n)=f(n-1)+f(n-3) f5 = f4+f2 = 1+1=2 f6 = f5+f3 = 2+1=3 f7=f6+f4=3+1=4 f8=f7+f5=4+2=6 矩阵路径矩阵的最小路径和从左上到右下，找最小的路径和。只能向右、或向下移动。 dp[i][j]=min(dp[i-1][j],dp[i][j-1])+grid[i][j] 只能从上边、或者左边的格子来。 可以优化空间。 12345678910111213141516171819public int minPathSum(int[][] grid) &#123; if (grid == null || grid.length == 0 || grid[0].length == 0) return 0; int rows = grid.length, cols = grid[0].length; int[] dp = new int[cols]; for (int i = 0; i &lt; rows; i++) &#123; for (int j = 0; j &lt; cols; j++) &#123; if (j == 0) &#123; // 第一列，只能从上一行来。这里有个小技巧，先判断j，规避掉dp[0-1] dp[j] = dp[j]; // 这一行可以不赋值，空着 &#125; else if (i == 0) &#123; // 第一行，只能从左边来 dp[j] = dp[j - 1]; &#125; else &#123; dp[j] = Math.min(dp[j - 1], dp[j]); &#125; dp[j] += grid[i][j]; &#125; &#125; return dp[cols - 1];&#125; 矩阵的总路径数规则和上一题一样，求唯一路径有多少条。 dp解法：dp[i][j] = dp[i-1][j] + dp[i][j-1] 从左边和上边（同样可以优化空间，同上） 数学解法：移动总次数s = m+n-2，向下次数d=n-1，唯一路径数 = s中选d个的组合，C(s,d) 1234567891011public int uniquePaths(int m, int n) &#123; if(m&lt;=0 || n&lt;=0)&#123; return 0; &#125; int s = m+n-2, d = Math.min(m-1,n-1); long res=1; // 会溢出 for(int i=1;i&lt;=d;i++)&#123; res = res * (s-d+i)/i; // 这种写法才能整除 &#125; return (int)res;&#125; 数组区间等差递增子区间的个数A = [0, 1, 2, 3, 4] dp[i] 表示以 A[i] 为结尾的等差递增子区间的个数。 当 A[i] - A[i-1] == A[i-1] - A[i-2]，那么 [A[i-2], A[i-1], A[i]] 构成一个等差递增子区间。而且在以 A[i-1] 为结尾的递增子区间的后面再加上一个 A[i]，一样可以构成新的递增子区间。 123456789dp[2] = 1 [0, 1, 2]dp[3] = dp[2] + 1 = 2 [0, 1, 2, 3], // [0, 1, 2] 之后加一个 3 [1, 2, 3] // 新的递增子区间dp[4] = dp[3] + 1 = 3 [0, 1, 2, 3, 4], // [0, 1, 2, 3] 之后加一个 4 [1, 2, 3, 4], // [1, 2, 3] 之后加一个 4 [2, 3, 4] // 新的递增子区间 因为递增子区间不一定以最后一个元素为结尾，可以是任意一个元素结尾，因此需要返回 dp 数组累加的结果。 12345678910111213141516171819class Solution &#123; public int numberOfArithmeticSlices(int[] nums) &#123; if (nums == null || nums.length &lt; 3) &#123; return 0; &#125; int n = nums.length; int[] dp = new int[n]; for (int i = 2; i &lt; n; i++) &#123; if (nums[i] - nums[i - 1] == nums[i - 1] - nums[i - 2]) &#123; dp[i] = dp[i - 1] + 1; &#125; &#125; int total = 0; for (int cnt : dp) &#123; total += cnt; &#125; return total; &#125;&#125; 另外一种数学的方式…可惜之前想多了，想岔了…直接判断A[i] - A[i - 1] == A[i - 1] - A[i - 2]就好！.. 注意count*(count+1)/2 123456789101112131415public class Solution &#123; public int numberOfArithmeticSlices(int[] A) &#123; int count = 0; int sum = 0; for (int i = 2; i &lt; A.length; i++) &#123; if (A[i] - A[i - 1] == A[i - 1] - A[i - 2]) &#123; count++; &#125; else &#123; sum += (count + 1) * (count) / 2; count = 0; &#125; &#125; return sum += count * (count + 1) / 2; &#125;&#125; 分割整数分割整数的最大乘积要么和整数乘，要么和整数的因子乘 123456789101112131415public int integerBreak(int n) &#123; if (n &lt;= 0) &#123; return 0; &#125; int[] dp = new int[n + 1]; dp[1] = 1; for (int i = 2; i &lt;= n; i++) &#123; for (int j = 1; j &lt;= i - 1; j++) &#123; int p1 = j * (i - j); int p2 = dp[j] * (i - j); dp[i] = Math.max(dp[i], Math.max(p1, p2)); &#125; &#125; return dp[n];&#125; 按平方数来分割整数组成n的最少平方数个数。 Example 1: 123Input: n = 12Output: 3 Explanation: 12 = 4 + 4 + 4. Example 2: 123Input: n = 13Output: 2Explanation: 13 = 4 + 9. 123456789101112131415161718192021public int numSquares(int n) &#123; List&lt;Integer&gt; squares = new ArrayList&lt;&gt;(); int square = 1, diff = 3; while (square &lt;= n) &#123; squares.add(square); square += diff; diff += 2; &#125; int[] dp = new int[n + 1]; for (int i = 1; i &lt;= n; i++) &#123; dp[i] = i; for (int v : squares) &#123; if (v &gt; i) &#123; break; &#125; dp[i] = Math.min(dp[i], dp[i - v] + 1); // 用平方数取前一个数最小个数 &#125; &#125; return dp[n];&#125; 分割整数映射字母A message containing letters from A-Z is being encoded to numbers using the following mapping: 1234&apos;A&apos; -&gt; 1&apos;B&apos; -&gt; 2...&apos;Z&apos; -&gt; 26 Given a non-empty string containing only digits, determine the total number of ways to decode it. Example 1: 123Input: &quot;12&quot;Output: 2Explanation: It could be decoded as &quot;AB&quot; (1 2) or &quot;L&quot; (12). Example 2: 123Input: &quot;226&quot;Output: 3Explanation: It could be decoded as &quot;BZ&quot; (2 26), &quot;VF&quot; (22 6), or &quot;BBF&quot; (2 2 6). dp[i]表示i个字符串的解法，d[i+1]新加入的字符，分为两种情况： 自己可以映射成字母 和前一个字符组合可以映射成字母 1234567891011121314151617181920public int numDecodings(String s) &#123; if (s == null || s.length() == 0) &#123; return 0; &#125; int n = s.length(); int[] dp = new int[n + 1]; dp[0] = 1; dp[1] = s.charAt(0) == '0' ? 0 : 1; for (int i = 2; i &lt;= n; i++) &#123; int first = Integer.parseInt(s.substring(i - 1, i)); int second = Integer.parseInt(s.substring(i - 2, i)); if (first &gt;= 1 &amp;&amp; first &lt;= 9) &#123; dp[i] += dp[i - 1]; &#125; if (second &gt;= 10 &amp;&amp; second &lt;= 26) &#123; dp[i] += dp[i - 2]; &#125; &#125; return dp[n];&#125; 最长递增子序列最长递增子序列O(n^2)解法。还有O(nlogn)解法看不懂！ 1234567891011121314151617181920212223public int lengthOfLIS(int[] nums) &#123; if (nums == null || nums.length == 0) &#123; return 0; &#125; int n = nums.length; int[] dp = new int[n]; dp[0] = 1; for (int i = 1; i &lt; n; i++) &#123; dp[i] = 1; for (int j = 0; j &lt; i; j++) &#123; if (nums[i] &gt; nums[j]) &#123; dp[i] = Math.max(dp[i], dp[j] + 1); &#125; &#125; &#125; int max = 0; for (int cnt : dp) &#123; if (cnt &gt; max) &#123; max = cnt; &#125; &#125; return max;&#125; 最长公共子序列最长公共子序列dp[i][j表示 S1 的前 i 个字符与 S2 的前 j 个字符最长公共子序列的长度。 1234567891011121314151617public int longestCommonSubsequence(String s1, String s2) &#123; if (s1 == null || s2 == null || s1.length() == 0 || s2.length() == 0) &#123; return 0; &#125; int len1 = s1.length(), len2 = s2.length(); int[][] dp = new int[len1 + 1][len2 + 1]; for (int i = 1; i &lt;= len1; i++) &#123; for (int j = 1; j &lt;= len2; j++) &#123; if (s1.charAt(i - 1) == s2.charAt(j - 1)) &#123; dp[i][j] = dp[i - 1][j - 1] + 1; &#125; else &#123; dp[i][j] = Math.max(dp[i][j - 1], dp[i - 1][j]); &#125; &#125; &#125; return dp[len1][len2];&#125; 背包问题股票问题只有一次买入卖出的股票– 据说贪心 记录当前最小买入价格 计算每个元素和当前最小买入价格的差值，取最大的 有一天cooldown、多次买入卖出的股票状态机啊…不会…pass!","tags":[]},{"title":"Java序列化与反序列化","date":"2020-06-19T16:45:34.000Z","path":"2020/06/20/Java序列化与反序列化/","text":"序列化与反序列化是成对存在的，文中简称为序列化。 写在前面： 文中有很多源码，稍显凌乱。也主要是自己的一个记录，未字斟句酌。 简介序列化（serialize） - 序列化是将对象转换为字节流。 反序列化（deserialize） - 反序列化是将字节流转换为对象。 序列化用途 将对象持久化，保存在内存、文件、数据库中 便于网络传输和传播 序列化工具java序列化 - 性能比较普通 thrift、protobuf - 适用于对性能敏感，对开发体验要求不高的内部系统。 hessian - 适用于对开发体验敏感，性能有要求的内外部系统。 jackson、gson、fastjson - 适用于对序列化后的数据要求有良好的可读性（转为 json 、xml 形式）。 Java序列化实现Serializable或Externalizable接口。 前者有默认序列化逻辑，后者需要强制实现自己的序列化逻辑。底层实现都是Serializable。 本文主要关注Serializable。 使用基础用法实现Serializable接口，通过ObjectOutputStream来序列化，ObjectInputStream反序列化。 123456789101112131415161718192021@Slf4j@Datapublic class Person implements Serializable &#123; private static final long serialVersionUID = 6666716291353949192L; private Integer age; private String name; @Test public void test() throws IOException, ClassNotFoundException &#123; Person person = new Person(); person.setAge(22); person.setName(\"lily\"); ObjectOutputStream objectOutputStream = new ObjectOutputStream(new FileOutputStream(\"java.person.txt\")); objectOutputStream.writeObject(person); log.info(\"writeObject = &#123;&#125;\", person); ObjectInputStream objectInputStream = new ObjectInputStream(new FileInputStream(\"java.person.txt\")); person = (Person) objectInputStream.readObject(); log.info(\"readObject = &#123;&#125;\", person); &#125;&#125; 输出 12writeObject = Person(age=22, name=lily)readObject = Person(age=22, name=lily) 自定义用法自定义序列化和反序列化逻辑，比如控制序列化字段、进行编码加密等。 在JavaBean中实现writeObject和readObject方法，方法签名是固定的。序列化过程中会判断JavaBean中的有无这样的方法，如果没有，就走入默认的defaultWriteFields和defaultReadFields的逻辑。 比如用age模拟加密和解密。 123456789101112private void writeObject(ObjectOutputStream out) throws Exception &#123; ObjectOutputStream.PutField putFields = out.putFields(); putFields.put(\"age\", age + 1); out.writeFields();&#125;private void readObject(ObjectInputStream in) throws Exception &#123; ObjectInputStream.GetField readFields = in.readFields(); Integer encryptionAge = (Integer) readFields.get(\"age\", \"\"); // 模拟解密 age = encryptionAge - 1;&#125; 其他知识点 如果不实现Serializable接口，会抛出NotSerializableException异常。 如果属性是引用对象，引用对象也要实现序列化 一个子类实现Serializable 接口，父类也需要实现（否则父类信息不会被序列化） 静态字段不会参与序列化（序列化保存的是对象的状态，静态变量属于类的状态） transient关键字修饰的对象不参与序列化 第二种自定义方式：writeReplace和readResolve writeObject 序列化时会调用 writeReplace 方法将当前对象替换成另一个对象并将其写入流中，此时如果有自定义的 writeObject 也不会生效了 readResolve 会在 readObject 调用之后自动调用，它最主要的目的就是对反序列化的对象进行修改后返回 同一对象在一个ObjectOutputStream里writeObject多次，后面几次只会输出句柄信息 潜在问题：如果在第一次序列化后，修改了内容，再次序列化时只会输出编码号，会丢失修改信息 serialVersionUID相当于类的版本号，如果没有显示定义serialVersionUID，编译期会根据类信息创建一个，当修改类后可能会引起反序列化失败（比如新增了字段，导致再次自动计算的serialVersionUID不相同） serialVersionUID的idea快捷生成 实现原理接口关系引用这里一张接口关系图 Serializable和Externalizable 序列化接口 Serializable 接口没有方法或字段，仅用于标识可序列化的语义，实际上 ObjectOutputStream#writeObject 会判断JavaBean有没有自定义的writeObject 方法，如果没有则调用默认的序列化方法。 Externalizable 接口该接口中定义了两个扩展的抽象方法：writeExternal 与 readExternal。 DataOutput和ObjectOutput DataOutput 定义了对 8种Java 基本类型 byte、short、int、long、float、double、char、boolean，以及 String 的操作。 ObjectOutput 在 DataOutput 的基础上定义了对 Object 类型的操作。 ObjectOutputStream 一般使用 ObjectOutputStream#writeObject 方法把一个对象进行持久化。（ObjectInputStream#readObject 则从持久化存储中把对象读取出来。） ObjectStreamClass 和 ObjectStreamField ObjectStreamClass 是类的序列化描述符，包含类描述信息，字段的描述信息和 serialVersionUID。可以使用 lookup 方法找到创建在虚拟机中加载的具体类的 ObjectStreamClass。 ObjectStreamField 保存字段的序列化描述符，包括字段名、字段值等。 ObjectOutputStream源码分析源码版本：jdk-12.0.1.jdk ObjectOutputStream 数据结构12345678910/** 输出流 */private final BlockDataOutputStream bout;/** 句柄映射，如果在句柄中找到当前对象，说明已经序列化过，只输出句柄信息 */private final HandleTable handles;/** 替换对象的映射 obj -&gt; replacement obj map */private final ReplaceTable subs; /** true 则调用writeObjectOverride()来替代writeObject() -- ObjectOutputStream的子类可重写writeObjectOverride()*/private final boolean enableOverride;/** true 则调用replaceObject() -- JavaBean中实现replaceObject() */private boolean enableReplace; ObjectOutputStream 构造函数12345678910111213141516171819202122/** 一些初试化 */public ObjectOutputStream(OutputStream out) throws IOException &#123; verifySubclass(); bout = new BlockDataOutputStream(out); handles = new HandleTable(10, (float) 3.00); subs = new ReplaceTable(10, (float) 3.00); enableOverride = false; writeStreamHeader(); bout.setBlockDataMode(true); if (extendedDebugInfo) &#123; debugInfoStack = new DebugTraceInfoStack(); &#125; else &#123; debugInfoStack = null; &#125;&#125;/** * 魔数和版本号 */protected void writeStreamHeader() throws IOException &#123; bout.writeShort(STREAM_MAGIC); bout.writeShort(STREAM_VERSION);&#125; 序列化入口 writeObject引用这里一张时序图 以下顺着基础用法的逻辑，看下代码实现。 1. writeObject12345678910111213141516public final void writeObject(Object obj) throws IOException &#123; // 如果当前是ObjectOutputStream的子类，走这个分支 if (enableOverride) &#123; writeObjectOverride(obj); return; &#125; try &#123; // 核心方法 writeObject0(obj, false); &#125; catch (IOException ex) &#123; if (depth == 0) &#123; writeFatalException(ex); &#125; throw ex; &#125;&#125; 2. writeObject0123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899/** * Underlying writeObject/writeUnshared implementation. * unshared=false表示共享对象，就是指同一个对象只输出句柄信息 * unshared=true表示不共享，会完整再序列化一次 */private void writeObject0(Object obj, boolean unshared) throws IOException&#123; boolean oldMode = bout.setBlockDataMode(false); // depth表示对象深度，比如当前对象为1，递归到对象属性时，depth++为2 depth++; try &#123; // handle previously written and non-replaceable objects int h; // 判断要不要序列化 以下4种情况不用序列化 if ((obj = subs.lookup(obj)) == null) &#123; writeNull(); return; &#125; else if (!unshared &amp;&amp; (h = handles.lookup(obj)) != -1) &#123; // 是否共享已序列化对象，一般为是 writeHandle(h); return; &#125; else if (obj instanceof Class) &#123; writeClass((Class) obj, unshared); return; &#125; else if (obj instanceof ObjectStreamClass) &#123; writeClassDesc((ObjectStreamClass) obj, unshared); return; &#125; // check for replacement object // 这里处理对象替换，也先不看 Object orig = obj; // ps最后回来看: 怎么处理泛型呢？类信息写入的类型是java.lang.Object；写数据时，字段对应的类型是实际类型，比如java.lang.Integer这样的 Class&lt;?&gt; cl = obj.getClass(); ObjectStreamClass desc; for (;;) &#123; // REMIND: skip this check for strings/arrays? Class&lt;?&gt; repCl; desc = ObjectStreamClass.lookup(cl, true); if (!desc.hasWriteReplaceMethod() || (obj = desc.invokeWriteReplace(obj)) == null || (repCl = obj.getClass()) == cl) &#123; break; &#125; cl = repCl; &#125; // 如果对象替换了，取新对象的描述符 if (enableReplace) &#123; Object rep = replaceObject(obj); if (rep != obj &amp;&amp; rep != null) &#123; cl = rep.getClass(); desc = ObjectStreamClass.lookup(cl, true); &#125; obj = rep; &#125; // 如果对象替换了，再判断一遍要不要序列化 if (obj != orig) &#123; subs.assign(orig, obj); if (obj == null) &#123; writeNull(); return; &#125; else if (!unshared &amp;&amp; (h = handles.lookup(obj)) != -1) &#123; writeHandle(h); return; &#125; else if (obj instanceof Class) &#123; writeClass((Class) obj, unshared); return; &#125; else if (obj instanceof ObjectStreamClass) &#123; writeClassDesc((ObjectStreamClass) obj, unshared); return; &#125; &#125; // 序列化的主体逻辑在这里 // 字符串和枚举在方法里写值进输出流了 // 数组里的元素，如果是原生类型，也直接写输出流，如果非原生类型，递归序列化 if (obj instanceof String) &#123; // 字符串 writeString((String) obj, unshared); &#125; else if (cl.isArray()) &#123; // 数组 writeArray(obj, desc, unshared); &#125; else if (obj instanceof Enum) &#123; // 枚举 writeEnum((Enum&lt;?&gt;) obj, desc, unshared); &#125; else if (obj instanceof Serializable) &#123; // 实现了Serializable的JavaBean，也是我们要主要看的逻辑 writeOrdinaryObject(obj, desc, unshared); &#125; else &#123; // 不是以上几种情况的抛出异常 if (extendedDebugInfo) &#123; throw new NotSerializableException( cl.getName() + \"\\n\" + debugInfoStack.toString()); &#125; else &#123; throw new NotSerializableException(cl.getName()); &#125; &#125; &#125; finally &#123; depth--; bout.setBlockDataMode(oldMode); &#125;&#125; 2.1 writeString&amp;writeArray&amp;writeEnum字符串、枚举的在这里就写值了 数组元素如果是原生类型的，也写值了，如果非原生，递归调用writeObject0 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091/** * Writes given string to stream, using standard or long UTF format * depending on string length. * UTF格式写string */private void writeString(String str, boolean unshared) throws IOException &#123; handles.assign(unshared ? null : str); long utflen = bout.getUTFLength(str); if (utflen &lt;= 0xFFFF) &#123; bout.writeByte(TC_STRING); bout.writeUTF(str, utflen); &#125; else &#123; bout.writeByte(TC_LONGSTRING); bout.writeLongUTF(str, utflen); &#125;&#125;/** * 写数组 * Writes given array object to stream. */private void writeArray(Object array, ObjectStreamClass desc, boolean unshared) throws IOException&#123; bout.writeByte(TC_ARRAY); writeClassDesc(desc, false); handles.assign(unshared ? null : array); Class&lt;?&gt; ccl = desc.forClass().getComponentType(); // 如果是原生类型的数组 if (ccl.isPrimitive()) &#123; if (ccl == Integer.TYPE) &#123; int[] ia = (int[]) array; bout.writeInt(ia.length); bout.writeInts(ia, 0, ia.length); &#125; // ... 省略其他原生类型 else &#123; throw new InternalError(); &#125; &#125; else &#123; Object[] objs = (Object[]) array; int len = objs.length; // 写长度信息 bout.writeInt(len); if (extendedDebugInfo) &#123; debugInfoStack.push( \"array (class \\\"\" + array.getClass().getName() + \"\\\", size: \" + len + \")\"); &#125; try &#123; for (int i = 0; i &lt; len; i++) &#123; if (extendedDebugInfo) &#123; debugInfoStack.push( \"element of array (index: \" + i + \")\"); &#125; try &#123; // 对每个对象序列化 writeObject0(objs[i], false); &#125; finally &#123; if (extendedDebugInfo) &#123; debugInfoStack.pop(); &#125; &#125; &#125; &#125; finally &#123; if (extendedDebugInfo) &#123; debugInfoStack.pop(); &#125; &#125; &#125;&#125;/** * Writes given enum constant to stream. */private void writeEnum(Enum&lt;?&gt; en, ObjectStreamClass desc, boolean unshared) throws IOException&#123; bout.writeByte(TC_ENUM); ObjectStreamClass sdesc = desc.getSuperDesc(); // 写枚举类信息 writeClassDesc((sdesc.forClass() == Enum.class) ? desc : sdesc, false); handles.assign(unshared ? null : en); // 写枚举name writeString(en.name(), false);&#125; 2.2 writeOrdinaryObject普通JavaBean的序列化逻辑 写类型 写类信息 写类数据 12345678910111213141516171819202122232425262728293031323334/** * Writes representation of a \"ordinary\" (i.e., not a String, Class, * ObjectStreamClass, array, or enum constant) serializable object to the * stream. */private void writeOrdinaryObject(Object obj, ObjectStreamClass desc, boolean unshared) throws IOException&#123; if (extendedDebugInfo) &#123; debugInfoStack.push( (depth == 1 ? \"root \" : \"\") + \"object (class \\\"\" + obj.getClass().getName() + \"\\\", \" + obj.toString() + \")\"); &#125; try &#123; desc.checkSerialize(); // 写类型 bout.writeByte(TC_OBJECT); // 写类信息 writeClassDesc(desc, false); handles.assign(unshared ? null : obj); // 写数据（Field信息和数据） if (desc.isExternalizable() &amp;&amp; !desc.isProxy()) &#123; // Externalizable接口 writeExternalData((Externalizable) obj); &#125; else &#123; writeSerialData(obj, desc); // Serializable接口 &#125; &#125; finally &#123; if (extendedDebugInfo) &#123; debugInfoStack.pop(); &#125; &#125;&#125; 2.2.1 writeClassDesc写类信息。 非代理类型的类信息，一般有类标志位、类名称、序列化协议版本、SUID、方法标志位、字段个数、然后每个字段的：字段TypeCode、字段名称、（非原生类型的）字段类型 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980 private void writeClassDesc(ObjectStreamClass desc, boolean unshared) throws IOException &#123; int handle; if (desc == null) &#123; writeNull(); &#125; else if (!unshared &amp;&amp; (handle = handles.lookup(desc)) != -1) &#123; writeHandle(handle); &#125; else if (desc.isProxy()) &#123; writeProxyDesc(desc, unshared); &#125; else &#123; writeNonProxyDesc(desc, unshared); &#125; &#125; /** * Writes class descriptor representing a standard (i.e., not a dynamic * proxy) class to stream. */ private void writeNonProxyDesc(ObjectStreamClass desc, boolean unshared) throws IOException &#123; bout.writeByte(TC_CLASSDESC); handles.assign(unshared ? null : desc); if (protocol == PROTOCOL_VERSION_1) &#123; // do not invoke class descriptor write hook with old protocol desc.writeNonProxy(this); &#125; else &#123; writeClassDescriptor(desc); &#125; Class&lt;?&gt; cl = desc.forClass(); bout.setBlockDataMode(true); if (cl != null &amp;&amp; isCustomSubclass()) &#123; ReflectUtil.checkPackageAccess(cl); &#125; annotateClass(cl); bout.setBlockDataMode(false); bout.writeByte(TC_ENDBLOCKDATA); // 往上递归，获取父类信息，直到父类没有实现Serializable writeClassDesc(desc.getSuperDesc(), false); &#125;// ObjectStreamClass.java /** * Writes non-proxy class descriptor information to given output stream. */ void writeNonProxy(ObjectOutputStream out) throws IOException &#123; out.writeUTF(name); out.writeLong(getSerialVersionUID()); byte flags = 0; if (externalizable) &#123; flags |= ObjectStreamConstants.SC_EXTERNALIZABLE; int protocol = out.getProtocolVersion(); if (protocol != ObjectStreamConstants.PROTOCOL_VERSION_1) &#123; flags |= ObjectStreamConstants.SC_BLOCK_DATA; &#125; &#125; else if (serializable) &#123; flags |= ObjectStreamConstants.SC_SERIALIZABLE; &#125; if (hasWriteObjectData) &#123; flags |= ObjectStreamConstants.SC_WRITE_METHOD; &#125; if (isEnum) &#123; flags |= ObjectStreamConstants.SC_ENUM; &#125; out.writeByte(flags); out.writeShort(fields.length); for (int i = 0; i &lt; fields.length; i++) &#123; ObjectStreamField f = fields[i]; out.writeByte(f.getTypeCode()); out.writeUTF(f.getName()); if (!f.isPrimitive()) &#123; out.writeTypeString(f.getTypeString()); &#125; &#125; &#125; 2.2.2 writeSerailData写类数据 12345678910111213141516171819202122232425262728293031323334353637383940private void writeSerialData(Object obj, ObjectStreamClass desc) throws IOException&#123; // 获取要序列化的对象 ObjectStreamClass.ClassDataSlot[] slots = desc.getClassDataLayout(); for (int i = 0; i &lt; slots.length; i++) &#123; ObjectStreamClass slotDesc = slots[i].desc; // 如果对象中重写了writeObject if (slotDesc.hasWriteObjectMethod()) &#123; PutFieldImpl oldPut = curPut; curPut = null; SerialCallbackContext oldContext = curContext; if (extendedDebugInfo) &#123; debugInfoStack.push( \"custom writeObject data (class \\\"\" + slotDesc.getName() + \"\\\")\"); &#125; try &#123; curContext = new SerialCallbackContext(obj, slotDesc); bout.setBlockDataMode(true); // 反射调用重写的writeObject slotDesc.invokeWriteObject(obj, this); bout.setBlockDataMode(false); bout.writeByte(TC_ENDBLOCKDATA); &#125; finally &#123; curContext.setUsed(); curContext = oldContext; if (extendedDebugInfo) &#123; debugInfoStack.pop(); &#125; &#125; curPut = oldPut; &#125; else &#123; // 默认的序列化方法 defaultWriteFields(obj, slotDesc); &#125; &#125;&#125; 2.2.2.1 defaultWriteFields真正写类数据的地方 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * Fetches and writes values of serializable fields of given object to * stream. The given class descriptor specifies which field values to * write, and in which order they should be written. */private void defaultWriteFields(Object obj, ObjectStreamClass desc) throws IOException&#123; Class&lt;?&gt; cl = desc.forClass(); if (cl != null &amp;&amp; obj != null &amp;&amp; !cl.isInstance(obj)) &#123; throw new ClassCastException(); &#125; desc.checkDefaultSerialize(); // 获取对象中原生类型的字段的总长度 int primDataSize = desc.getPrimDataSize(); if (primDataSize &gt; 0) &#123; if (primVals == null || primVals.length &lt; primDataSize) &#123; primVals = new byte[primDataSize]; &#125; // 获取对象中原生类型的字段的所有值，放入字节数组primVals // *这里是最终写对象里字段的值的地方* desc.getPrimFieldValues(obj, primVals); // 将primVals写入输出流 bout.write(primVals, 0, primDataSize, false); &#125; int numObjFields = desc.getNumObjFields(); if (numObjFields &gt; 0) &#123; ObjectStreamField[] fields = desc.getFields(false); Object[] objVals = new Object[numObjFields]; // 剩下的非原生类型的字段 int numPrimFields = fields.length - objVals.length; desc.getObjFieldValues(obj, objVals); for (int i = 0; i &lt; objVals.length; i++) &#123; if (extendedDebugInfo) &#123; debugInfoStack.push( \"field (class \\\"\" + desc.getName() + \"\\\", name: \\\"\" + fields[numPrimFields + i].getName() + \"\\\", type: \\\"\" + fields[numPrimFields + i].getType() + \"\\\")\"); &#125; try &#123; // 递归序列化这个非原生类型字段对象 writeObject0(objVals[i], fields[numPrimFields + i].isUnshared()); &#125; finally &#123; if (extendedDebugInfo) &#123; debugInfoStack.pop(); &#125; &#125; &#125; &#125;&#125; 到这序列化的主体逻辑就结束了。 反序列化的主要操作和序列化都是一一对应的。读出每个类标志，用对应方式去解析。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192private Object readObject0(boolean unshared) throws IOException &#123; boolean oldMode = bin.getBlockDataMode(); if (oldMode) &#123; int remain = bin.currentBlockRemaining(); if (remain &gt; 0) &#123; throw new OptionalDataException(remain); &#125; else if (defaultDataEnd) &#123; /* * Fix for 4360508: stream is currently at the end of a field * value block written via default serialization; since there * is no terminating TC_ENDBLOCKDATA tag, simulate * end-of-custom-data behavior explicitly. */ throw new OptionalDataException(true); &#125; bin.setBlockDataMode(false); &#125; // 这里开始 byte tc; while ((tc = bin.peekByte()) == TC_RESET) &#123; bin.readByte(); handleReset(); &#125; depth++; totalObjectRefs++; try &#123; switch (tc) &#123; case TC_NULL: return readNull(); // 共享的句柄索引 case TC_REFERENCE: return readHandle(unshared); case TC_CLASS: return readClass(unshared); // 类说明 case TC_CLASSDESC: case TC_PROXYCLASSDESC: return readClassDesc(unshared); // 字符串 case TC_STRING: case TC_LONGSTRING: return checkResolve(readString(unshared)); case TC_ARRAY: return checkResolve(readArray(unshared)); case TC_ENUM: return checkResolve(readEnum(unshared)); // JavaBean case TC_OBJECT: return checkResolve(readOrdinaryObject(unshared)); case TC_EXCEPTION: IOException ex = readFatalException(); throw new WriteAbortedException(\"writing aborted\", ex); case TC_BLOCKDATA: case TC_BLOCKDATALONG: if (oldMode) &#123; bin.setBlockDataMode(true); bin.peek(); // force header read throw new OptionalDataException( bin.currentBlockRemaining()); &#125; else &#123; throw new StreamCorruptedException( \"unexpected block data\"); &#125; case TC_ENDBLOCKDATA: if (oldMode) &#123; throw new OptionalDataException(true); &#125; else &#123; throw new StreamCorruptedException( \"unexpected end of block data\"); &#125; default: throw new StreamCorruptedException( String.format(\"invalid type code: %02X\", tc)); &#125; &#125; finally &#123; depth--; bin.setBlockDataMode(oldMode); &#125;&#125; 其他知识点字段值的读取和写入ObjectOutputStream写值的逻辑：获取到当前对象中的原生类型字段，primDataSize是对象中所有原生类型字段的总长度，desc.getPrimFieldValues(obj, primVals);是将对象中的所有原生字段的值写入primVals这个字节数组，获取对象的值时是用Unsafe类直接通过偏移量取值，unsafe.getBoolean(obj, offset)这样的方式。 同理，ObjectInputStream读值的逻辑，unsafe.putBoolean(obj, key, Bits.getBoolean(buf, off)); 实现在ObjectStreamClass#getPrimFieldValues和ObjectStreamClass#setPrimFieldValues 共享句柄序列化过程中出现过的对象、字符串、数值，甚至拼接出来的类信息，如果是shared模式，都不会再完整序列化一次，只会输出handles句柄的索引。 写入句柄的地方 一个直观的例子举上面Person的例子。 write Person: person -&gt; write Integer: age &amp; write String: name write Integer: age -&gt; write int value(11) &amp; write Number : null write String: name Person里再加Birthday one, Birthday two属性。 Birthday属性 year, month, day. 123456789101112131415161718192021222324252627282930313233343536373839@Slf4j@Datapublic class Person implements Serializable &#123; private static final long serialVersionUID = 6666716291353949192L; private Integer age; private String name; private Birthday one; private Birthday two; @Data class Birthday implements Serializable &#123; private Integer year; private Integer month; private Integer day; &#125; @Test public void test() throws IOException, ClassNotFoundException &#123; Person person = new Person(); person.setAge(22); person.setName(\"lily\"); Birthday one = new Birthday(); one.setYear(2020); Birthday two = new Birthday(); two.setYear(2019); person.setOne(one); person.setTwo(two);// person.setIsParent(false); ObjectOutputStream objectOutputStream = new ObjectOutputStream(new FileOutputStream(\"java.person.txt\")); objectOutputStream.writeObject(person); log.info(\"writeObject = &#123;&#125;\", person); ObjectInputStream objectInputStream = new ObjectInputStream(new FileInputStream(\"java.person.txt\")); person = (Person) objectInputStream.readObject(); log.info(\"readObject = &#123;&#125;\", person); &#125;&#125; 以JavaBean为例，不严谨的一份序列化结果接近这样： 魔数、版本号只在初始化写一次 魔数、版本号 | 类标志、类信息开始、类名称、序列化协议版本、SUID、一些序列信息标志（比如是Serializable还是externalizable..）、字段个数、（for每个字段的）字段TypeCode、字段名称、（非原生类型的）字段类型、类信息结束（向上递归父类的信息）【父类的类标志….（非原生类型的）字段类型、类信息结束】（从父类到子类，for每个类）所有原生字段的值+递归所有非原生字段的序列化 高亮那一段解释不来…可以解释了，Birthday的fields如debug截图 GsonJSON (JavaScript Object Notation)是一种轻量级数据交换格式。 使用依赖 maven 123456&lt;dependency&gt; &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt; &lt;artifactId&gt;gson&lt;/artifactId&gt; &lt;version&gt;2.8.6&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt;&lt;/dependency&gt; 基础用法api非常的简单、易用 基本类型123456789101112131415// SerializationGson gson = new Gson();gson.toJson(1); // ==&gt; 1gson.toJson(\"abcd\"); // ==&gt; \"abcd\"gson.toJson(new Long(10)); // ==&gt; 10int[] values = &#123; 1 &#125;;gson.toJson(values); // ==&gt; [1]// Deserializationint one = gson.fromJson(\"1\", int.class);Integer one = gson.fromJson(\"1\", Integer.class);Long one = gson.fromJson(\"1\", Long.class);Boolean false = gson.fromJson(\"false\", Boolean.class);String str = gson.fromJson(\"\\\"abc\\\"\", String.class);String[] anotherStr = gson.fromJson(\"[\\\"abc\\\"]\", String[].class); 引用类型12345678910111213141516171819class BagOfPrimitives &#123; private int value1 = 1; private String value2 = \"abc\"; private transient int value3 = 3; BagOfPrimitives() &#123; // no-args constructor &#125;&#125;// SerializationBagOfPrimitives obj = new BagOfPrimitives();Gson gson = new Gson();String json = gson.toJson(obj); // ==&gt; json is &#123;\"value1\":1,\"value2\":\"abc\"&#125;// DeserializationBagOfPrimitives obj2 = gson.fromJson(json, BagOfPrimitives.class);// ==&gt; obj2 is just like obj 数组1234567891011Gson gson = new Gson();int[] ints = &#123;1, 2, 3, 4, 5&#125;;String[] strings = &#123;\"abc\", \"def\", \"ghi\"&#125;;// Serializationgson.toJson(ints); // ==&gt; [1,2,3,4,5]gson.toJson(strings); // ==&gt; [\"abc\", \"def\", \"ghi\"]// Deserializationint[] ints2 = gson.fromJson(\"[1,2,3,4,5]\", int[].class); // ==&gt; ints2 will be same as ints 泛型1234567891011121314class Foo&lt;T&gt; &#123; T value;&#125;Gson gson = new Gson();Foo&lt;Bar&gt; foo = new Foo&lt;Bar&gt;();gson.toJson(foo); // May not serialize foo.value correctlygson.fromJson(json, foo.getClass()); // foo.value 不能反序列化成 Bar// 正确反序列化泛型的方式 利用TypeTokenType fooType = new TypeToken&lt;Foo&lt;Bar&gt;&gt;() &#123;&#125;.getType();gson.toJson(foo, fooType);gson.fromJson(json, fooType); 实现原理核心：TypeAdapter类型适配器，用到了适配器模式，作为JsonWriter/JsonReader和类型T的对象之间的适配器，将一个对象写入json数据，或从json数据中读入一个对象。 12345678public abstract class TypeAdapter&lt;T&gt; &#123; public abstract void write(JsonWriter out, T value) throws IOException; public abstract T read(JsonReader in) throws IOException; ...&#125; Gson为每一种类型创建一个TypeAdapter，同样的，每一个Type都对应唯一一个TypeAdapter。 在Gson中，类型Type大致可以分为两类： 基本平台类型，如int.class、Integer.class、String.class、Url.class等 组合及自定义类型，如Collection.class、Map.class和用户自定义的JavaBean等 引用这里一张示意图： Gson根据传入的Type找对应的TypeAdapter，如果是基本平台类型，利用TypeAdapter可直接读写json，如果是组合及自定义类型，则在对应的TypeAdapter里封装了对内部属性的处理，是一个迭代的过程（和上面Java自带的序列化writeOrdinaryObject&amp;readOrdinaryObject是很类似的）。 源码分析Gson属性和构造函数一些属性 123456789 /** * 避免获取adapter的时候产生无限递归 */ private final ThreadLocal&lt;Map&lt;TypeToken&lt;?&gt;, FutureTypeAdapter&lt;?&gt;&gt;&gt; calls = new ThreadLocal&lt;Map&lt;TypeToken&lt;?&gt;, FutureTypeAdapter&lt;?&gt;&gt;&gt;();// 缓存的adapter private final Map&lt;TypeToken&lt;?&gt;, TypeAdapter&lt;?&gt;&gt; typeTokenCache = new ConcurrentHashMap&lt;TypeToken&lt;?&gt;, TypeAdapter&lt;?&gt;&gt;();// 注册的adapter工厂final List&lt;TypeAdapterFactory&gt; factories; 最终调用的构造函数 new Gson()得到默认的实例，或者用GsonBuilder自定义一些策略 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889Gson(Excluder excluder, FieldNamingStrategy fieldNamingStrategy, Map&lt;Type, InstanceCreator&lt;?&gt;&gt; instanceCreators, boolean serializeNulls, boolean complexMapKeySerialization, boolean generateNonExecutableGson, boolean htmlSafe, boolean prettyPrinting, boolean lenient, boolean serializeSpecialFloatingPointValues, LongSerializationPolicy longSerializationPolicy, String datePattern, int dateStyle, int timeStyle, List&lt;TypeAdapterFactory&gt; builderFactories, List&lt;TypeAdapterFactory&gt; builderHierarchyFactories, List&lt;TypeAdapterFactory&gt; factoriesToBeAdded) &#123; // 默认的一些序列化策略，比如字段名称策略，自定义的实例化构造器，pretty输出... this.excluder = excluder; this.fieldNamingStrategy = fieldNamingStrategy; this.instanceCreators = instanceCreators; this.constructorConstructor = new ConstructorConstructor(instanceCreators); this.serializeNulls = serializeNulls; this.complexMapKeySerialization = complexMapKeySerialization; this.generateNonExecutableJson = generateNonExecutableGson; this.htmlSafe = htmlSafe; this.prettyPrinting = prettyPrinting; this.lenient = lenient; this.serializeSpecialFloatingPointValues = serializeSpecialFloatingPointValues; this.longSerializationPolicy = longSerializationPolicy; this.datePattern = datePattern; this.dateStyle = dateStyle; this.timeStyle = timeStyle; this.builderFactories = builderFactories; this.builderHierarchyFactories = builderHierarchyFactories; // 注册adapter工厂 List&lt;TypeAdapterFactory&gt; factories = new ArrayList&lt;TypeAdapterFactory&gt;(); // built-in type adapters that cannot be overridden factories.add(TypeAdapters.JSON_ELEMENT_FACTORY); factories.add(ObjectTypeAdapter.FACTORY); // the excluder must precede all adapters that handle user-defined types factories.add(excluder); // users' type adapters 如果有自定义的工厂，在这里被加入，顺序比较靠前 factories.addAll(factoriesToBeAdded); // type adapters for basic platform types 基本平台类型的adapter工厂 factories.add(TypeAdapters.STRING_FACTORY); factories.add(TypeAdapters.INTEGER_FACTORY); factories.add(TypeAdapters.BOOLEAN_FACTORY); factories.add(TypeAdapters.BYTE_FACTORY); factories.add(TypeAdapters.SHORT_FACTORY); TypeAdapter&lt;Number&gt; longAdapter = longAdapter(longSerializationPolicy); factories.add(TypeAdapters.newFactory(long.class, Long.class, longAdapter)); factories.add(TypeAdapters.newFactory(double.class, Double.class, doubleAdapter(serializeSpecialFloatingPointValues))); factories.add(TypeAdapters.newFactory(float.class, Float.class, floatAdapter(serializeSpecialFloatingPointValues))); factories.add(TypeAdapters.NUMBER_FACTORY); factories.add(TypeAdapters.ATOMIC_INTEGER_FACTORY); factories.add(TypeAdapters.ATOMIC_BOOLEAN_FACTORY); factories.add(TypeAdapters.newFactory(AtomicLong.class, atomicLongAdapter(longAdapter))); factories.add(TypeAdapters.newFactory(AtomicLongArray.class, atomicLongArrayAdapter(longAdapter))); factories.add(TypeAdapters.ATOMIC_INTEGER_ARRAY_FACTORY); factories.add(TypeAdapters.CHARACTER_FACTORY); factories.add(TypeAdapters.STRING_BUILDER_FACTORY); factories.add(TypeAdapters.STRING_BUFFER_FACTORY); factories.add(TypeAdapters.newFactory(BigDecimal.class, TypeAdapters.BIG_DECIMAL)); factories.add(TypeAdapters.newFactory(BigInteger.class, TypeAdapters.BIG_INTEGER)); factories.add(TypeAdapters.URL_FACTORY); factories.add(TypeAdapters.URI_FACTORY); factories.add(TypeAdapters.UUID_FACTORY); factories.add(TypeAdapters.CURRENCY_FACTORY); factories.add(TypeAdapters.LOCALE_FACTORY); factories.add(TypeAdapters.INET_ADDRESS_FACTORY); factories.add(TypeAdapters.BIT_SET_FACTORY); factories.add(DateTypeAdapter.FACTORY); factories.add(TypeAdapters.CALENDAR_FACTORY); factories.add(TimeTypeAdapter.FACTORY); factories.add(SqlDateTypeAdapter.FACTORY); factories.add(TypeAdapters.TIMESTAMP_FACTORY); factories.add(ArrayTypeAdapter.FACTORY); factories.add(TypeAdapters.CLASS_FACTORY); // type adapters for composite and user-defined types 复合类型和自定义类型的adapter工厂 factories.add(new CollectionTypeAdapterFactory(constructorConstructor)); factories.add(new MapTypeAdapterFactory(constructorConstructor, complexMapKeySerialization)); this.jsonAdapterFactory = new JsonAdapterAnnotationTypeAdapterFactory(constructorConstructor); factories.add(jsonAdapterFactory); factories.add(TypeAdapters.ENUM_FACTORY); factories.add(new ReflectiveTypeAdapterFactory( constructorConstructor, fieldNamingStrategy, excluder, jsonAdapterFactory)); this.factories = Collections.unmodifiableList(factories); &#125; toJson1234567891011121314151617@Slf4jpublic class LearningGsonTest &#123; @Test public void testGson() &#123; Person person = new Person(); person.setAge(22); person.setName(\"lily\"); log.info(\"person = &#123;&#125;\", person); Gson gson = new Gson(); String json = gson.toJson(person); log.info(\"json = &#123;&#125;\", json); person = gson.fromJson(json, Person.class); log.info(\"person = &#123;&#125;\", person); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849 public String toJson(Object src) &#123; if (src == null) &#123; return toJson(JsonNull.INSTANCE); &#125; // 取Object的Type // Class.class属于Type的一种 return toJson(src, src.getClass()); &#125; public String toJson(Object src, Type typeOfSrc) &#123; StringWriter writer = new StringWriter(); toJson(src, typeOfSrc, writer); return writer.toString(); &#125; public void toJson(Object src, Type typeOfSrc, Appendable writer) throws JsonIOException &#123; try &#123; JsonWriter jsonWriter = newJsonWriter(Streams.writerForAppendable(writer)); toJson(src, typeOfSrc, jsonWriter); &#125; catch (IOException e) &#123; throw new JsonIOException(e); &#125; &#125;// 最终都会到这个方法 public void toJson(Object src, Type typeOfSrc, JsonWriter writer) throws JsonIOException &#123; // 关键步骤1，获取到对应的TypeAdapter TypeAdapter&lt;?&gt; adapter = getAdapter(TypeToken.get(typeOfSrc)); boolean oldLenient = writer.isLenient(); writer.setLenient(true); boolean oldHtmlSafe = writer.isHtmlSafe(); writer.setHtmlSafe(htmlSafe); boolean oldSerializeNulls = writer.getSerializeNulls(); writer.setSerializeNulls(serializeNulls); try &#123; // 关键步骤2，利用TypeAdapter写出json数据 ((TypeAdapter&lt;Object&gt;) adapter).write(writer, src); &#125; catch (IOException e) &#123; throw new JsonIOException(e); &#125; catch (AssertionError e) &#123; AssertionError error = new AssertionError(\"AssertionError (GSON \" + GsonBuildConfig.VERSION + \"): \" + e.getMessage()); error.initCause(e); throw error; &#125; finally &#123; writer.setLenient(oldLenient); writer.setHtmlSafe(oldHtmlSafe); writer.setSerializeNulls(oldSerializeNulls); &#125; &#125; 看一下TypeToken.get(typeOfSrc) 123456789101112131415161718/** * Gets type literal for the given &#123;@code Type&#125; instance. */public static TypeToken&lt;?&gt; get(Type type) &#123; return new TypeToken&lt;Object&gt;(type);&#125;/** * Unsafe. Constructs a type literal manually. */@SuppressWarnings(\"unchecked\")TypeToken(Type type) &#123; // 带泛型信息的类型 this.type = $Gson$Types.canonicalize($Gson$Preconditions.checkNotNull(type)); // 泛型擦除的类型，比如List.class this.rawType = (Class&lt;? super T&gt;) $Gson$Types.getRawType(this.type); this.hashCode = this.type.hashCode();&#125; 穿插一下Java类型（Type）系统的介绍。 详细内容可以看这两篇文章Java中的Type类型详解](https://juejin.im/post/5adefaba518825670e5cb44d)和[Java Type类型](https://www.jianshu.com/p/39cc237ad815)，或者Google下。 简单说明下 Type是Java语言中所有类型的公共父接口，有4个扩展接口，主要是用来记录泛型的信息（一个泛型中的信息可能是另一个泛型） GenericArrayType：泛型数组类型，比如T[] array、List&lt;String&gt;[] array中的T[]、List&lt;String&gt;[] ParameterizedType：参数化类型，比如Lis&lt;String&gt; list、Map&lt;String,Object&gt; map中的Lis&lt;String&gt;、Map&lt;String,Object&gt; TypeVariable：类型变量，比如List&lt;T&gt; list中的T、Map&lt;K,V&gt; map、E[] array中的K、V、E（一般外层还有一个GenericArrayType或者ParameterizedType） WildcardType：通配符类型，比如Map&lt;? extends A,? super B&gt; map中的? extends A、? super B，A称为上界，B称为下界；如果只有List&lt;?&gt; list，默认下界是String，默认上界是Object Class是Type的一个直接实现类 其他4个接口都有自己的实现类，在sun.reflect包下 举例子： 比如List&lt;T extends Map&gt;[] array是一个泛型数组类型GenericArrayType，其中List&lt;T&gt;是ParameterizedType，T是TypeVariable genericComponentType可以理解为数组中每个元素的（泛型）类型 再如List&lt;? extends Map&gt; list，本身是一个ParameterizedType，? extends Map是WildcardType T[] array，本身是GenericArrayType，T是TypeVariable getAdapter1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * Returns the type adapter for &#123;@code&#125; type. * * @throws IllegalArgumentException if this GSON cannot serialize and * deserialize &#123;@code type&#125;. */@SuppressWarnings(\"unchecked\")public &lt;T&gt; TypeAdapter&lt;T&gt; getAdapter(TypeToken&lt;T&gt; type) &#123; // 先看缓存中有无对应的adapter TypeAdapter&lt;?&gt; cached = typeTokenCache.get(type == null ? NULL_KEY_SURROGATE : type); if (cached != null) &#123; return (TypeAdapter&lt;T&gt;) cached; &#125; // FutureTypeAdapter的本地缓存，避免无限递归取adapter Map&lt;TypeToken&lt;?&gt;, FutureTypeAdapter&lt;?&gt;&gt; threadCalls = calls.get(); boolean requiresThreadLocalCleanup = false; if (threadCalls == null) &#123; threadCalls = new HashMap&lt;TypeToken&lt;?&gt;, FutureTypeAdapter&lt;?&gt;&gt;(); calls.set(threadCalls); requiresThreadLocalCleanup = true; &#125; // the key and value type parameters always agree // 如果在FutureTypeAdapter的本地缓存中找到当前类型，表明这个类型的adapter正在创建中 FutureTypeAdapter&lt;T&gt; ongoingCall = (FutureTypeAdapter&lt;T&gt;) threadCalls.get(type); if (ongoingCall != null) &#123; return ongoingCall; &#125; try &#123; // 放入future的本地缓存 FutureTypeAdapter&lt;T&gt; call = new FutureTypeAdapter&lt;T&gt;(); threadCalls.put(type, call); // 遍历adapter工厂，创建adapter // 工厂创建的逻辑是，当前类型不是这个工厂负责的，返回null，反之创意一个adapter实例返回，接下来会讲一个普通类型的工厂、一个复合工厂创建实例的过程 for (TypeAdapterFactory factory : factories) &#123; TypeAdapter&lt;T&gt; candidate = factory.create(this, type); if (candidate != null) &#123; // 如果取到adapter，在FutureTypeAdapter中放入真实adapter call.setDelegate(candidate); typeTokenCache.put(type, candidate); return candidate; &#125; &#125; throw new IllegalArgumentException(\"GSON (\" + GsonBuildConfig.VERSION + \") cannot handle \" + type); &#125; finally &#123; // 从future的本地缓存中移除当前类型 threadCalls.remove(type); if (requiresThreadLocalCleanup) &#123; calls.remove(); &#125; &#125;&#125; 为什么要有个FutureTypeAdapter的本地缓存呢？ 先剧透ReflectiveTypeAdapterFactory在创建adapter的时候，会遍历属性取对应的adapte。比如Person里有个Person属性，就会递归取Person的adapter.. 1234567@Data@Slf4jpublic class Person &#123; private Integer age; private String name; private Person person;&#125; FutureTypeAdapter本质上是个委托者，内部引用了真正的adapter，在执行json读写时会用这个真正的adapter进行操作。 123456789101112131415161718192021222324252627static class FutureTypeAdapter&lt;T&gt; extends TypeAdapter&lt;T&gt; &#123; // 真正的adapter private TypeAdapter&lt;T&gt; delegate; public void setDelegate(TypeAdapter&lt;T&gt; typeAdapter) &#123; if (delegate != null) &#123; throw new AssertionError(); &#125; delegate = typeAdapter; &#125; @Override public T read(JsonReader in) throws IOException &#123; if (delegate == null) &#123; throw new IllegalStateException(); &#125; // 用真正adapter进行读 return delegate.read(in); &#125; @Override public void write(JsonWriter out, T value) throws IOException &#123; if (delegate == null) &#123; throw new IllegalStateException(); &#125; // 用真正adapter进行写 delegate.write(out, value); &#125;&#125; AdapterFactory平台基础类型的Adapter是预先定义好的，每个类型对应一个adapter，比如String类型的AdapterFactory返回的adapter永远是TypeAdapters.STRING 复合类型和自定义类型（反射类型）的Adapter是需要动态创建的，因为泛型不同、JavaBean的属性不同，等等 接下来看3个实现，其他类型大同小异，理解的思路是一样的 String类型STRING_FACTORY和’STRING_ADAPTER’ 实际上没有STRING_ADAPTER这个名字，真正名字是STRING 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748// TypeAdapterFactory是一个只有create方法的工厂，用于创建adapterpublic interface TypeAdapterFactory &#123; &lt;T&gt; TypeAdapter&lt;T&gt; create(Gson gson, TypeToken&lt;T&gt; type);&#125;// String类型的工厂public static final TypeAdapterFactory STRING_FACTORY = newFactory(String.class, STRING); // STRING是个adapter// 创建一个工厂实例的公共方法... public static &lt;TT&gt; TypeAdapterFactory newFactory( final Class&lt;TT&gt; type, final TypeAdapter&lt;TT&gt; typeAdapter) &#123; return new TypeAdapterFactory() &#123; @SuppressWarnings(\"unchecked\") // we use a runtime check to make sure the 'T's equal @Override public &lt;T&gt; TypeAdapter&lt;T&gt; create(Gson gson, TypeToken&lt;T&gt; typeToken) &#123; // 这一类工厂创建adapter的逻辑是一样的 // 如果传入的typeToken.getRawType()和预先定义的type是相同的，就返回预先定义的adapter，否则返回null（表示这个rawType不是当前工厂和adapter负责的） return typeToken.getRawType() == type ? (TypeAdapter&lt;T&gt;) typeAdapter : null; &#125; @Override public String toString() &#123; return \"Factory[type=\" + type.getName() + \",adapter=\" + typeAdapter + \"]\"; &#125; &#125;; &#125;// String类型的TypeAdapter public static final TypeAdapter&lt;String&gt; STRING = new TypeAdapter&lt;String&gt;() &#123; @Override public String read(JsonReader in) throws IOException &#123; JsonToken peek = in.peek(); // 如果是null if (peek == JsonToken.NULL) &#123; in.nextNull(); return null; &#125; // 某个兼容..先跳过 /* coerce booleans to strings for backwards compatibility */ if (peek == JsonToken.BOOLEAN) &#123; return Boolean.toString(in.nextBoolean()); &#125; // 调用JsonReader读json return in.nextString(); &#125; @Override public void write(JsonWriter out, String value) throws IOException &#123; // 调用JsonWriter写json out.value(value); &#125; &#125;; Integer、Long、Boolean、AtomicInteger等等平台基础类型的factory的框架是类似的，或者一样的，有细微的差别（比如同时支持非包装类型和包装类型） 集合类型CollectionTypeAdapterFactory 集合类型的factory和adapter MapTypeAdapterFactory和集合的很类似 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485public final class CollectionTypeAdapterFactory implements TypeAdapterFactory &#123; private final ConstructorConstructor constructorConstructor; public CollectionTypeAdapterFactory(ConstructorConstructor constructorConstructor) &#123; this.constructorConstructor = constructorConstructor; &#125; // create在这里 @Override public &lt;T&gt; TypeAdapter&lt;T&gt; create(Gson gson, TypeToken&lt;T&gt; typeToken) &#123; Type type = typeToken.getType(); // 如果Collection不是rawType的超类/超接口，那就不是集合工厂来负责 Class&lt;? super T&gt; rawType = typeToken.getRawType(); if (!Collection.class.isAssignableFrom(rawType)) &#123; return null; &#125; // 获取集合元素的类型，这里是带泛型信息的类型 Type elementType = $Gson$Types.getCollectionElementType(type, rawType); // 取元素类型的adapter TypeAdapter&lt;?&gt; elementTypeAdapter = gson.getAdapter(TypeToken.get(elementType)); // 获取构造器，如果没有自定义等等情况，一般是用反射的constructor ObjectConstructor&lt;T&gt; constructor = constructorConstructor.get(typeToken); @SuppressWarnings(&#123;\"unchecked\", \"rawtypes\"&#125;) // create() doesn't define a type parameter // 创建一个集合类型的adapter实例 TypeAdapter&lt;T&gt; result = new Adapter(gson, elementType, elementTypeAdapter, constructor); return result; &#125; // 集合类型的adapter private static final class Adapter&lt;E&gt; extends TypeAdapter&lt;Collection&lt;E&gt;&gt; &#123; // 集合元素的adapter private final TypeAdapter&lt;E&gt; elementTypeAdapter; // 构造器 private final ObjectConstructor&lt;? extends Collection&lt;E&gt;&gt; constructor; public Adapter(Gson context, Type elementType, TypeAdapter&lt;E&gt; elementTypeAdapter, ObjectConstructor&lt;? extends Collection&lt;E&gt;&gt; constructor) &#123; // 这里元素adapter还有一个封装，会在writeJson时取运行时的Type，毕竟运行时的最准确（比如定义时候是List&lt;? extends Cat&gt;，运行时实际放入的是Cat接口的一个实现WhiteCat），再根据运行时Type取相应adapter... this.elementTypeAdapter = new TypeAdapterRuntimeTypeWrapper&lt;E&gt;(context, elementTypeAdapter, elementType); this.constructor = constructor; &#125; // 读json @Override public Collection&lt;E&gt; read(JsonReader in) throws IOException &#123; if (in.peek() == JsonToken.NULL) &#123; in.nextNull(); return null; &#125; Collection&lt;E&gt; collection = constructor.construct(); // 读入 [ in.beginArray(); while (in.hasNext()) &#123; // 委托元素adapter读每个元素 E instance = elementTypeAdapter.read(in); // 元素放入集合 collection.add(instance); &#125; // 读入 ] in.endArray(); return collection; &#125; @Override public void write(JsonWriter out, Collection&lt;E&gt; collection) throws IOException &#123; if (collection == null) &#123; out.nullValue(); return; &#125; // 写 ] out.beginArray(); for (E element : collection) &#123; // 写每个元素 elementTypeAdapter.write(out, element); &#125; // 写 ] out.endArray(); &#125; &#125;&#125; TypeAdapterRuntimeTypeWrapper 1234567891011121314151617181920212223242526272829303132@Overridepublic void write(JsonWriter out, T value) throws IOException &#123; // 优先级 运行时类型adapter（最高） &gt; 声明类型adapter &gt; 运行时类型reflective adapter &gt; 声明类型reflective adapter // Order of preference for choosing type adapters // First preference: a type adapter registered for the runtime type // Second preference: a type adapter registered for the declared type // Third preference: reflective type adapter for the runtime type (if it is a sub class of the declared type) // Fourth preference: reflective type adapter for the declared type TypeAdapter chosen = delegate; // 运行时Type Type runtimeType = getRuntimeTypeIfMoreSpecific(type, value); // 如果runtimeType和传入的Type（即用户声明的对象的Type）不一样 if (runtimeType != type) &#123; TypeAdapter runtimeTypeAdapter = context.getAdapter(TypeToken.get(runtimeType)); if (!(runtimeTypeAdapter instanceof ReflectiveTypeAdapterFactory.Adapter)) &#123; // The user registered a type adapter for the runtime type, so we will use that // ReflectiveTypeAdapterFactory是注册的最后一个factory // 如果运行时TypeAdapter不是最后一种Adapter chosen = runtimeTypeAdapter; &#125; else if (!(delegate instanceof ReflectiveTypeAdapterFactory.Adapter)) &#123; // The user registered a type adapter for Base class, so we prefer it over the // reflective type adapter for the runtime type // （delegate是根据用户声明的类型找到的）如果delegate也不是最后一种，用delegate chosen = delegate; &#125; else &#123; // Use the type adapter for runtime type chosen = runtimeTypeAdapter; &#125; &#125; chosen.write(out, value);&#125; 反射类型ReflectiveTypeAdapterFactory，可以理解为用户自定义的JavaBean对应的Factory。 这是一个通过遍历对象中的属性来进行序列化的adapter。 接口、或者抽象类，不能反序列化（因为不能实例化） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253/** * Type adapter that reflects over the fields and methods of a class. */public final class ReflectiveTypeAdapterFactory implements TypeAdapterFactory &#123; private final ConstructorConstructor constructorConstructor; private final FieldNamingStrategy fieldNamingPolicy; private final Excluder excluder; private final JsonAdapterAnnotationTypeAdapterFactory jsonAdapterFactory; private final ReflectionAccessor accessor = ReflectionAccessor.getInstance(); // 先看构造器 public ReflectiveTypeAdapterFactory(ConstructorConstructor constructorConstructor, FieldNamingStrategy fieldNamingPolicy, Excluder excluder, JsonAdapterAnnotationTypeAdapterFactory jsonAdapterFactory) &#123; this.constructorConstructor = constructorConstructor; this.fieldNamingPolicy = fieldNamingPolicy; this.excluder = excluder; this.jsonAdapterFactory = jsonAdapterFactory; &#125; public boolean excludeField(Field f, boolean serialize) &#123; return excludeField(f, serialize, excluder); &#125; static boolean excludeField(Field f, boolean serialize, Excluder excluder) &#123; return !excluder.excludeClass(f.getType(), serialize) &amp;&amp; !excluder.excludeField(f, serialize); &#125; /** first element holds the default name */ private List&lt;String&gt; getFieldNames(Field f) &#123; SerializedName annotation = f.getAnnotation(SerializedName.class); if (annotation == null) &#123; String name = fieldNamingPolicy.translateName(f); return Collections.singletonList(name); &#125; String serializedName = annotation.value(); String[] alternates = annotation.alternate(); if (alternates.length == 0) &#123; return Collections.singletonList(serializedName); &#125; List&lt;String&gt; fieldNames = new ArrayList&lt;String&gt;(alternates.length + 1); fieldNames.add(serializedName); for (String alternate : alternates) &#123; fieldNames.add(alternate); &#125; return fieldNames; &#125; @Override public &lt;T&gt; TypeAdapter&lt;T&gt; create(Gson gson, final TypeToken&lt;T&gt; type) &#123; Class&lt;? super T&gt; raw = type.getRawType(); // 因为Object是所有类的父类，所以这个Factory可以适配任意类型 if (!Object.class.isAssignableFrom(raw)) &#123; return null; // it's a primitive! &#125; ObjectConstructor&lt;T&gt; constructor = constructorConstructor.get(type); // 构造一个adapter实例，getBoundFields封装了类中的属性，继续往里看 return new Adapter&lt;T&gt;(constructor, getBoundFields(gson, type, raw)); &#125; // 封装Field private Map&lt;String, BoundField&gt; getBoundFields(Gson context, TypeToken&lt;?&gt; type, Class&lt;?&gt; raw) &#123; Map&lt;String, BoundField&gt; result = new LinkedHashMap&lt;String, BoundField&gt;(); // 如果是接口，就没有Field，直接返回 if (raw.isInterface()) &#123; return result; &#125; Type declaredType = type.getType(); while (raw != Object.class) &#123; // 当前类声明的属性 Field[] fields = raw.getDeclaredFields(); for (Field field : fields) &#123; // 判断这个属性是否进行序列化和反序列化 boolean serialize = excludeField(field, true); boolean deserialize = excludeField(field, false); // 如果序列化和反序列都不需要，跳过 if (!serialize &amp;&amp; !deserialize) &#123; continue; &#125; accessor.makeAccessible(field); // 取属性的带泛型信息的Type Type fieldType = $Gson$Types.resolve(type.getType(), raw, field.getGenericType()); // 获取属性对应的序列化名称 // 如果没有@SerializedName注解，就取Field的name，再根据fieldNamingPolicy策略做下转换，比如全驼峰、驼峰带空格等 // 如果有注解，用注解里定义的name和其他备选 // fieldNames里的第一个是默认name List&lt;String&gt; fieldNames = getFieldNames(field); // previous是用来判断，是否有两个field的name重复了 BoundField previous = null; for (int i = 0, size = fieldNames.size(); i &lt; size; ++i) &#123; String name = fieldNames.get(i); if (i != 0) serialize = false; // only serialize the default name 序列化的时候只序列化第一个name，反序列时可以通过其他的备选name来反序列化 // 封装field BoundField boundField = createBoundField(context, field, name, TypeToken.get(fieldType), serialize, deserialize); // 放进result（不允许存在相同name的boundField，如果有相同name，反序列时就不知道该把value赋值给哪个属性了） BoundField replaced = result.put(name, boundField); if (previous == null) previous = replaced; &#125; // 如果name重复，抛出异常 if (previous != null) &#123; throw new IllegalArgumentException(declaredType + \" declares multiple JSON fields named \" + previous.name); &#125; &#125; // 向上递归父类 type = TypeToken.get($Gson$Types.resolve(type.getType(), raw, raw.getGenericSuperclass())); raw = type.getRawType(); &#125; return result; &#125; // 封装field，本质上是每个Field的adapter private ReflectiveTypeAdapterFactory.BoundField createBoundField( final Gson context, final Field field, final String name, final TypeToken&lt;?&gt; fieldType, boolean serialize, boolean deserialize) &#123; final boolean isPrimitive = Primitives.isPrimitive(fieldType.getRawType()); // special casing primitives here saves ~5% on Android // 如果在属性上有@JsonAdapter注解，指定了adapter，用这个adapter（优先级最高） JsonAdapter annotation = field.getAnnotation(JsonAdapter.class); TypeAdapter&lt;?&gt; mapped = null; if (annotation != null) &#123; mapped = jsonAdapterFactory.getTypeAdapter( constructorConstructor, context, fieldType, annotation); &#125; final boolean jsonAdapterPresent = mapped != null; // 如果未指定，用属性的Type找adapter if (mapped == null) mapped = context.getAdapter(fieldType); final TypeAdapter&lt;?&gt; typeAdapter = mapped; // 返回封装的Field return new ReflectiveTypeAdapterFactory.BoundField(name, serialize, deserialize) &#123; @SuppressWarnings(&#123;\"unchecked\", \"rawtypes\"&#125;) // the type adapter and field type always agree // 写json @Override void write(JsonWriter writer, Object value) throws IOException, IllegalAccessException &#123; Object fieldValue = field.get(value); // 如果有指定adapter，直接用指定的，否则要再检查一下运行时的精确Type TypeAdapter t = jsonAdapterPresent ? typeAdapter : new TypeAdapterRuntimeTypeWrapper(context, typeAdapter, fieldType.getType()); t.write(writer, fieldValue); &#125; // 读json @Override void read(JsonReader reader, Object value) throws IOException, IllegalAccessException &#123; // 不论是指定的，还是声明的type取到的adapter，直接读 Object fieldValue = typeAdapter.read(reader); if (fieldValue != null || !isPrimitive) &#123; field.set(value, fieldValue); &#125; &#125; @Override public boolean writeField(Object value) throws IOException, IllegalAccessException &#123; if (!serialized) return false; Object fieldValue = field.get(value); return fieldValue != value; // avoid recursion for example for Throwable.cause &#125; &#125;; &#125; // 定义的抽象类，Field的封装，上面看过了 static abstract class BoundField &#123; final String name; final boolean serialized; final boolean deserialized; protected BoundField(String name, boolean serialized, boolean deserialized) &#123; this.name = name; this.serialized = serialized; this.deserialized = deserialized; &#125; abstract boolean writeField(Object value) throws IOException, IllegalAccessException; abstract void write(JsonWriter writer, Object value) throws IOException, IllegalAccessException; abstract void read(JsonReader reader, Object value) throws IOException, IllegalAccessException; &#125; // 属性的adapter封装看完了，到对象的adapter了 public static final class Adapter&lt;T&gt; extends TypeAdapter&lt;T&gt; &#123; private final ObjectConstructor&lt;T&gt; constructor; private final Map&lt;String, BoundField&gt; boundFields; // 持有构造器和属性 Adapter(ObjectConstructor&lt;T&gt; constructor, Map&lt;String, BoundField&gt; boundFields) &#123; this.constructor = constructor; this.boundFields = boundFields; &#125; // 读json @Override public T read(JsonReader in) throws IOException &#123; // 空对象 if (in.peek() == JsonToken.NULL) &#123; in.nextNull(); return null; &#125; // 构造器make一个实例 T instance = constructor.construct(); try &#123; // 读对象开始 &#123; in.beginObject(); while (in.hasNext()) &#123; // 读一个名称 String name = in.nextName(); // 取对应field封装 BoundField field = boundFields.get(name); // 如果field找不到，或不需要反序列化，则跳过 if (field == null || !field.deserialized) &#123; in.skipValue(); &#125; else &#123; // 委托filed封装读入value // 如果定义的field是个List&lt;String&gt; list的话，实际上是会委托到集合Adapter去操作的，集合Adapter又会委托到集合元素的Adapter...就是这么一层层递归下去的，直到递归到平台基本类型的Adapter，执行基础的read和write field.read(in, instance); &#125; &#125; &#125; catch (IllegalStateException e) &#123; throw new JsonSyntaxException(e); &#125; catch (IllegalAccessException e) &#123; throw new AssertionError(e); &#125; // 读对象结束 &#125; in.endObject(); return instance; &#125; // 写json @Override public void write(JsonWriter out, T value) throws IOException &#123; if (value == null) &#123; out.nullValue(); return; &#125; // 写 &#123; out.beginObject(); try &#123; // 委托每个属性adapter写name和value for (BoundField boundField : boundFields.values()) &#123; if (boundField.writeField(value)) &#123; out.name(boundField.name); boundField.write(out, value); &#125; &#125; &#125; catch (IllegalAccessException e) &#123; throw new AssertionError(e); &#125; // 写 &#125; out.endObject(); &#125; &#125;&#125; 小结源码差不多就到这里了，还有很多Gson的细节、以及扩展性的地方，就不在这里深入讨论了。 捋一下大体流程 根据对象的Type，由Factory创建adapter 创建adapter的过程中，会递归对内部属性创建adapter – 可选，不同Type逻辑不同 委托adapter读写json 源码中比较难看懂的是有很多工厂类、委托类，如果能理清大体的逻辑，就比较容易触类旁通了。 to be continued… Jackson FastJson 其他序列化 性能对比比较简单粗暴地对比.. 123456789101112131415161718192021222324252627282930313233343536@Slf4jpublic class PerformanceComparison &#123; @Test public void testPerformance() throws IOException &#123; StopWatch stopWatch = new StopWatch(); stopWatch.start(\"mock list\"); List&lt;People&gt; peoples = Lists.newArrayList(); int cnt = 10; for (int i = 0; i &lt; cnt; i++) &#123; People people = new People(); people.setAge(i); people.setName(\"lily\" + i); people.setNicknames(Lists.newArrayList(\"nick1-\" + i, \"nick2-\" + i)); People friend = new People(); friend.setName(\"friend\"); people.setFriends(Maps.newHashMap(\"friend\", friend)); peoples.add(people); &#125; stopWatch.stop(); stopWatch.start(\"print\"); log.info(\"peoples = &#123;&#125;\", peoples); stopWatch.stop(); Gson gson = new Gson(); stopWatch.start(\"gson toJson\"); String json = gson.toJson(peoples); stopWatch.stop(); log.info(\"gson toJson = &#123;&#125;\", json); stopWatch.start(\"serialization \"); ObjectOutputStream outputStream = new ObjectOutputStream(new FileOutputStream(\"performance.txt\")); outputStream.writeObject(peoples); stopWatch.stop(); log.info(\"\\n&#123;&#125;\", stopWatch.prettyPrint()); &#125;&#125; 结果 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484916:19:56.483 [main] INFO wyq.learning.quickstart.serialization.PerformanceComparison - cnt = 10StopWatch '': running time = 26069677 ns---------------------------------------------ns % Task name---------------------------------------------002320593 009% mock list006721048 026% gson toJson017028036 065% serialization 16:19:56.521 [main] INFO wyq.learning.quickstart.serialization.PerformanceComparison - cnt = 100StopWatch '': running time = 32385670 ns---------------------------------------------ns % Task name---------------------------------------------000322435 001% mock list016472014 051% gson toJson015591221 048% serialization 16:19:56.611 [main] INFO wyq.learning.quickstart.serialization.PerformanceComparison - cnt = 1000StopWatch '': running time = 89620834 ns---------------------------------------------ns % Task name---------------------------------------------001630599 002% mock list022239885 025% gson toJson065750350 073% serialization 16:19:57.315 [main] INFO wyq.learning.quickstart.serialization.PerformanceComparison - cnt = 10000StopWatch '': running time = 703038881 ns---------------------------------------------ns % Task name---------------------------------------------005390392 001% mock list033934519 005% gson toJson663713970 094% serialization 16:20:03.333 [main] INFO wyq.learning.quickstart.serialization.PerformanceComparison - cnt = 100000StopWatch '': running time = 6017844063 ns---------------------------------------------ns % Task name---------------------------------------------074621832 001% mock list181025115 003% gson toJson5762197116 096% serialization 10次的平均： 12345678910111213141516:36:16.309 [main] INFO wyq.learning.quickstart.serialization.PerformanceComparison - cnt = 1016:36:16.316 [main] INFO wyq.learning.quickstart.serialization.PerformanceComparison - gson = 6ms16:36:16.316 [main] INFO wyq.learning.quickstart.serialization.PerformanceComparison - serial = 3ms16:36:16.406 [main] INFO wyq.learning.quickstart.serialization.PerformanceComparison - cnt = 10016:36:16.406 [main] INFO wyq.learning.quickstart.serialization.PerformanceComparison - gson = 1ms16:36:16.406 [main] INFO wyq.learning.quickstart.serialization.PerformanceComparison - serial = 7ms16:36:17.025 [main] INFO wyq.learning.quickstart.serialization.PerformanceComparison - cnt = 100016:36:17.026 [main] INFO wyq.learning.quickstart.serialization.PerformanceComparison - gson = 3ms16:36:17.026 [main] INFO wyq.learning.quickstart.serialization.PerformanceComparison - serial = 58ms16:36:21.700 [main] INFO wyq.learning.quickstart.serialization.PerformanceComparison - cnt = 1000016:36:21.701 [main] INFO wyq.learning.quickstart.serialization.PerformanceComparison - gson = 16ms16:36:21.701 [main] INFO wyq.learning.quickstart.serialization.PerformanceComparison - serial = 450ms16:37:14.224 [main] INFO wyq.learning.quickstart.serialization.PerformanceComparison - cnt = 10000016:37:14.224 [main] INFO wyq.learning.quickstart.serialization.PerformanceComparison - gson = 122ms16:37:14.224 [main] INFO wyq.learning.quickstart.serialization.PerformanceComparison - serial = 5123ms 可以看出来，个数越多，性能差异越明显。 Mock小工具仿Gson反序列化的逻辑，比较简单的Mock工具，可以支持常见的JavaBean的对象Mock，省去一一手工创建对象、赋值的麻烦。 ps如果对数值有要求，还是要后续自己赋值的。这个小工具的初衷是ut测试时，程序中对属性有非空校验。 pps暂未实现全部的类型 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111@Slf4jpublic class SimpleMock &#123; @SuppressWarnings(\"unchecked\") public static &lt;T, TT&gt; T mockOf(Type type) &#123; Class&lt;? super T&gt; rawType = (Class&lt;? super T&gt;) getRawType(type); Object object = null; if (rawType == String.class) &#123; object = \"mock\" + RandomUtils.nextInt(); &#125; else if (rawType == Long.class || rawType == long.class) &#123; object = RandomUtils.nextLong(); &#125; else if (rawType == Integer.class || rawType == int.class) &#123; object = RandomUtils.nextInt(); &#125; else if (rawType == BigDecimal.class) &#123; object = BigDecimal.valueOf(RandomUtils.nextFloat()).setScale(2, RoundingMode.HALF_UP); &#125; else if (rawType == Date.class) &#123; object = new Date(); &#125; else if (type instanceof GenericArrayType || type instanceof Class &amp;&amp; ((Class) type).isArray()) &#123; Type componentType = type instanceof GenericArrayType ? ((GenericArrayType) type).getGenericComponentType() : ((Class) type).getComponentType(); if (componentType == type) &#123; return null; &#125; Class&lt;TT&gt; rawComponentType = (Class&lt;TT&gt;) getRawType(componentType); List&lt;TT&gt; list = new ArrayList&lt;TT&gt;(); for (int i = 0, num = RandomUtils.nextInt(1, 5); i &lt; num; i++) &#123; TT instance = mockOf(rawComponentType); list.add(instance); &#125; Object array = Array.newInstance(rawComponentType, list.size()); for (int i = 0; i &lt; list.size(); i++) &#123; Array.set(array, i, list.get(i)); &#125; object = array; &#125; else if (Collection.class.isAssignableFrom(rawType)) &#123; Type elementType = type instanceof ParameterizedType ? ((ParameterizedType) type).getActualTypeArguments()[0] : Object.class; if (elementType == type) &#123; return null; &#125; Collection&lt;TT&gt; collection = construct(rawType); for (int i = 0, size = RandomUtils.nextInt(1, 5); i &lt; size; i++) &#123; collection.add(mockOf((Class&lt;TT&gt;) elementType)); &#125; object = collection; &#125; else if (Map.class.isAssignableFrom(rawType)) &#123; log.info(\"map ignored\"); &#125; else if (Object.class.isAssignableFrom(rawType)) &#123; try &#123; T t = (T) rawType.getDeclaredConstructor().newInstance(); Field[] fields = rawType.getDeclaredFields(); for (Field field : fields) &#123; Type fieldType = field.getGenericType(); if (fieldType instanceof ParameterizedType &amp;&amp; ((ParameterizedType) fieldType).getActualTypeArguments()[0] == type) &#123; log.info(\"cycle ignored\"); continue; &#125; TT value = mockOf(fieldType); field.setAccessible(true); field.set(t, value); &#125; object = t; &#125; catch (InstantiationException | IllegalAccessException | InvocationTargetException | NoSuchMethodException e) &#123; e.printStackTrace(); &#125; &#125; if (object == null) &#123; return null; &#125; return (T) object; &#125; @SuppressWarnings(\"unchecked\") private static &lt;T&gt; T construct(Class&lt;? super T&gt; rawType) &#123; if (Collection.class.isAssignableFrom(rawType)) &#123; if (SortedSet.class.isAssignableFrom(rawType)) &#123; return (T) new TreeSet&lt;Object&gt;(); &#125; else if (Set.class.isAssignableFrom(rawType)) &#123; return (T) new LinkedHashSet&lt;Object&gt;(); &#125; else if (Queue.class.isAssignableFrom(rawType)) &#123; return (T) new ArrayDeque&lt;Object&gt;(); &#125; else &#123; return (T) new ArrayList&lt;Object&gt;(); &#125; &#125; if (Map.class.isAssignableFrom(rawType)) &#123; return (T) new LinkedHashMap&lt;Object, Object&gt;(); &#125; return null; &#125; @Test public void test() &#123; Gson gson = new Gson(); Product product = SimpleMock.mockOf(Product.class); log.info(\"product = &#123;&#125;\", gson.toJson(product)); List&lt;Product&gt; products = SimpleMock.mockOf(new TypeToken&lt;List&lt;Product&gt;&gt;() &#123; &#125;.getType()); log.info(\"products = &#123;&#125;\", gson.toJson(products)); &#125; @Data public static class Product &#123; private Long id; private String name; private List&lt;String&gt; list; private List&lt;Product&gt; products; private Integer[] integers; &#125;&#125; 参考 Java序列化 Java 序列化和反序列化的几篇文章 Gson源码解析和它的设计模式","tags":[]},{"title":"JVM的几个大知识点","date":"2020-06-19T14:11:21.000Z","path":"2020/06/19/JVM的几个大知识点/","text":"运行时数据区域回答7个 线程私有：JVM虚拟机栈、本地方法栈、程序计数器 线程共享：堆、方法区、直接内存、常量池（属于方法区） 垃圾收集对象可达两个算法 引用计数法 当有一个地方引用了对象，对象引用计数+1，引用失效时计数-1，当计数为0时，对象可回收（为0后也不会计数再增加了，因为没有引用，就没有操作方） 可达性分析 从GC Roots出发，通过引用链可达的是存活的，不可达的是可回收的 GC Roots有哪些 虚拟机栈引用的变量 本地方法栈引用的变量 方法区类静态属性引用的变量 方法区常量引用的变量 引用类型 强引用：只要存在引用就不会被回收 软引用：当内存不够时，会被回收不 弱引用：只要gc就会被回收 虚引用：（本质上不算引用，和弱引用一样都会在gc时候被回收）和ReferenceQueue引用队列配合使用可以得知对象将被回收 算法 标记清除 标记阶段标记出未存活的对象，清除阶段回收这些对象。 （回收时会将回收空间合并，再加入空闲链表） 标记和清除的效率都不太高，有内存碎片的问题 标记整理 让所有存活对象都向一端移动，清除掉边界外的对象。 先标记，再移动。 需要移动大量对象，效率也不高 复制 将内存划分成两块，每次用其中一个来分配对象，当一块内存用完时，将存活对象移动另一块，清理当前内存块。 缺点是内存使用率变低了。 分代收集 将内存分成不同的几块，采用不同的收集算法。 商业虚拟机一般分为新生代和老年代。 新生代用复制算法。 老年代用标记清除或标记整理。 收集器 收集器名称 算法 适用区间 GC线程个数 GC线程与用户线程的交互 备注 Serial 复制 Copy 新生代 单线程 串行 Parallel Scavenge \\ PS Scavenge parallel scavenge（复制 Copy） 新生代 多线程 串行 尽量短地暂停用户线程 ParNew 多线程复制 Parallel Copy 新生代 多线程 串行 Serial Old \\ MarkSweepConpact 标记整理 serial mark-sweep 老年代 单线程 串行 Parallel Old \\ PS MarkSweep 标记清除 MarkSweep 老年代 多线程 串行 ConcurrentMarkSweep 标记清除 老年代 多线程 并行 G1 Young Generation 复制 新生代 同样分了新生代和老年代 G1 Mixed Generation 老年代 JVM参数与收集器组合 参数 收集器组合 对应到书本里的叫法 说明 -XX:+UseSerialGC young Copy old MarkSweepConpact young Serial old Serial Old -XX:+UseG1GC young G1 Young and old G1 Mixed -XX:+UseParallelGC -XX:+UseParallelOldGC young PS Scavenge old PS MarkSweep young Parallel Scavenge old Serial OldParallel Old 参数只写一个效果也相同【Java9前Unix server默认】 -XX:+UseParNewGC young ParNew old MarkSweepConpact young ParNew old Serial Old java8&amp;9中过时及移除【不推荐】 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC young ParNew old ConcurrentMarkSweep young ParNew old CMS 只设置-XX:+UseConcMarkSweepGC也行 -XX:-UseParNewGC -XX:+UseConcMarkSweepGC young Copy old ConsurrentMarkSweep young Serial old CMS java8&amp;9中过时及移除【不推荐】 各系统的默认参数 平台 参数 Windows -XX:+UseG1GC from Java 9, or before that -XX:+UseSerialGC Unix -XX:+UseG1GC from Java 9, or before that -XX:+UseParallelGC -XX:+UseParallelOldGC -XX:+UseAdaptiveSizePolicy 其他参数： -XX:SurvivorRatio=8 表示新生代的Eden占8/10，S1和S2各占1/10. 默认值是8 -XX:NewRatio=2 表示老年代是新生代的2倍，老年代占2/3，新生代1/3。默认值是2 内存分配与回收策略内存分配 对象优先在 Eden 分配 大多数情况下，对象在新生代 Eden 上分配，当 Eden 空间不够时，发起 Minor GC。 大对象直接进入老年代 大对象是指需要连续内存空间的对象，最典型的大对象是那种很长的字符串以及数组。 经常出现大对象会提前触发垃圾收集以获取足够的连续空间分配给大对象。 -XX:PretenureSizeThreshold，大于此值的对象直接在老年代分配，避免在 Eden 和 Survivor 之间的大量内存复制。 PretenureSizeThreshold 默认值是0，意味着任何对象都会现在新生代分配内存。 长期存活的对象进入老年代 为对象定义年龄计数器，对象在 Eden 出生并经过 Minor GC 依然存活，将移动到 Survivor 中，年龄就增加 1 岁， 增加到一定年龄则移动到老年代中。 -XX:MaxTenuringThreshold 用来定义年龄的阈值。 默认值是15，为0表示每次minor gc存活的对象都会进入老年代。 动态对象年龄判定 虚拟机并不是永远要求对象的年龄必须达到 MaxTenuringThreshold 才能晋升老年代。 survivor中对象按年龄从小开始累加，当累加到的大小占空间的一半，大于等于这个年龄的对象直接进入老年代。 空间分配担保 在发生 Minor GC 之前，虚拟机先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果条件成立的话，那么 Minor GC 可以确认是安全的。 如果不成立的话虚拟机会查看 HandlePromotionFailure 的值是否允许担保失败，如果允许那么就会继续检查老年代 最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次 Minor GC;如果小 于，或者 HandlePromotionFailure 的值不允许冒险，那么就要进行一次 Full GC。 内存回收Minor GC触发条件非常简单，当 Eden 空间满时，就将触发一次 Minor GC。 而 Full GC 则相对复杂，有以下条件: 调用 System.gc()只是建议虚拟机执行 Full GC，但是虚拟机不一定真正去执行。不建议使用这种方式，而是让虚拟机管理内存。 老年代空间不足 老年代空间不足的常见场景为前文所讲的大对象直接进入老年代、长期存活的对象进入老年代等。 为了避免以上原因引起的 Full GC，应当尽量不要创建过大的对象以及数组。除此之外，可以通过 -Xmn 虚拟机参数 调大新生代的大小，让对象尽量在新生代被回收掉，不进入老年代。还可以通过 -XX:MaxTenuringThreshold 调大对 象进入老年代的年龄，让对象在新生代多存活一段时间。 空间分配担保失败使用复制算法的 Minor GC 需要老年代的内存空间作担保，如果担保失败会执行一次 Full GC。 JDK 1.7 及以前的永久代空间不足 在 JDK 1.7 及以前，HotSpot 虚拟机中的方法区是用永久代实现的，永久代中存放的为一些 Class 的信息、常量、静 态变量等数据。 当系统中要加载的类、反射的类和调用的方法较多时，永久代可能会被占满，在未配置为采用 CMS GC 的情况下也 会执行 Full GC。如果经过 Full GC 仍然回收不了，那么虚拟机会抛出 java.lang.OutOfMemoryError。 为避免以上原因引起的 Full GC，可采用的方法为增大永久代空间或转为使用 CMS GC。 Concurrent Mode Failure 执行 CMS GC 的过程中同时有对象要放入老年代，而此时老年代空间不足(可能是 GC 过程中浮动垃圾过多导致暂时 性的空间不足)，便会报 Concurrent Mode Failure 错误，并触发 Full GC。 类加载机制参考文章http://www.fasterj.com/articles/oraclecollectors1.shtml 原文摘录 Young generation collectors Copy (enabled with -XX:+UseSerialGC) - the serial copy collector, uses one thread to copy surviving objects from Eden to Survivor spaces and between Survivor spaces until it decides they’ve been there long enough, at which point it copies them into the old generation. PS Scavenge (enabled with -XX:+UseParallelGC) - the parallel scavenge collector, like the Copy collector, but uses multiple threads in parallel and has some knowledge of how the old generation is collected (essentially written to work with the serial and PS old gen collectors). ParNew (enabled with -XX:+UseParNewGC) - the parallel copy collector, like the Copy collector, but uses multiple threads in parallel and has an internal ‘callback’ that allows an old generation collector to operate on the objects it collects (really written to work with the concurrent collector). G1 Young Generation (enabled with -XX:+UseG1GC) - the garbage first collector, uses the ‘Garbage First’ algorithm which splits up the heap into lots of smaller spaces, but these are still separated into Eden and Survivor spaces in the young generation for G1. Old generation collectors MarkSweepCompact (enabled with -XX:+UseSerialGC) - the serial mark-sweep collector, the daddy of them all, uses a serial (one thread) full mark-sweep garbage collection algorithm, with optional compaction. PS MarkSweep (enabled with -XX:+UseParallelOldGC) - the parallel scavenge mark-sweep collector, parallelised version (i.e. uses multiple threads) of the MarkSweepCompact. ConcurrentMarkSweep (enabled with -XX:+UseConcMarkSweepGC) - the concurrent collector, a garbage collection algorithm that attempts to do most of the garbage collection work in the background without stopping application threads while it works (there are still phases where it has to stop application threads, but these phases are attempted to be kept to a minimum). Note if the concurrent collector fails to keep up with the garbage, it fails over to the serial MarkSweepCompact collector for (just) the next GC. G1 Mixed Generation (enabled with -XX:+UseG1GC) - the garbage first collector, uses the ‘Garbage First’ algorithm which splits up the heap into lots of smaller spaces.","tags":[]},{"title":"对SpringIOC循环依赖的理解","date":"2020-06-18T21:39:03.000Z","path":"2020/06/19/对SpringIOC循环依赖的理解/","text":"spring里有三级缓存，这三个map更多的是标识bean的创建状态 singletonObjects “单例池” “容器”，缓存创建完成的单例bean的地方 earlySingletonObjects 早期的bean，不是完整的，只是一个instance singletonFactories 创建bean的原始工厂（实例化完成createBeanInstance，populateBean和initializeBean还没执行，放的是原始工厂。提前暴露的factory，在getObject的时候会执行SmartInstantiationAwareBeanPostProcessor接口的BeanPostProcessor，也包括动态代理的那些，能保证取到的bean引用是最终的bean） earlySingletonObjects和singletonFactories没有必然的联系，不循环依赖的场景也要走singletonFactories的 12345678/** Cache of singleton objects: bean name to bean instance. */private final Map&lt;String, Object&gt; singletonObjects = new ConcurrentHashMap&lt;&gt;(256);/** Cache of singleton factories: bean name to ObjectFactory. */private final Map&lt;String, ObjectFactory&lt;?&gt;&gt; singletonFactories = new HashMap&lt;&gt;(16);/** Cache of early singleton objects: bean name to bean instance. */private final Map&lt;String, Object&gt; earlySingletonObjects = new HashMap&lt;&gt;(16); 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * Return the (raw) singleton object registered under the given name. * &lt;p&gt;Checks already instantiated singletons and also allows for an early * reference to a currently created singleton (resolving a circular reference). * @param beanName the name of the bean to look for * @param allowEarlyReference whether early references should be created or not * @return the registered singleton object, or &#123;@code null&#125; if none found */protected Object getSingleton(String beanName, boolean allowEarlyReference) &#123; // 先看singletonObjects Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null &amp;&amp; isSingletonCurrentlyInCreation(beanName)) &#123; synchronized (this.singletonObjects) &#123; // 再看earlySingletonObjects singletonObject = this.earlySingletonObjects.get(beanName); if (singletonObject == null &amp;&amp; allowEarlyReference) &#123; // 如果允许earlyReference，从singletonFactories中取bean // singletonFactories中的bean是经过getEarlyBeanReference的bean，一大作用是提前创建好代理类，放入的是代理类，作用就是为了处理循环依赖 ObjectFactory&lt;?&gt; singletonFactory = this.singletonFactories.get(beanName); if (singletonFactory != null) &#123; singletonObject = singletonFactory.getObject(); this.earlySingletonObjects.put(beanName, singletonObject); this.singletonFactories.remove(beanName); &#125; &#125; &#125; &#125; return (singletonObject != NULL_OBJECT ? singletonObject : null);&#125;/** * Obtain a reference for early access to the specified bean, * typically for the purpose of resolving a circular reference. * @param beanName the name of the bean (for error handling purposes) * @param mbd the merged bean definition for the bean * @param bean the raw bean instance * @return the object to expose as bean reference */protected Object getEarlyBeanReference(String beanName, RootBeanDefinition mbd, Object bean) &#123; Object exposedObject = bean; if (!mbd.isSynthetic() &amp;&amp; hasInstantiationAwareBeanPostProcessors()) &#123; for (BeanPostProcessor bp : getBeanPostProcessors()) &#123; if (bp instanceof SmartInstantiationAwareBeanPostProcessor) &#123; SmartInstantiationAwareBeanPostProcessor ibp = (SmartInstantiationAwareBeanPostProcessor) bp; exposedObject = ibp.getEarlyBeanReference(exposedObject, beanName); &#125; &#125; &#125; return exposedObject;&#125; ioc AbstractApplicationContext#refresh#finishBeanFactoryInitialization DefaultListableBeanFactory#preInstantSingletons AbstractBeanFactory#getBean#doGetBean DefaultSingletonBeanRegistry#getSingleton AbstractautowireCapableBeanFactory#createBean#doCreateBean createBeanInstance – 实例化对象，反射或cglib populateBean – 属性注入 initializeBean – 执行工厂回调，初始化方法init-method和bean后置处理BeanPostProcessor（代理就是在这里创建的） ​ ABC循环依赖实例 到C取A的引用时，ObjectFactory的getObject调用了BBP的一些处理getEarlyBeanReference（例如动态代理的创建） C引用到的是A的最终bean 对动态代理（动态代理里的是A的实例）：C取完对A的引用，B取完对C的引用，A取完对B的引用，A继续注入其他的属性，米有毛病 对cglib：C取完对A的引用，这时A的实例是一个新的cglib产生的，容器中最终A的实例是这个cglib产生的，同时，A的B没有自动注入 cglib代理的类不能实现自动注入？— 整明白了。虽然cglib生成的代理里bService属性是null，但是它持有一个AServiceImpl的引用！所以对功能没有影响。也别想着整个幺蛾子了！ cglib的方式，正在创建的A实例还是当前的，earlySingleObject中有一个A的实例是C的属性注入过程中产生的。","tags":[]},{"title":"零碎汇总","date":"2020-06-11T17:31:48.000Z","path":"2020/06/12/零碎汇总/","text":"mac显示隐藏文件 shift+command+. Sublime json格式化 快捷键 ctrl+command+j utf8转gbk iconv -f UTF-8 -t GBK utf8.html &gt; gbk.html Git status 的中文编码git config –global core.quotepath false","tags":[]},{"title":"Redis设计与实现-读书笔记","date":"2020-06-11T17:28:50.000Z","path":"2020/06/12/Redis设计与实现-读书笔记/","text":"数据结构与对象 简单动态字符串 SDS simple dynamic string ads.h/adshdr 链表（双端链表） 列表键的实现之一：列表健包含了数量较多的元素，或者列表中的元素是比较长的字符串 adlist.h/listNode adlist.h/list 字典 哈希键的实现之一：哈希键的键值对比较多，或者键值对中元素是比较长的字符串 dict.h/dict dict.h/dictht 哈希表 跳跃表 有序集合键的实现之一：包含元素数量多，或者有序集合中元素的成员（member）是比较长的字符串 redis.h/zskiplistNode redis.h/zskiplist 整数集合 集合键的实现之一：集合只包含整数值元素，并且数量不多 intset.h/intset 压缩列表 列表键和哈希键的底层实现之一：当列表键只包含少量列表项，并且每个列表项要么是小整数，要么是长度比较短的字符串 ziplist 对象 以上的数据结构都封装在对象中再使用 包含字符串对象、列表对象、哈希对象、集合对象和有序集合对象这五种类型的对象 Redis根据对象的类型来判断一个对象是否可以执行给定的命令 还实现了基于引用计数计数的内存回收机制，对象可共享 type的枚举值 encoding属性记录了对象所使用的编码，也即是说这个对象使用了什么数据结构作为对象的底层实现 每种类型的对象都至少使用了两种不同的编码 string: int / embstr / raw list: ziplist/ linkedlist 压缩列表或者链表 （ziplist字符串元素长度小于64字节，元素个数小于512个） hash: ziplist / ht 压缩列表或者字典 （ziplist键值对键和值的字符串长度都小于64字节&amp;&amp;键值对数量个数小于512个） set: intset / ht 整数集合或者字典 （intset所有元素都是整数值&amp;&amp;个数小于512个） （使用字典时，值设置为NULL） zset: ziplist / skiplist 压缩列表或者跳跃表 （ziplist所有元素成员的长度小于64字节&amp;&amp;个数小于128个） 同时使用跳跃表和字典：跳跃表实现有序集合，方便范围操作；字典方便查找成员的分值 多态 DEL、EXPIRE等命令和LLEN等命令的区别在于，前者是基于类型的多态——一个命令可以同时用于处理多种不同类型的键，而后者是基于编码的多态——一个命令可以同时用于处理多种不同编码（比如说list的ziplist和linkedlist都实现了LLEN）。 多机数据库主从复制主库读写，从库只读 Sentinel 哨兵由一个或多个Sentinel实例（instance）组成的Sentinel系统（system）可以监视任意多个主服务器，以及这些主服务器属下的所有从服务器，并在被监视的主服务器进入下线状态时，自动将下线主服务器属下的某个从服务器升级为新的主服务器，然后由新的主服务器代替已下线的主服务器继续处理命令请求。 集群集群通过分片（sharding）来进行数据共享，并提供复制和故障转移功能。","tags":[]},{"title":"知识点汇总3","date":"2020-05-31T14:03:50.000Z","path":"2020/05/31/知识点汇总3/","text":"ioc原理、aop原理和应用 ioc原理 控制反转（依赖注入） 本质是，spring维护了一个实例的容器，在需要使用某个实例的地方，自动注入这个实例 主要运用了反射机制，通过反射来创建约定的实例，并维护在容器中 aop原理 面向切面编程 AOP原理 原理是动态代理。代理模式的定义：给某一个对象提供一个代理，并由代理对象控制对原对象的引用。实现方式： 首先有接口A，类a实现接口A 接着创建一个bInvocationHandler类，实现InvocationHandler接口，持有一个被代理对象的实例target，invoke方法中触发method 12345678910/** * proxy: 代表动态代理对象，编译时候生成的 * method：代表正在执行的方法 * args：代表调用目标方法时传入的实参 */public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println(\"代理执行\" +method.getName() + \"方法\"); Object result = method.invoke(target, args); return result;&#125; 3. 创建代理对象 1A a = (A) Proxy.newProxyInstance(A.class.getClassLoader(), new Class&lt;?&gt;[]&#123;A.class&#125;, handler) ![image-20200601110636533](../../image/image-20200601110636533.png) 2. 比如日志、监控等公共行为可以通过AOP来实现，避免大量重复代码 3. 元素 1. 切面：拦截器类，定义切点以及通知 2. 切点：具体拦截的某个业务点 3. 通知：切面当中的方法，声明通知方法在目标业务层的执行位置，通知类型如下： 1. 前置通知：@Before 在目标业务方法执行之前执行 2. 后置通知：@After 在目标业务方法执行之后执行 3. 返回通知：@AfterReturning 在目标业务方法返回结果之后执行 4. 异常通知：@AfterThrowing 在目标业务方法抛出异常之后 5. 环绕通知：@Around 功能强大，可代替以上四种通知，还可以控制目标业务方法是否执行以及何时执行 4. aspectj切面扫描的细节再看下 大数据相关，MapReduce 不考虑 Docker的原理【待看】 Docker核心技术与实现原理 Docker底层原理介绍 Http协议 基础概念 URI：uniform resource identifier 统一资源标识符 URL：uniform resource locator 统一资源定位符 URN：uniform resource name 统一资源名称 URI包括URL和URN 请求报文的格式 request line 请求行：请求方法，URL，协议 request headers 请求头：各种header 请求行和请求头合称为请求消息头 空行分隔开请求头和请求消息体 request message body 请求消息体：key-value形式或者raw格式等等 响应报文的格式 status line 状态行：协议，状态码 response headers 响应头 状态行和响应头合称为响应消息头 空行分隔开消息头和消息体 response message body 响应消息体 HTTP方法 get 主要用来获取资源 head 获取报文首部，主要用于确认 URL 的有效性以及资源更新的日期时间等。 post 主要用来传输数据 put 上传文件，不带验证机制存在安全问题，一般不使用 patch 对资源进行部分修改 – 也不常用 delete 删除文件，与put功能相反，同样不带验证机制 options 查询支持的方法，会返回Allow: GET, POST, HEAD, OPTIONS这样的内容 connect 要求在与代理服务器通信时建立隧道。使用 SSL(Secure Sockets Layer，安全套接层)和 TLS(Transport Layer Security，传输层安全)协议把通信内容 加密后经网络隧道传输。 trace 追踪路径，一般也不用… HTTP状态码 简要记一下 1XX 信息性状态码，接收的请求正在处理 2XX 请求正常处理完毕 3XX 重定向 4XX 客户端错误 5XX 服务端错误 再关注下前面的http和HTTPS的比较 cookie session介绍一下 cookie 是服务器发送到用户浏览器并保持在本地的一小块数据，会在浏览器向同一服务器再次发起请求时被带上。 用途： 会话状态管理（比如用户登录状态、购物车等） 个性化设置（比如用户自定义设置、主题等） 浏览器行为分析 生成方式 服务器发送Set-Cookie: yummy_cookie=choco这样的header，客户端得到响应报文后把cookie存在浏览器 浏览器通过document.cookie属性可创建新的cookie HttpOnly 标记为 HttpOnly 的 Cookie 不能被 JavaScript 脚本调用。 Secure 标记为 Secure 的 Cookie 只能通过被 HTTPS 协议加密过的请求发送给服务端。但即便设置了 Secure 标记，敏感信 息也不应该通过 Cookie 传输，因为 Cookie 有其固有的不安全性，Secure 标记也无法提供确实的安全保障。 session 存储在服务端，可以存储在服务器上的文件、数据库或者内存中。也可以将 Session 存储在 Redis 这种内存型数据库中 使用 Session 维护用户登录状态的过程如下: 用户进行登录时，用户提交包含用户名和密码的表单，放入 HTTP 请求报文中; 服务器验证该用户名和密码，如果正确则把用户信息存储到 Redis 中，它在 Redis 中的 Key 称为 Session ID; 服务器返回的响应报文的 Set-Cookie 首部字段包含了这个 Session ID，客户端收到响应报文之后将该 Cookie 值存入浏览器中; 客户端之后对同一个服务器进行请求时会包含该 Cookie 值，服务器收到之后提取出 Session ID，从 Redis 中取 出用户信息，继续之前的业务操作。 cookie和session的选择 cookie只能存储ASCII码字符串，session可以存储任何类型的数据 cookie存储在浏览器中，安全性较低 对于大型网址，如果所有用户信息都存储在session中，开销比较大 – 【感觉不是个问题…】 session表结构怎么设计，储存在哪里？ 我们项目里没有直接使用session，用的是商城统一单点登录 如果我设计 首先一个用户请求过来，如果没有带session id，先重定向到登录页 收到登录请求，身份验证通过后，生成一个session，key为唯一ID，即session id，value为需要存储的信息，比如用户名、生成时间等，将session id作为cookie响应发回浏览器 众多的session是key-value结构，session本身也是key-value结构 存储在Redis 你们的session cookie在项目里运用到哪里？ session是SSO用的，cookie也主要是SSO用的 偶尔用的cookie是虚拟登录这样的场景 比如超级账号：员工的erp账号以只读的形式登录到用户账号，主要用于排查问题 比如账号管家：系统中，账号体系中的主账号可以登录到子账号上，一般也只读 再如虚拟登录，业务范畴上，两个账号建立授权关系，B账号可以虚拟登录到A账号上，代为操作系统 实现：被登录人一般是sso中的session对应的用户，属于资源所属者；操作者是erp账号、主账号、虚拟登录账号等，会有登录类型区分，这些信息会先加密，再存入cookie中（还会有不同的拦截器，进行身份和权限验证） 单点登录的实现 CAS TGT：Ticket Granted Ticket（俗称大令牌，或者说票根，他可以签发ST）。【类似session】 TGC：Ticket Granted Cookie（cookie中的value），存在Cookie中，根据他可以找到TGT。【类似session id】 ST：Service Ticket （小令牌），是TGT生成的，默认是用一次就生效了。也就是上面的ticket值。 ps: 未登录状态下，访问app1时，展示登录页，浏览器会写入cas服务器的TGC；第二次访问app2，（因为app2本身校验当前请求未登录）重定向到cas服务器时，会带上TGC，cas服务器根据TGC判断用户已登录，签发新的ST再重定向到app2，这时候app2用ST校验通过，记录下自己的session cookie，提供请求内容。 OAuth 【不看了不看了！】 https://juejin.im/post/5cc81d5451882524f72cd32c https://juejin.im/post/5b3b3b61f265da0f955ca780 算法题：[删除链表中重复的节点]在一个排序的链表中,存在重复的节点,请删除该链表中重复的节点,重复的节点不保留,返回链表头指针.例如,链表1-2-3-3-4-4-5处理后为1-2-5 TCP/UDP的区别介绍一下 UDP：用户数据报协议 UDP(User Datagram Protocol)是无连接的，尽最大可能交付，没有拥塞控制，面向报文 (对于应用程序传下来的报文不合并也不拆分，只是添加 UDP 首部)，支持一对一、一对多、多对一和多对多 的交互通信。 TCP：传输控制协议 TCP(Transmission Control Protocol)是面向连接的，提供可靠交付，有流量控制，拥塞控 制，提供全双工通信，面向字节流(把应用层传下来的报文看成字节流，把字节流组织成大小不等的数据 块)，每一条 TCP 连接只能是点对点的(一对一)。 UDP首部格式 首部字段只有 8 个字节，包括源端口、目的端口、长度、检验和。12 字节的伪首部是为了计算检验和临时添加的。 TCP首部格式 序号 :用于对字节流进行编号，例如序号为 301，表示第一个字节的编号为 301，如果携带的数据长度为 100 字节，那么下一个报文段的序号应为 401。 确认号 :期望收到的下一个报文段的序号。例如 B 正确收到 A 发送来的一个报文段，序号为 501，携带的数据 长度为 200 字节，因此 B 期望下一个报文段的序号为 701，B 发送给 A 的确认报文段中确认号就为 701。 数据偏移 :指的是数据部分距离报文段起始处的偏移量，实际上指的是首部的长度。 确认 ACK :当 ACK=1 时确认号字段有效，否则无效。TCP 规定，在连接建立后所有传送的报文段都必须把 ACK 置 1。 同步 SYN :在连接建立时用来同步序号。当 SYN=1，ACK=0 时表示这是一个连接请求报文段。若对方同意建 立连接，则响应报文中 SYN=1，ACK=1。 终止 FIN :用来释放一个连接，当 FIN=1 时，表示此报文段的发送方的数据已发送完毕，并要求释放连接。 窗口 :窗口值作为接收方让发送方设置其发送窗口的依据。之所以要有这个限制，是因为接收方的数据缓存空 间是有限的。 TCP如何保证传输的有效性。 使用超时重传来实现可靠传输：如果一个已经发送的报文段在超时时间内没有收到确认，那么就重传这个报文段。 TCP滑动窗口 暂时存放字节流。发送方和接收方各有一个窗口，接收方通过TCP报文段中的窗口字段告诉发送方自己的窗口大小，发送方根据这个值和其他信息设置自己的窗口大小。 发送窗口内的字节都允许被发送，接收窗口内的字节都允许被接收。如果发送窗口左部的字节已经发送并且收到了确认，那么就将发送窗口向右滑动一定距离，直到左部第一个字节不是已发送并且已确认的状态;接收窗口的滑动类似，接收窗口左部字节已经发送确认并交付主机，就向右滑动接收窗口。 接收窗口只会对窗口内最后一个按序到达的字节进行确认，例如接收窗口已经收到的字节为 {31, 34, 35}，其中 {31} 按序到达，而 {34, 35} 就不是，因此只对字节 31 进行确认。发送方得到一个字节的确认之后，就知道这个字节之前 的所有字节都已经被接收。 TCP的拥塞控制 与流量控制的区别： 流量控制是上一题里窗口，接收方发送窗口值来控制发送方的窗口大小，从而影响发送方的发送速率。将窗口值设置为0，则发送方不能发送数据。 控制发送方的发送速率，保证接收方来得及接收。 拥塞控制 是为了降低整个网络的拥塞程度 主要通过四个算法进行拥塞控制：慢开始、拥塞避免、快重传、快恢复。 发送方需要维护一个叫做拥塞窗口(cwnd)的状态变量（只是一个状态变量，不是发送方窗口。再区别一下，拥塞窗口讨论的是报文段数量，发送窗口讨论的是字节数量） 慢开始与拥塞避免 发送的最初是慢开始，cwnd=1，发送方只能发送一个报文段；接收到确认后，将cwnd加倍，之后能发送的报文段数量是2、4、8.. ssthresh是慢开始门限（初始值自己定），当cwnd &gt;= ssthresh 时，进入拥塞避免，每个轮 次只将 cwnd 加 1。 如果出现超时，则另ssthresh = cwnd / 2，并重新执行慢开始。 见图1、2、3 快重传与快恢复 【在接收方，要求每次接收到报文段都应该对最后一个已收到的有序报文段进行确认。例如已经接收到 M1 和 M2，此时收到 M4，应当发送对 M2 的确认。】 在发送方，如果收到三个重复确认，那么可以知道下一个报文段丢失，此时执行快重传，立即重传下一个报文段。【例如收到三个 M2，则 M3 丢失，立即重传 M3。】 同时执行快恢复，令 ssthresh = cwnd / 2 ，cwnd = ssthresh，并直接进入拥塞避免。 见上图4、5 TCP建立连接的三次握手 假设A为客户端，B为服务端 首先B处于监听（listen）状态，等待客户的连接请求 A向B发送连接（SYN，同步）请求报文，SYN=1，ACK=0，seq=x（选择一个初始的序号x） B收到连接请求报文，如果同意建立连接，则向A发送连接确认报文，SYN=1，ACK=1，ack=x+1（确认号为x+1），seq=y（同时也选择一个初始的序号y） A收到B的连接确认报文后，还要向B发出确认，seq=x+1（序号为x+1），ack=y+1（确认号为y+1） 为什么要三次握手？ 三次握手是为了防止失效的连接请求到达服务器，让服务器错误打开连接。 客户端发送的连接请求如果在网络中滞留，那么隔很长时间才能收到服务器发回的连接确认，在这段时间内，客户端等待一个超时重传时间后，就会重新发送连接请求。同时滞留的连接请求最后还是会到达服务器，如果只是两次握手，那么服务器会打开两个连接。如果有第三次握手，客户端会忽略服务器之后发送的对滞留连接请求的连接确认，不进行第三次握手，因此就不会再次打开连接。 TCP四次挥手断开连接 ack都为1. A 发送连接释放报文，FIN=1。 B 收到之后发出确认，此时 TCP 属于半关闭状态，B 能向 A 发送数据但是 A 不能向 B 发送数据。 当 B 不再需要连接时，发送连接释放报文，FIN=1。 A 收到后发出确认，进入 TIME-WAIT 状态，等待 2 MSL(最大报文存活时间)后释放连接。 B 收到 A 的确认后释放连接。 四次挥手的原因 客户端发送FIN连接释放报文后，服务器收到这个报文就进入CLOSE_WAIT状态，这个状态是为了让服务器端发送未传送完毕的数据，发完后服务器就会发送FIN连接释放报文。 TIME_WAIT 客户端收到服务端的FIN报文后进入此状态，并不是直接进入CLOSED状态，还需要等待一个时间计时器设置的时间2MSL。有两个理由： 确保最后一个确认报文能够到达。如果 B 没收到 A 发送来的确认报文，那么就会重新发送连接释放请求报文， A 等待一段时间就是为了处理这种情况的发生。 等待一段时间是为了让本次连接持续时间内所产生的所有报文都从网络中消失，使得下一个新的连接不会出现旧的连接请求报文。 Java的锁机制 Java锁机制 AQS机制 背景知识 指令流水线：现代处理器的体系结构中，采用流水线的方式对指令进行处理。每个指令的工作可分为5个阶段：取指令、指令译码、执行指令、访存取数和结果写回。 CPU多级缓存：计算机系统中，存在CPU高速缓存，用于减少处理器访问内存所需平均时间。当处理器发出内存访问请求时，会先查看缓存中是否有请求数据，若命中则直接返回该数据；若不存在，则先从内存中将数据载入缓存，再将其返回处理器。 问题引入 原子性：即一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。（比如i++，如果对实例变量i的操作不做额外的控制，那么多个线程同时调用，就会出现覆盖现象，丢失部分更新。） – 因为指令流水线 可见性：是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值（存在可见性问题的根本原因是由于缓存的存在）– 因为存在缓存 顺序性：即程序执行的顺序按照代码的先后顺序执行 – 因为存在指令重排 JMM内存模型 主要目标是定义程序中各个变量的访问规则，即在虚拟机中将变量存储到内存和从内存中取出变量这样的底层细节。这里的变量指共享变量（存在竞争问题的变量），如实例字段、静态字段、数组对象元素等。不包括线程私有的局部变量、方法参数等。 内存划分：分为主内存和工作内存，【每个线程都有自己的工作内存，它们共享主内存。】【线程对共享变量的所有读写操作都在自己的工作内存中进行，不能直接读写主内存中的变量。】【不同线程间也无法直接访问对方工作内存中的变量，线程间变量值的传递必须通过主内存完成。】 主内存（Main Memory）存储所有共享变量的值。 工作内存（Working Memory）存储该线程使用到的共享变量在主内存的的值的副本拷贝。 内存间交互规则【一个变量如何从主内存拷贝到工作内存，如何从工作内存同步到主内存中】 8种原子操作 lock: 将一个变量标识为被一个线程独占状态 unclock: 将一个变量从独占状态释放出来，释放后的变量才可以被其他线程锁定 read: 将一个变量的值从主内存传输到工作内存中，以便随后的load操作 load: 把read操作从主内存中得到的变量值放入工作内存的变量的副本中 use: 把工作内存中的一个变量的值传给执行引擎，每当虚拟机遇到一个使用到变量的指令时都会使用该指令 assign: 把一个从执行引擎接收到的值赋给工作内存中的变量，每当虚拟机遇到一个给变量赋值的指令时，都要使用该操作 store: 把工作内存中的一个变量的值传递给主内存，以便随后的write操作 write: 把store操作从工作内存中得到的变量的值写到主内存中的变量 原子操作的使用规则 read、load、use必须成对顺序出现，但不要求连续出现。assign、store、write同之； 变量诞生和初始化：变量只能从主内存“诞生”，且须先初始化后才能使用，即在use/store前须先load/assign； lock一个变量后会清空工作内存中该变量的值，使用前须先初始化；unlock前须将变量同步回主内存； 一个变量同一时刻只能被一线程lock，lock几次就须unlock几次；未被lock的变量不允许被执行unlock，一个线程不能去unlock其他线程lock的变量。 对于double和long，虽然内存模型允许对非volatile修饰的64位数据的读写操作分为两次32位操作来进行，但商用虚拟机几乎把64位数据的读写实现为了原子操作，可以忽略这个问题。 先行发生原则 【Java内存模型具备一些先天的“有序性”，即不需要通过任何同步手段（volatile、synchronized等）就能够得到保证的有序性，这个通常也称为happens-before原则。】 如果两个操作的执行次序不符合先行原则且无法从happens-before原则推导出来，那么它们就不能保证它们的有序性，虚拟机可以随意地对它们进行重排序。 程序次序规则（Program Order Rule）：一个线程内，逻辑上书写在前面的操作先行发生于书写在后面的操作。 监视器锁规则（Monitor Lock Rule）：一个unLock操作先行发生于后面对同一个锁的lock操作。“后面”指时间上的先后顺序。 volatile变量规则（Volatile Variable Rule）：对一个volatile变量的写操作先行发生于后面对这个变量的读操作。“后面”指时间上的先后顺序。 传递规则（Transitivity）：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C。 线程启动规则（Thread Start Rule）：Thread对象的start()方法先行发生于此线程的每个一个动作。 线程中断规则（Thread Interruption Rule）：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生（通过Thread.interrupted()检测）。 线程终止规则（Thread Termination Rule）：线程中所有的操作都先行发生于线程的终止检测，我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值手段检测到线程已经终止执行。 对象终结规则（Finaizer Rule）：一个对象的初始化完成（构造函数执行结束）先行发生于他的finalize()方法的开始。 问题解决 原子性 由JMM保证的原子性变量操作 基本数据类型的读写（工作内存）是原子的 JMM的lock和unlock指令可以实现更大范围的原子性保证，虚拟机提供synchronized关键字和Lock锁来保证原子性。 可见性 volatile关键字修饰的变量，被线程修改后会立即同步回主内存，其他线程要读取这个变量会从主内存刷新值到工作内存。（因为缓存一致性协议会让其他工作内存中的该变量拷贝无效，必须得从主内存再读取）即read、load、use三者连续顺序执行，assign、store、write连续顺序执行。 synchronized/Lock 由lock和unlock的使用规则保证【这里有疑问啊，synchronized有lock和unlock，但是Lock没有吧…Lock怎么保证可见性？还是说Lock保证不了可见性。可见性只能由volatile保证？–参见ConcurrentHashMap，有synchronized，还配合volatile使用—ConcurrentHashMap有些是不加锁的操作，比如get，所以还是用volatile保证可见性。synchronized 锁的是某个node节点，对这个node节点的】 synchronized有语义规定，说是通过内存屏障实现的 线程解锁前，必须把共享变量的最新值刷新到主内存中 线程加锁前，将清空工作内存中共享变量的值，从而使用共享变量时需要从主内存中重新读取最新的值 Lock用了cas，有lock cmpxchg，lock前缀指令保证了可见性，同时有内存屏障的作用 同时，这俩还能保证临界区操作的所有变量的可见性因为内存屏障 LOCK前缀的指令具有如下效果： 把写缓冲区中所有的数据刷新到内存中 注意，是所有的数据，可不仅仅是对state的修改 ReentrantLock对可见性的支持 All threads will see the most recent write to a volatile field, along with any writes which preceded that volatile read/write. Reentrantlock的lock和unlock方法实际上会cas一个state的变量，state是volatile的，因此夹在两次state之间的操作都能保证可见性。这应该算是happen before的传递性… 顺序性 volatile 禁止指令重排序 synchronized/Lock “一个变量在同一个时刻只允许一条线程对其执行lock操作” – 感觉这个也没用，不然双重检查的单例怎么还用volatile关键字来防止重排序 – 最多保证原子性，被加锁内容按照顺序被多个线程执行 锁机制 volatile： 保证可见性和顺序性【实现方式：lock前缀指令+依赖MESI缓存一致性协议】 volatile修饰的变量，在进行写操作的时候会多一行汇编代码，lock指令，做两件事： 将当前处理器缓存行的数据写回系统内存 引起其他处理器里缓存了该内存地址的数据无效。【实现缓存一致性协议，处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了（处理器发现自己缓存行对应的内存地址被修改，就会将自己的缓存设置成无效状态）】 final：有两个重排序规则 – 不甚了解 写final域的重排序规则：在构造函数内对一个final域的写入，与随后把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序。 读final域的重排序规则：初次读一个包含final域的对象的引用，与随后初次读这个final域，这两个操作之间不能重排序。 synchronized关键字 使用哪个对象的监视器： 修饰对象方法时，使用当前对象的监视器 修饰静态方法时，使用类类型（Class 的对象）监视器 修饰代码块时，使用括号中的对象的监视器 必须为 Object 类或其子类的对象 无锁 -&gt; 偏向锁 -&gt; 轻量级锁 -&gt; 重量级锁 简单理解，只有一个线程CAS时，如果CAS成功，表示没有锁竞争，保持偏向锁状态，如果CAS失败，说明有竞争，（先撤销偏向锁，将对象头设置成无锁状态，并设置为不可偏向）升级为轻量级锁。 几种锁的适用场景 偏向锁：锁不仅不存在线程竞争，而且总是由同一个线程多次获得，这时候偏向锁的代价最低。适用只有一个线程访问同步块的场景。（如果有别的线程来获取锁，发现） 轻量级锁：同步块执行时间非常快的，执行完就替换回mark word，别的线程要加锁也很快，CAS。（如果同步块执行很久，竞争线程自旋cas非常久，就很耗cpu，所以会升级到重量级锁，竞争线程阻塞挂起） 重量级锁：同步块执行时间比较长的，原因如2 锁升级机制 1. 偏向锁：线程检查锁对象的状态是否是可偏向的，是的话，检查mark word中的线程ID是不是自己，是的话进入代码块，不是的话，将线程ID cas进mark word。cas失败的话，说明之前是别的线程（假设A）取到的了，等待全局安全点，JVM暂停线程A，检查线程A的状态：如果A不在活动中，将锁对象的mark word中的线程ID置空，再cas成自己的线程ID；如果A在活动中（未退出代码块），升级为轻量级锁：JVM在线程A中分配锁记录，拷贝锁对象mark word，并将锁对象mark word指向这个锁记录；在线程B中分配锁记录，拷贝锁对象mark word，并持续自旋cas（如果自旋n次还失败，就要再次升级成重量级锁了..）... 2. 轻量级锁：如果不止一个线程尝试获取锁，就会升级到轻量级锁。**通过自适应自旋CAS的方式获取锁。**如果获取失败，说明存在竞争，膨胀为重量级锁，线程阻塞。默认自旋10次。**将对象头中的Mark Word复制到栈帧（一块空间，称为锁记录）中，然后用CAS将对象头中的Mark Word替换为指向栈帧中锁记录的指针。** 3. 重量级锁：通过系统的线程互斥锁来实现的，未获取到锁的线程会阻塞挂起 大佬的图，来源见水印 右下角的轻量级锁释放的补充说明： 在某个线程A正持有轻量级锁的时候（还在代码块内运行，时间比较长），某个线程B自旋cas竞争锁（肯定是cas失败了）失败了，这时候就会升级成重量级锁了，mark word指向了互斥量的指针，这和线程A中锁记录的值不同，线程A后续释放锁就失败了（意识到已经升级成重量级锁，唤醒其他挂起的线程） ![img](../../image/172a2f26935d33c8.png) 4. AQS 【内存屏障和”lock”前缀指令】理解 volatile通过编译器，既会增加”lock”前缀指令，也会加上内存屏障（mfence等） 内存屏障是抽象概念，各个硬件、处理器实现不同 lock前缀指令和mfence等是具体实现 mesi协议保证缓存和主存间的一致性 有了msei协议，为什么汇编层面还需要lock(volatile)来实现可见性？ - Rob Zhang的回答 - 知乎 https://www.zhihu.com/question/334662600/answer/747038084 内存屏障能保证从storebuffer到缓存再到主存的一致性，在多线程运行中可以作为mesi的补充（因为mesi管不到那么多），但内存屏障 lock前缀主要是为了提供原子操作，虽然它也包含了内存屏障功能（强制将寄存器、缓存（、storebuffer/invalid queue或类似的东西）等强制同步到主存） &gt; 关于内存屏障的几个问题？ - cao的回答 - 知乎 https://www.zhihu.com/question/47990356/answer/108650501 &gt; &gt; x86在Windows下的内存屏障是用lock前缀指令来达到效果的 **简单理解：** **内存屏障保证了寄存器和缓存之间的一致性** **lock前缀保证操作原子性** **二者都能保证可见性** x86架构的内存屏障 1. sfence: Store Barrier = StoreStore Barriers 写屏障 所有sfence之前的store指令都在sfence之前被执行，并刷出到CPU的L1 Cache中； 所有在sfence之后的store指令都在sfence之后执行，禁止重排序到sfence之前。 所以，所有Store Barrier之前发生的内存更新都是可见的。 2. lfence: Load Barrier = LoadLoad Barriers 读屏障 所有在lfence之后的load指令，都在lfence之后执行，并且一直等到load buffer被该CPU读完才能执行之后的load指令（即要刷新失效的缓存）。配合sfence，使所有sfence之前发生的内存更新，对lfence之后的load操作都可见。 3. mfence: Full Barrier = StoreLoad Barriers 全屏障 综合了sfence和lfence的作用，强制所有在mfence之前的store/load指令都在mfence之前被执行，之后的store/load指令都在之后执行，禁止跨越mfence重排序。并且都刷新到缓存&amp;重新载入无效缓存。 3种重排序类型 1是编译器重排序，2和3是处理器重排序。会导致多线程程序出现内存可见性问题。 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-LevelParallelism，ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。 死锁 死锁定义：多个进程循环等待它方占有的资源而无限期地僵持下去的局面。 产生死锁的必要条件： 互斥（mutualexclusion），一个资源每次只能被一个进程使用 不可抢占（nopreemption），进程已获得的资源，在未使用完之前，不能强行剥夺 占有并等待（hold andwait），一个进程因请求资源而阻塞时，对已获得的资源保持不放 环形等待（circularwait），若干进程之间形成一种首尾相接的循环等待资源关系。 对待死锁的策略主要有： 死锁预防：破坏导致死锁必要条件中的任意一个就可以预防死锁。例如，要求用户申请资源时一次性申请所需要的全部资源，这就破坏了保持和等待条件；将资源分层，得到上一层资源后，才能够申请下一层资源，它破坏了环路等待条件。预防通常会降低系统的效率。 死锁避免：避免是指进程在每次申请资源时判断这些操作是否安全，例如，使用银行家算法。死锁避免算法的执行会增加系统的开销。 死锁检测：死锁预防和避免都是事前措施，而死锁的检测则是判断系统是否处于死锁状态，如果是，则执行死锁解除策略。 死锁解除：这是与死锁检测结合使用的，它使用的方式就是剥夺。即将某进程所拥有的资源强行收回，分配给其他的进程。 避免死锁的几个常见方法 避免一个线程同时获取多个锁 避免一个线程在锁内同时占用多个资源，尽量保证每个锁只占用一个资源。 尝试使用定时锁，使用lock.tryLock(timeout)来代替使用内部锁机制。 对于数据库锁，加锁和解锁必须在一个数据库连接里，否则会出现解锁失败的情况。 三次握手和四次挥手,time_wait是什么状态【见83、84】 B树 B+树区别说一下 B树非叶子节点也存储数据 数据库平时用到过什么 咱们先问MySQL MySQL索引原理知道吗，问了两种索引的区别 索引的分类和优缺点 innodb和myisam的区别 乐观锁悲观锁区别说一下 数据库四种隔离状态 分别有什么问题 redis的数据结构 Redis基础 Redis 键值（Key-Value）存储数据库 string 字符类型 map 散列类型 list 列表类型 set 集合类型 sortedset 有序集合类型 sortset底层，原理，怎么保证有序 TreeSet具体实现是TreeMap，底层是红黑树 containsKey、get、put、remove 时间复杂度log(n) 红黑树 通过对任何一条（根到叶子的）路径上的各个节点的着色方式的限制，确保没有一条路径会比其他路径长出2倍，因而近乎是平衡的 性质： 每个节点是红色的，或是黑色的 根节点是黑色的 每个叶子节点（Nil）是黑色的 如果一个节点是红色的，则它的两个子节点是黑色的 对每个节点，从该节点到其子孙节点的所有路径上包含相同个数的黑色节点。（红节点不能有红孩子）（从该节点出发的所有下降路径，有相同的黑节点个数） 黑高度：从一个节点到达一个叶子节点的任意一条路径上黑色节点的个数 红黑树的黑高度定义为根节点的黑高度 冯诺依曼计算机的结构 运算器（算术逻辑单元，处理寄存器） 控制器（指令寄存器，程序计数器） 存储器（存储数据和指令） 输入设备 输出设备 操作系统的虚拟内存 100-104 不懂 https://juejin.im/entry/592257b62f301e006b183b95 进程的调度 进程间的通讯方式 线程间的同步方式 进程和线程的区别 常见的排序算法 冒泡排序-复杂度O(n^2)-交换排序 对所有相邻记录的关键码值进行比较，如果是逆序（L.r[1].key &gt; L.r[2].key），则将其交换，最终达到有序化。 对无序区从前向后依次将相邻记录的关键码进行比较，若逆序，则将其交换，从而使得关键码值小的记录向上“飘浮”（左移），关键码值大的记录向下“坠落”（右移）。 每经过一趟冒泡排序，都使无序区中关键码值最大的记录进入有序区，对于由n个记录组成的记录序列，最多经过n-1趟冒泡排序，就可将这n个记录重新按关键码顺序排列。可看出，若“在一趟排序过程中没有进行过交换记录的操作”，则可结束整个排序过程。 12345678910111213141516171819202122 /** * 冒泡排序--更像坠落排序 * * @param nums */@Overridepublic void sort(T[] nums) &#123; int len = nums.length; boolean isSorted = false; // i区分无序区和有序区 for (int i = len - 1; i &gt;= 0 &amp;&amp; !isSorted; i--) &#123; isSorted = true; // j将大元素右移 for (int j = 0; j &lt; i; i++) &#123; if (less(nums[j + 1], nums[j])) &#123; isSorted = false; swap(nums, j, j + 1); &#125; &#125; &#125;&#125; 2. 选择排序-复杂度O(n^2)-选择排序 每一趟从待排序的记录中选出关键码最小的记录，顺序放在已排好序的子序列后面，直到全部记录排序完毕。 123456789101112131415161718/** * 选择排序 * * @param nums */@Overridepublic void sort(T[] nums) &#123; for (int i = 0; i &lt; nums.length; i++) &#123; // index指向每轮最小的数 int index = i; for (int j = i + 1; j &lt; nums.length; j++) &#123; if (less(nums[j], nums[index])) &#123; index = j; &#125; &#125; swap(nums, i, index); &#125;&#125; 3. 插入排序-复杂度O(n^2) -插入排序 基本思想是，将待排序的记录，按其关键码的大小插入到已经排好序的有序子表中，直到全部记录插入完成为止。 12345678910111213/** * 插入排序 * * @param nums */@Overridepublic void sort(T[] nums) &#123; for (int i = 1; i &lt; nums.length; i++) &#123; for (int j = i; j &gt; 0 &amp;&amp; less(nums[j], nums[j - 1]); j--) &#123; swap(nums, j, j - 1); &#125; &#125;&#125; 4. 归并排序-复杂度O(nlogn)-归并排序 123456789101112131415161718192021222324252627282930313233343536373839 /** * 归并排序 * * @param nums */public void sort(T[] nums, Class&lt;T&gt; clazz) &#123; T[] copy = (T[]) Array.newInstance(clazz, nums.length); System.arraycopy(nums, 0, copy, 0, nums.length); sort(nums, copy, 0, nums.length);&#125; private void sort(T[] nums, T[] copy, int begin, int end) &#123; if (begin + 1 == end) &#123; return; &#125; int half = (end - begin) / 2; sort(nums, copy, begin, begin + half); sort(nums, copy, begin + half, end); merge(copy, nums, begin, begin + half, end);&#125; private void merge(T[] nums, T[] copy, int begin, int mid, int end) &#123; int i = begin, j = mid, k = begin; while (i &lt; mid &amp;&amp; j &lt; end) &#123; if (nums[i].compareTo(nums[j]) &lt; 0) &#123; copy[k] = nums[i++]; &#125; else &#123; copy[k] = nums[j++]; &#125; k++; &#125; while (i &lt; mid) &#123; copy[k++] = nums[i++]; &#125; while (j &lt; end) &#123; copy[k++] = nums[j++]; &#125;&#125; 6. 快速排序-复杂度O(nlogn)-交换排序 123456789101112131415161718192021222324252627282930 /** * 快速排序 * * @param nums */@Overridepublic void sort(T[] nums) &#123; sort(nums, 0, nums.length - 1);&#125; private void sort(T[] nums, int begin, int end) &#123; int left = begin + 1, right = end; while (left &lt; right) &#123; while (left &lt;= end &amp;&amp; less(nums[left], nums[begin])) &#123; left++; &#125; while (right &gt;= begin &amp;&amp; less(nums[begin], nums[right])) &#123; right--; &#125; if (left &lt; right) &#123; swap(nums, left, right); &#125; &#125; if (right &lt;= end &amp;&amp; right &gt;= begin) &#123; swap(nums, begin, right); sort(nums, begin, right - 1); sort(nums, right + 1, end); &#125;&#125; 7. 堆排序-复杂度O(nlogn)-堆排序 **位置 k 的节点的父节点位置 为 k/2，而它的两个子节点的位置分别为 2k 和 2k+1。** ![image-20200613191135925](../../image/image-20200613191135925.png) 1234567891011121314151617181920212223242526272829303132333435363738394041 /** * 堆排序 排成最大堆 * 数组第 0 个位置不能有元素 * * @param nums */@Overridepublic void sort(T[] nums) &#123; int cnt = nums.length - 1; for (int k = cnt / 2; k &gt;= 1; k--) &#123; sink(nums, k, cnt); &#125; while (cnt &gt; 1) &#123; swap(nums, 1, cnt); cnt--; sink(nums, 1, cnt); &#125;&#125; /** * 下沉 * * @param nums * @param k */private void sink(T[] nums, int k, int len) &#123; while (k * 2 &lt;= len) &#123; int child = k * 2; // 判断child + 1未越界 if (child + 1 &lt; len &amp;&amp; less(nums[child], nums[child + 1])) &#123; child++; &#125; // 如果子节点比k小，退出循环 if (less(nums[child], nums[k])) &#123; break; &#125; swap(nums, k, child); k = child; &#125;&#125; spring 事务实现 Spring事务的底层依赖MySQL的事务，代码层面上利用AOP实现。 常用的是@Transactional注解，会被解析生成一个代理服务，TransactionInterceptor对它进行拦截处理，进行事务开启、 commit或者rollback的操作。 另外，spring还定义了事务传播行为，有7种类型，项目中常见的是PROPAGATION_REQUIRED。如果没有事务就新建事务，如果存在事务，就加入这个事务。 执行事务的时候使用TransactionInterceptor进行拦截，然后处理 [事务传播行为](https://segmentfault.com/a/1190000013341344) | 事务传播行为类型 | 说明 | | ---------------------------- | ------------------------------------------------------------ | | **PROPAGATION_REQUIRED** | 如果当前没有事务，就新建一个事务，如果已经存在一个事务中，加入到这个事务中。这是最常见的选择。（如果父方法有事务，加入父方法的事务；父方法没有事务，则自己新建一个事务） | | PROPAGATION_SUPPORTS | 支持当前事务，如果当前没有事务，就以非事务方式执行。（如果父方法有事务，加入父方法的事务；父方法没有事务，则以非事务执行） | | PROPAGATION_MANDATORY | 使用当前的事务，如果当前没有事务，就抛出异常。（依赖父方法事务） | | **PROPAGATION_REQUIRES_NEW** | 新建事务，如果当前存在事务，把当前事务挂起。（如果父方法有事务，把父方法事务挂起，自己新建事务；父方法没有事务，则自己新建一个事务） | | PROPAGATION_NOT_SUPPORTED | 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。（如果父方法有事务，把父方法事务挂起，以非事务执行自己的操作；父方法没有事务，则以非事务执行）（总是以非事务执行，不报错） | | PROPAGATION_NEVER | 以非事务方式执行，如果当前存在事务，则抛出异常。（总是以非事务执行，如果父方法存在事务，抛异常） | | **PROPAGATION_NESTED** | 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与PROPAGATION_REQUIRED类似的操作。 | REQUIRED、REQUIRES_NEW、NESTED的对比 REQUIRED共用一个事务。 REQUIRES_NEW 有独立的子事务，子事务异常**不会**导致父事务回滚，父事务异常也**不会**导致子事务回滚，相互独立。 NESTED 子事务嵌套在父事务中，父事务回滚会引起子事务回滚；父事务正常、子事务异常，子事务可以单独回滚。 [源码详解](https://juejin.im/post/59e87b166fb9a045030f32ed) 1. txNamespaceHandle注册的`InfrastructureAdvisorAutoProxyCreator`是一个BeanPostProcessor，主要是为了创建动态代理（wrapIfNecessary） 这几个类是可以自动创建代理的 ![image-20200617195143639](../../image/image-20200617195143639.png) 2. 在创建代理的时候，获取切面 txNamespaceHandler注册了一个Advisor（BeanFactoryTransactionAttributeSourceAdvisor），再在这个advisor中判断是否当前bean符合这个切面（主要实现就是看有没有@Transactional注解） ![image-20200617191910196](../../image/image-20200617191910196.png) 3. `TransactionInterceptor`是advice，增强，执行切面工作 摘录：https://my.oschina.net/fifadxj/blog/785621 spring-jdb的事务流程： 1234567891011121314DefaultTransactionDefinition def = new DefaultTransactionDefinition();PlatformTransactionManager txManager = new DataSourceTransactionManager(dataSource);TransactionStatus status = txManager.getTransaction(def);try &#123; //get jdbc connection... //execute sql... txManager.commit(status);&#125;catch (Exception e) &#123; txManager.rollback(status); throw e;&#125; PlatformTransactionManager的getTransaction(), rollback(), commit()是spring处理事务的核心api，分别对应事务的开始，提交和回滚。 - TransactionSynchronizationManager负责从ThreadLocal中存取jdbc connection - 创建事务的时候会通过dataSource.getConnection()获取一个新的jdbc connection，然后绑定到ThreadLocal - 在业务代码中执行sql时，通过DataSourceUtils.getConnection()从ThreadLocal中获取当前事务的jdbc connection, 然后在该jdbc connection上执行sql - commit和rollback事务时，从ThreadLocal中获取当前事务的jdbc connection，然后对该jdbc connection进行commit和rollback ![143421_Bmpa_1452390.png (1716×1288)](../../image/143421_Bmpa_1452390.png) mybatis-spring的事务流程： 配置 1234567891011121314&lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\" /&gt;&lt;/bean&gt;&lt;bean id=\"sqlSessionFactory\" class=\"org.mybatis.spring.SqlSessionFactoryBean\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\" /&gt; &lt;property name=\"transactionFactory\"&gt; &lt;bean class=\"org.apache.ibatis.spring.transaction.SpringManagedTransactionFactory\" /&gt; &lt;/property&gt; &lt;/bean&gt;&lt;bean id=\"sqlSession\" class=\"org.mybatis.spring.SqlSessionTemplate\"&gt; &lt;constructor-arg index=\"0\" ref=\"sqlSessionFactory\" /&gt;&lt;/bean&gt; - mybatis-spring依赖DataSourceTransactionManager来处理事务，并没有创建自己的PlatformTransactionManager实现。 - mybatis通过SqlSessionFactoryBuilder创建SqlSessionFactory，而mybatis-spring通过SqlSessionFactoryBean创建SqlSessionFactory。 - 配置使用SpringManagedTransactionFactory来创建MyBatis的Transaction实现SpringManagedTransaction - 配置使用SqlSessionTemplate代替通过SqlSessionFactory.openSession()获取SqlSession 调用过程 ![143554_iORI_1452390.png (2401×2029)](../../image/143554_iORI_1452390.png) 可以看到mybatis-spring处理事务的主要流程和spring jdbc处理事务并没有什么区别，都是通过DataSourceTransactionManager的getTransaction(), rollback(), commit()完成事务的生命周期管理，而且jdbc connection的创建也是通过DataSourceTransactionManager.getTransaction()完成，mybatis并没有参与其中，mybatis只是在执行sql时通过DataSourceUtils.getConnection()获得当前thread的jdbc connection，然后在其上执行sql。 sqlSessionTemplate是DefaultSqlSession的一个代理类，它通过SqlSessionUtils.getSqlSession()试图从ThreadLocal获取当前事务所使用的SqlSession。如果是第一次获取时会调用SqlSessionFactory.openSession()创建一个SqlSession并绑定到ThreadLocal，同时还会通过TransactionSynchronizationManager注册一个SqlSessionSynchronization。 SqlSessionSynchronization是一个事务生命周期的callback接口，mybatis-spring通过SqlSessionSynchronization在事务提交和回滚前分别调用DefaultSqlSession.commit()和DefaultSqlSession.rollback() 这里的DefaultSqlSession只会进行一些自身缓存的清理工作，并不会真正提交事务给数据库，原因是这里的DefaultSqlSession使用的Transaction实现为SpringManagedTransaction，SpringManagedTransaction在提交事务前会检查当前事务是否应该由spring控制，如果是，则不会自己提交事务，而将提交事务的任务交给spring，所以DefaultSqlSession并不会自己处理事务。 DefaultSqlSession执行sql时，会通过SpringManagedTransaction调用DataSourceUtils.getConnection()从ThreadLocal中获取jdbc connection并在其上执行sql。 **mybatis-spring做的最主要的事情是：** 1. **在SqlSession执行sql时通过用SpringManagedTransaction代替mybatis的JdbcTransaction，让SqlSession从spring的ThreadLocal中获取jdbc connection。** 2. **通过注册事务生命周期callback接口SqlSessionSynchronization，让SqlSession有机会在spring管理的事务提交或回滚时清理自己的内部缓存。** spring的循环依赖如何解决？为什么要三级缓存？ https://juejin.im/post/5c98a7b4f265da60ee12e9b2 https://juejin.im/post/5e927e27f265da47c8012ed9 spring对循环依赖的处理有三种情况： 构造器的循环依赖：这种依赖spring是处理不了的，直 接抛出BeanCurrentlylnCreationException异常。 单例模式下的setter循环依赖：通过“三级缓存”处理循环依赖。 非单例循环依赖：无法处理。 如何解决的？ 只能解决单例的属性循环依赖的情况。本质上是通过将创建好的、或正在创建中的bean缓存起来。比如A和B循环依赖，创建A时先将A的实例放入缓存，自动注入属性B时，发现缓存中没有B，那么来创建B的实例，将B实例化放入缓存，注入属性A，发现A在缓存中，取出来赋值给A。bean B创建完成返回，赋值给A的属性B。这时候A和B的bean就都创建好了。 为什么要三级？看起来一级就可以实现呀？ [为什么要三级缓存](https://juejin.im/post/5ec88da26fb9a047e25d5691#h-2)：循环依赖的关键点：**提前暴露绑定A原始引用的工厂类到工厂缓存。等需要时触发后续操作处理A的早期引用，将处理结果放入二级缓存** 只有一级singeltonObjects肯定是不行的，需要一个放半成品的地方 实际上二级就够了，可以解决循环依赖的问题 考虑到代理的情况，就需要objectFactories这个三级缓存了，因为代理的创建是在第三步，这时候动态代理还没产生，注入了也不是最终的实例。放入三级缓存时，重写了getObject方法，会调用BeanPostProcessor的getEarlyBeanReference，这时候取到的就会是动态代理后的。 jvm参数调优详细过程，到为什么这么设置，好处，一些gc场景，如何去分析gc日志 jvm调优的基本原则： 大多数Java应用不需要进行JVM优化 大多数导致GC频繁、内存使用率高的问题的原因是代码层面的问题（代码层面） 上线前应考虑将JVM参数设置最优 减少创建对象的数量（代码层面） 较少使用全局变量和大对象（代码层面） 优先架构调优和代码调优，JVM优化是不得已的手段，或者说是发现问题 分析gc情况优化代码比优化JVM参数更好（代码层面） https://juejin.im/post/5dea4cb46fb9a01626644c36 新生代配置原则： 1.追求响应时间优先 这种需求下，新生代尽可能设置大一些，并通过实际情况调整新生代大小，直至接近系统的最小响应时间。因为新生代比较大，发生垃圾回收的频率会比较低，响应时间快速。 2.追求吞吐量优先 吞吐量优先的应用，在新生代中的大部分对象都会被回收，所以，新生代尽可能设置大。此时不追求响应时间，垃圾回收可以并行进行。 3.避免设置过小新生代 设置过小，YGC会很频繁，同时，很可能导致对象直接进入老年代中，老年代空间不足发生FullGC。 老年代配置原则： 1.追求响应时间优先 这种情况下，可以使用CMS收集器，以获取最短回收停顿时间，但是其内存分配需要注意，如果设置小了会造成回收频繁并且碎片变多；如果设置大了，回收的时间会很长。所以，最优的方案是根据GClog分析垃圾回收信息，调整内存大小。 2.追求吞吐量优先 吞吐量优先通常需要分配一个大新生代、小老年代，将短期存活的对象在新生代回收掉。 优先级队列的底层原理？ 堆，默认是小顶堆 入队 123456789101112131415161718192021222324public boolean offer(E e) &#123; if (e == null) throw new NullPointerException(); modCount++; int i = size; if (i &gt;= queue.length) grow(i + 1); siftUp(i, e); size = i + 1; return true;&#125;private static &lt;T&gt; void siftUpComparable(int k, T x, Object[] es) &#123; Comparable&lt;? super T&gt; key = (Comparable&lt;? super T&gt;) x; while (k &gt; 0) &#123; // 如果父节点比自己大 int parent = (k - 1) &gt;&gt;&gt; 1; Object e = es[parent]; if (key.compareTo((T) e) &gt;= 0) break; es[k] = e; k = parent; &#125; es[k] = key;&#125; 出队 1234567891011121314151617181920212223242526272829303132333435363738public E poll() &#123; final Object[] es; final E result; if ((result = (E) ((es = queue)[0])) != null) &#123; modCount++; final int n; final E x = (E) es[(n = --size)]; es[n] = null; if (n &gt; 0) &#123; final Comparator&lt;? super E&gt; cmp; if ((cmp = comparator) == null) siftDownComparable(0, x, es, n); else siftDownUsingComparator(0, x, es, n, cmp); &#125; &#125; return result;&#125;private static &lt;T&gt; void siftDownComparable(int k, T x, Object[] es, int n) &#123; // assert n &gt; 0; Comparable&lt;? super T&gt; key = (Comparable&lt;? super T&gt;)x; int half = n &gt;&gt;&gt; 1; // loop while a non-leaf while (k &lt; half) &#123; // 从孩子中选一个小的 int child = (k &lt;&lt; 1) + 1; // assume left child is least Object c = es[child]; int right = child + 1; if (right &lt; n &amp;&amp; ((Comparable&lt;? super T&gt;) c).compareTo((T) es[right]) &gt; 0) c = es[child = right]; if (key.compareTo((T) c) &lt;= 0) break; es[k] = c; k = child; &#125; es[k] = key;&#125; redis如何实现分布式锁，zk如何实现分布式锁，两者的区别。如果service还没执行完，分布式锁在redis中已经过期了，怎么解决这种问题 redis实现分布式锁：setNX，创建成功表明获得了锁（要注意设置超时、谁加锁谁解锁、解锁的原子性） zk实现分布式锁：在路径下创建临时顺序节点，序号最小的节点表示获得了锁，其他竞争者监听自己的前一个节点 redisson给的答案是锁获取成功后，注册一个定时任务，每隔一定时间(this.internalLockLeaseTime / 3L, 10s)就去续约 加一个监听器，如果key快要超时了，就进行续约（重置成30s） synchronized底层实现，加在方法上和加在同步代码块中编译后的区别、类锁、对象锁 编译时候加入监视器锁 1234567891011public class SyncTest &#123; public void syncBlock() &#123; synchronized (this) &#123; System.out.println(\"hello block\"); &#125; &#125; public synchronized void syncMethod() &#123; System.out.println(\"hello method\"); &#125;&#125; 加在方法上：方法上有synchronized关键字，flags里有ACC_SYNCHRONIZED https://blog.csdn.net/hosaos/java/article/details/100990954 ACC_SYNCHRONIZED是获取监视器锁的一种隐式实现(没有显示的调用monitorenter，monitorexit指令) 如果字节码方法区中的ACC_SYNCHRONIZED标志被设置，那么线程在执行方法前会先去获取对象的monitor对象，如果获取成功则执行方法代码，执行完毕后释放monitor对象 123456789101112131415public synchronized void syncMethod(); descriptor: ()V flags: ACC_PUBLIC, ACC_SYNCHRONIZED Code: stack=2, locals=1, args_size=1 0: getstatic #2 // Field java/lang/System.out:Ljava/io/PrintStream; 3: ldc #5 // String hello method 5: invokevirtual #4 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 8: return LineNumberTable: line 15: 0 line 16: 8 LocalVariableTable: Start Length Slot Name Signature 0 9 0 this Lwyq/learning/quickstart/juc/SyncTest; 加在同步块上：monitorenter / monitorexit 关键字 12345678910111213141516171819202122232425262728293031323334353637383940public void syncBlock(); descriptor: ()V flags: ACC_PUBLIC Code: stack=2, locals=3, args_size=1 0: aload_0 1: dup 2: astore_1 3: monitorenter 4: getstatic #2 // Field java/lang/System.out:Ljava/io/PrintStream; 7: ldc #3 // String hello block 9: invokevirtual #4 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 12: aload_1 13: monitorexit 14: goto 22 17: astore_2 18: aload_1 19: monitorexit 20: aload_2 21: athrow 22: return Exception table: from to target type 4 14 17 any 17 20 17 any LineNumberTable: line 9: 0 line 10: 4 line 11: 12 line 12: 22 LocalVariableTable: Start Length Slot Name Signature 0 23 0 this Lwyq/learning/quickstart/juc/SyncTest; StackMapTable: number_of_entries = 2 frame_type = 255 /* full_frame */ offset_delta = 17 locals = [ class wyq/learning/quickstart/juc/SyncTest, class java/lang/Object ] stack = [ class java/lang/Throwable ] frame_type = 250 /* chop */ offset_delta = 4 volatile在编译上的体现 1234567public class VolatileTest &#123; private volatile int i; public void plus() &#123; i = 2; &#125;&#125; 字节码 网上查到的是变量上flags有ACC_VOLATILE标识，自己编译出来没看到… 123456789101112131415public void plus(); descriptor: ()V flags: ACC_PUBLIC Code: stack=2, locals=1, args_size=1 0: aload_0 1: iconst_2 2: putfield #2 // Field i:I 5: return LineNumberTable: line 11: 0 line 12: 5 LocalVariableTable: Start Length Slot Name Signature 0 6 0 this Lwyq/learning/quickstart/juc/VolatileTest; 看文章说还是lock前缀指令 http://gee.cs.oswego.edu/dl/jmm/cookbook.html – x86架构下，实现是lock前缀指令，支持”SSE2”扩展 (Pentium4 and later)的版本支持mfence指令（比lock前缀更推荐），cas的cmpxchg的实现需要lock前缀 https://www.cnblogs.com/xrq730/p/7048693.html 1. 锁总线，其它CPU对内存的读写请求都会被阻塞，直到锁释放，不过实际后来的处理器都采用锁缓存替代锁总线，因为锁总线的开销比较大，锁总线期间其他CPU没法访问内存 2. lock后的写操作会回写已修改的数据，同时让其它CPU相关缓存行失效，从而重新从主存中加载最新的数据 3. 不是内存屏障却能完成类似内存屏障的功能，阻止屏障两边的指令重排序 整理一下最终的实现： 1. lock前缀指令会引起处理器缓存回写到内存 2. 一个处理器的缓存回写到内存会导致其他处理器的缓存无效，这是MESI实现的（缓存一致性协议） 3. 另外，lock前缀指令能完成内存屏障的功能，阻止屏障前后的指令重排序 这篇文章https://juejin.im/post/5ea938426fb9a043856f2f6a提到，x86下使用`lock`来实现`StoreLoad`，并且只有 `StoreLoad` 有效果。x86 上怎么使用 Barrier 的说明可以在 openjdk 的代码中看到，在这里[src/hotspot/cpu/x86/assembler_x86.hpp](https://github.com/openjdk/jdk/blob/9a69bb807beb6693c68a7b11bee435c0bab7ceac/src/hotspot/cpu/x86/assembler_x86.hpp)。 mvcc，怎么实现rr rc ​ ​ mysql间隙锁有没有了解，死锁有没有了解，写一段会造成死锁的sql语句，死锁发生了如何解决，mysql有没有提供什么机制去解决死锁 gap lock 如何保证RocketMQ 消息的顺序性，如何解决重复消费问题 针对kafka来说 如何保证消息的顺序性： 一个分区内的消息是顺序的 一个主题的不同分区之间，消息不能保证有序 – 对同一类消息指定相同的key，相同的key会哈希到同一个分区，这样可以保证这部分消息的有序性 https://www.cnblogs.com/756623607-zhang/p/10506909.html 如何解决重复消费： 1. kafka自带的消费机制 consumer消费后，会定期将消费过的offset偏移量提交给broker。如果consumer重启，会继续上次的offset开始消费。 2. 业务上保证幂等性 如果进程挂了或机器宕机，没来得及提交offset，需要业务上进行幂等。 比如建立一张消息表。 1. 生产者，发送消息前判断库中是否有记录（有记录说明已发送），没有记录，先入库，状态为待消费，然后发送消息并把主键id带上。 2. 消费者，接收消息，通过主键ID查询记录表，判断消息状态是否已消费。若没消费过，则处理消息，处理完后，更新消息记录的状态为已消费。 explain 可以看到哪些信息，什么信息说明什么，explain的结果列讲一下 https://dev.mysql.com/doc/refman/8.0/en/explain-output.html | Column | JSON Name | Meaning || ———————————————————— | ————— | ———————————————————— || id | select_id | The SELECT identifier select标识 || select_type | None | The SELECT type select类型 || table | table_name | The table for the output row 表名 || partitions | partitions | The matching partitions 使用的分区 || type | access_type | The join type join类型 || possible_keys | possible_keys | The possible indexes to choose 可能使用的索引 || key | key | The index actually chosen 实际使用的索引 || key_len | key_length | The length of the chosen key 实际使用的索引的长度 || ref | ref | The columns compared to the index 与索引进行对比的列 || rows | rows | Estimate of rows to be examined 预估要检查的行数 || filtered | filtered | Percentage of rows filtered by table condition 符合条件的数据的百分比 || Extra | None | Additional information 额外的信息 | select_type 常见的有SIMPLE（简单查询，无union、subqueries）、PRIMARY（子查询的外层）、SUBQUERY、UNION等 type system：表中只有一行数据，const的特殊情况 const：至多有一行matching，可以理解为主键或唯一索引的= （单表，对tbl_name来说，1是const） 1234SELECT * FROM tbl_name WHERE primary_key=1;SELECT * FROM tbl_name WHERE primary_key_part1=1 AND primary_key_part2=2; eq_ref：主键或唯一索引的= （多表关联，other_table的结果不定，所以对ref_table来说，选择不是const） 123456SELECT * FROM ref_table,other_table WHERE ref_table.key_column=other_table.column;SELECT * FROM ref_table,other_table WHERE ref_table.key_column_part1=other_table.column AND ref_table.key_column_part2=1; ref：（非主键与非唯一索引的）其他索引的=和&lt;=&gt;（等和不等） 12345678SELECT * FROM ref_table WHERE key_column=expr;SELECT * FROM ref_table,other_table WHERE ref_table.key_column=other_table.column;SELECT * FROM ref_table,other_table WHERE ref_table.key_column_part1=other_table.column AND ref_table.key_column_part2=1; fulltext 用到了全文索引 ref_or_null 类似ref，会额外检索包含null的行 index_merge 用到了多个索引，索引合并优化 unique_subquery 替换下面的in子查询，子查询返回不重复的集合 1value IN (SELECT primary_key FROM single_table WHERE some_expr) index_subquery 区别于unique_subquery，用于非唯一索引，可以返回重复值 1value IN (SELECT key_column FROM single_table WHERE some_expr) range 索引范围查找，包括主键、唯一索引、其他索引——即，所有key =, &lt;&gt;, &gt;, &gt;=, &lt;, &lt;=, IS NULL, &lt;=&gt;, BETWEEN, LIKE, or IN() 1234567891011SELECT * FROM tbl_name WHERE key_column = 10;SELECT * FROM tbl_name WHERE key_column BETWEEN 10 and 20;SELECT * FROM tbl_name WHERE key_column IN (10,20,30);SELECT * FROM tbl_name WHERE key_part1 = 10 AND key_part2 IN (10,20,30); index 类似all，但是只扫描索引，有两种情况 覆盖索引，select中的列都在索引中，extra中显示using index 利用索引的顺序进行全表扫描（比如有order by），extra中不宣誓using index all 全表扫描 rows和filtered rows：MySQL认为需要检查的行数 filtered：rows中会被过滤出来的——即符合条件的——的数据的百分比 rows*filtered=查询出的结果数 extra 常见的有 using index 列信息只从索引出，不用再从实际行取。使用了覆盖索引 using where 没有可用的索引，通过where条件过滤 using filesort 需要额外排序 ….还有好多 aqs，countDownLatch如何实现 ​ java如何实现序列化的，Serialization底层如何实现的 简单说来，是将类信息和数据信息递归写成字节信息 线上服务器cpu飙高，如何处理这个问题 定位进程：top 查看cpu占用情况 定位线程：如果是Java应用，top -Hp pid 定位代码` printf %x tid 打印出线程ID对应的16进制数 0xtid jstack pid |grep -A 200 0xtid 内核态 和 用户态、cas 和 sout 哪个用到了内核态和用户态的切换 sout用到了切换 哪些典型的应用用的是udp dns: Domain Name System，域名系统 域名解析 TFTP: Trivial File Transfer Protocol,简单文件传输协议 1.包总量较少的通信（DNS、SNMP等） 2.视频、音频等多媒体通信（即时通信） 3.限定于 LAN 等特定网络中的应用通信 4.广播通信（广播、多播） 计算密集型/IO密集型 任务 分别如何设置线程池的核心线程数和最大线程数，为什么这么设置 https://blog.csdn.net/weixin_40151613/java/article/details/81835974 计算密集型： CPU使用率比较高，（也就是一些复杂运算，逻辑处理） 线程数设置为CPU核数 IO密集型： cpu使用率较低，程序中会存在大量I/O操作占据时间，导致线程空余出来 **一般设置线程数为CPU核数的2倍** 最佳线程数目 = （（线程等待时间+线程CPU时间）/线程CPU时间 ）* CPU数目 线程等待时间越长，需要越多的线程 补充 1. 高并发、任务执行时间短的业务：线程池线程数可以设置为CPU核数+1，减少线程上下文的切换 2. 并发不高、任务执行时间长的业务： 1. 假如是业务时间长集中在IO操作上，也就是IO密集型的任务，因为IO操作并不占用CPU，所以不要让所有的CPU闲下来，可以适当加大线程池中的线程数目，让CPU处理更多的业务 2. 假如是业务时间长集中在计算操作上，也就是计算密集型任务，和（1）一样，线程池中的线程数设置得少一些，减少线程上下文的切换 3. 并发高、业务执行时间长，解决这种类型任务的关键不在于线程池而在于整体架构的设计 1. 数据能否做缓存 2. 增加服务器 3. 业务执行时间长的问题，也可能需要分析一下，看看能不能使用中间件（任务时间过长的可以考虑拆分逻辑放入队列等操作）对任务进行拆分和解耦。 synchronized底层实现【见JMM】 https://juejin.im/post/5bfe6ddee51d45491b0163eb Mark Word 对象头【见JMM】 redis实现 带注解3.0源码 《Redis设计与实现》 https://github.com/huangz1990/redis-3.0-annotated Java中的NIO，BIO，AIO分别是什么# BIO:同步并阻塞，服务器实现模式为一个连接一个线程，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销，当然可以通过线程池机制改善。BIO方式适用于连接数目比较小且固定的架构，这种方式对服务器资源要求比较高，并发局限于应用中，JDK1.4以前的唯一选择，但程序直观简单易理解。 NIO:同步非阻塞，服务器实现模式为一个请求一个线程，即客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接有I/O请求时才启动一个线程进行处理。NIO方式适用于连接数目多且连接比较短（轻操作）的架构，比如聊天服务器，并发局限于应用中，编程比较复杂，JDK1.4开始支持。 AIO:异步非阻塞，服务器实现模式为一个有效请求一个线程，客户端的I/O请求都是由OS先完成了再通知服务器应用去启动线程进行处理.AIO方式使用于连接数目多且连接比较长（重操作）的架构，比如相册服务器，充分调用OS参与并发操作，编程比较复杂，JDK7开始支持。 java中的反射 field的赋值底层实现 以UnsafeBooleanFieldAccessorImpl为例，也是利用unsafe 偏移 ps: Unsafe工具类 static final Unsafe unsafe = Unsafe.getUnsafe(); 1234567891011121314151617181920212223242526272829// set public void set(Object obj, Object value) throws IllegalArgumentException, IllegalAccessException &#123; ensureObj(obj); if (isFinal) &#123; throwFinalFieldIllegalAccessException(value); &#125; if (value == null) &#123; throwSetIllegalArgumentException(value); &#125; if (value instanceof Boolean) &#123; // 这里 unsafe.putBoolean(obj, fieldOffset, ((Boolean) value).booleanValue()); return; &#125; throwSetIllegalArgumentException(value); &#125;// get public Object get(Object obj) throws IllegalArgumentException &#123; return Boolean.valueOf(getBoolean(obj)); &#125; public boolean getBoolean(Object obj) throws IllegalArgumentException &#123; ensureObj(obj); // 这里 return unsafe.getBoolean(obj, fieldOffset); &#125; 测试一下git commit -am ‘xx’ MyBatis，Mybatis与Spring MyBatis 消除了大部分 JDBC 的样板代码、手动设置参数以及检索结果。通过简洁的设计最大限度地简化开发和提升性能。 解除SQL与程序代码的耦合，通过提供dao层，将业务逻辑和数据访问逻辑分离开。设计更清晰，更易维护。 MyBatis整体架构 MyBatis层级结构 裸用`sqlSession`是上面的红框 spring用mapper/dao接口代理，本质上是一个MapperProxy，从下面的红框开始执行 ![image-20200708201723303](../../image/image-20200708201723303.png) spring事务是在哪个环节起作用？ https://mybatis.org/spring/zh/transactions.html &gt; 一个使用 MyBatis-Spring 的其中一个主要原因是它允许 MyBatis 参与到 Spring 的事务管理中。而不是给 MyBatis 创建一个新的专用事务管理器，MyBatis-Spring 借助了 Spring 中的 DataSourceTransactionManager 来实现事务管理。 &gt; &gt; 一旦配置好了 Spring 的事务管理器，你就可以在 Spring 中按你平时的方式来配置事务。并且支持 @Transactional 注解和 AOP 风格的配置。在事务处理期间，一个单独的 `SqlSession` 对象将会被创建和使用。当事务完成时，这个 session 会以合适的方式提交或回滚。 &gt; &gt; 事务配置好了以后，MyBatis-Spring 将会透明地管理事务。 所以，最外层是事务，每个事务会起一个`SqlSession`。 几篇文章： 入门，裸用mybatis：https://juejin.im/post/5aa5c6fb5188255587232e5a#heading-0 mybatis执行，包括整合spring后的流程：https://juejin.im/post/5e350d895188254dfd43def5#heading-9 关于JDBC：https://juejin.im/post/5c75e6666fb9a049cd54dc88 Mybatis和spring整合的使用：https://juejin.im/post/5cdfed6ef265da1b6720dcaf mybatis框架说明： 整体执行流程说明： sqlSession执行流程说明： 关键流程（以下整个可以看成裸用MyBatis的执行流程） config文件加载：解析xml文件配置项 mapper文件加载：上一个流程中的一个环节，解析完后封装成MappedStatement，存入configuration SqlSource创建流程：上一流程的一个环节，SqlSource是MappedStatement的一部分，主要存放sql和占位的参数名称 -- 解析环节结束 SqlSession执行流程：`sqlSessionFactory.openSession`主要是建立了一个和数据库的连接connection 获取BoundSql流程：`sqlSession.xx`方法执行时，需要获取BoundSql，BoundSql本质上是SqlSource和执行请求的入参的一个组合 参数映射流程：根据顺序，或者根据名称（只是大略看了一眼） 结果集映射流程：根据名称（只是大略看了一眼） mybatis的openSession默认开启事务，autocommit为false，隔离级别为null mybatis的JdbcTransaction ![image-20200710111527094](../../image/image-20200710111527094.png) 整合spring的几个组件 `org.mybatis.spring.SqlSessionFactoryBean` 注入sqlSessionFactory `org.mybatis.spring.mapper.MapperScannerConfigurer`扫描指定包 1. 将包下class文件加入到beanDefinition中，bean类型指定为MapperFactoryBean 2. SqlSessionFactoryBean构建sqlSessionFactory时，扫描mapper xml文件，根据namespace在MapperRegistry中注入对应mapper接口的MapperProxyFactory 3. MapperFactoryBean-&gt;getObject中生成mapper的代理类MapperProxy（通过MapperFactoryBean中的interface，即mapper的namespace找到MapperProxyFactory，再生产出代理类） 以下大概知道了 现在差一个中间环节，mapper的beanDefinition怎么变成MapperProxy..以及MapperFactoryBean的作用 还有个SqlSessionTemplate：https://juejin.im/post/5cea1f386fb9a07ea803a70e 还有MapperProxyFactory -- 来创建MapperProxy Java动态代理：https://juejin.im/post/5c1ca8df6fb9a049b347f55c MapperFactoryBean MapperProxy MapperMethod -- 到这里之后，流程就转到sqlSession.selectOne之类的了 Mybatis缓存 https://juejin.im/post/5e81fb126fb9a03c546c22bb MyBatis 系统中默认定义了两级缓存：**一级缓存**和**二级缓存** - 默认情况下，只有一级缓存开启。（SqlSession级别的缓存，也称为本地缓存） - 二级缓存需要手动开启和配置，它是基于 namespace 级别的缓存，缓存只作用于 cache 标签所在的映射文件中的语句。 130. DelayQueue https://www.cnblogs.com/jobs/archive/2007/04/27/730255.html DelayQueue = BlockingQueue + PriorityQueue + Delayed 扩展信息备查 http status备查 1XX 信息 100 Continue :表明到目前为止都很正常，客户端可以继续发送请求或者忽略这个响应。 2XX 成功 200 OK 204 No Content :请求已经成功处理，但是返回的响应报文不包含实体的主体部分。一般在只需要从客户端 往服务器发送信息，而不需要返回数据时使用。 206 Partial Content :表示客户端进行了范围请求，响应报文包含由 Content-Range 指定范围的实体内容。 3XX 重定向 301 Moved Permanently :永久性重定向 302 Found :临时性重定向 303 See Other :和 302 有着相同的功能，但是 303 明确要求客户端应该采用 GET 方法获取资源。 注:虽然 HTTP 协议规定 301、302 状态下重定向时不允许把 POST 方法改成 GET 方法，但是大多数浏览器都 会在 301、302 和 303 状态下的重定向把 POST 方法改成 GET 方法。 304 Not Modified :如果请求报文首部包含一些条件，例如:If-Match，If-Modified-Since，If-None- Match，If-Range，If-Unmodified-Since，如果不满足条件，则服务器会返回 304 状态码。 307 Temporary Redirect :临时重定向，与 302 的含义类似，但是 307 要求浏览器不会把重定向请求的 POST 方法改成 GET 方法。 4XX 客户端错误 400 Bad Request :请求报文中存在语法错误。 401 Unauthorized :该状态码表示发送的请求需要有认证信息(BASIC 认证、DIGEST 认证)。如果之前已进 行过一次请求，则表示用户认证失败。 403 Forbidden :请求被拒绝。 404 Not Found 5XX 服务器错误 500 Internal Server Error :服务器正在执行请求时发生错误。 503 Service Unavailable :服务器暂时处于超负载或正在进行停机维护，现在无法处理请求。 synchronized锁消除和锁粗化 https://juejin.im/post/5d96db806fb9a04e0f30f0eb 锁消除 锁消除主要是 JIT 编译器的优化操作，首先对于热点代码 JIT 编译器会将其编译为机器码，后续执行的时候就不需要在对每一条 class 字节码解释为机器码然后再执行了从而提升效率，它会根据逃逸分析来对代码做一定程度的优化比如锁消除，栈上分配等等 1234567public void f() &#123; Object obj = new Object(); synchronized(obj) &#123; System.out.println(obj); &#125;&#125;复制代码 JIT 编译器发现 f() 中的对象只会被一个线程访问，那么就会取消同步 12345public void f() &#123; Object obj = new Object(); System.out.println(obj);&#125;复制代码 锁粗化 如果在一段代码中连续的对同一个对象反复加锁解锁，其实是相对耗费资源的，这种情况下可以适当放宽加锁的范围，减少性能消耗。 当 JIT 发现一系列连续的操作都对同一个对象反复加锁和解锁，甚至加锁操作出现在循环体中的时候，会将加锁同步的范围扩散到整个操作序列的外部。 123456for (int i = 0; i &lt; 10000; i++) &#123; synchronized(this) &#123; do(); &#125;&#125;复制代码 粗化后的代码 12345synchronized(this) &#123; for (int i = 0; i &lt; 10000; i++) &#123; do(); &#125;&#125; ​","tags":[]},{"title":"Redis学习笔记","date":"2020-05-29T16:54:04.000Z","path":"2020/05/30/learn-redis/","text":"redis环境docker安装redis 1234docker pull redis:latestdocker run -itd --name redis-test -p 6379:6379 redisdocker exec -it redis-test /bin/bashredis-cli 数据结构the little redis book 字符串（Strings）set` strlen &lt;key&gt; getrange &lt;key&gt; &lt;start&gt; &lt;end&gt; append &lt;key&gt; &lt;value&gt; 计数incr incrby decr decrby 存储对象（简单或复杂）和计数 位图setbit getbit 散列（Hashes）hset hget 相关的操作还包括在同一时间设置多个域、同一时间获取多个域、获取所有的域和值、列出所有的域或者删除指定的一个域： 12345hmset users:goku race saiyan age 737hmget users:goku race powerlevelhgetall users:gokuhkeys users:gokuhdel users:goku age 列表（Lists）对于一个给定的关键字，列表数据结构让你可以存储和处理一组值。你可以添加一个值到列表里、获取列表的第一个值或最后一个值以及用给定的索引来处理值。列表数据结构维护了值的顺序，提供了基于索引的高效操作。 12lpush newusers gokultrim newusers 0 50 12keys = redis.lrange(&apos;newusers&apos;, 0, 10)redis.mget(*keys.map &#123;|u| &quot;users:#&#123;u&#125;&quot;&#125;) 集合（Sets）集合数据结构常常被用来存储只能唯一存在的值，并提供了许多的基于集合的操作，例如并集。集合数据结构没有对值进行排序，但是其提供了高效的基于值的操作。 使用集合数据结构的典型用例是朋友名单的实现： 12sadd friends:leto ghanima paul chani jessicasadd friends:duncan paul jessica alia 而且，我们可以查看两个或更多的人是不是有共同的朋友： 1sinter friends:leto friends:duncan 甚至可以在一个新的关键字里存储结果： 1sinterstore friends:leto_duncan friends:leto friends:duncan 有序集合（Sorted Sets）最后也是最强大的数据结构是分类集合数据结构。 提供了排序功能 sorting and ranking 1zadd friends:duncan 70 ghanima 95 paul 95 chani 75 jessica 1 vladimir 对于duncan的朋友，要怎样计算出标记（score）为90或更高的人数？ 1zcount friends:duncan 90 100 如何获取chani在名单里的倒排索引值（rank）？ 1zrevrank friends:duncan chani 时间复杂度常数时间复杂度O(1) 对数时间复杂度O(log(n)) 线性时间复杂度O(n) 二次时间O(n^2) 指数时间O(c^n) Round Trips and Pipelining数据往返？批量数据？ mget命令，接受多个关键字，然后返回值： 12keys = redis.lrange(&apos;newusers&apos;, 0, 10)redis.mget(*keys.map &#123;|u| &quot;users:#&#123;u&#125;&quot;&#125;) 或者是sadd命令，能添加一个或多个成员到集合里： 12sadd friends:vladimir pitersadd friends:paul jessica leto &quot;leto II&quot; chani 支持流式请求 12345redis.pipelined do 9001.times do redis.incr(&apos;powerlevel&apos;) endend 事务Redis是单线程运行，每个命令都具有原子性 为什么选择单线程模型 首先要执行multi命令，紧随其后的是所有你想要执行的命令（作为事务的一部分），最后执行exec命令去实际执行命令，或者使用discard命令放弃执行命令。Redis的事务功能保证了什么？ 事务中的命令将会按顺序地被执行 事务中的命令将会如单个原子操作般被执行（没有其它的客户端命令会在中途被执行） 事务中的命令要么全部被执行，要么不会执行 1234multihincrby groups:1percent balance -9000000000hincrby groups:99percent balance 9000000000exec 但这样并不能避免并发问题，因为可能有多个Redis客户端在","tags":[]},{"title":"知识点汇总2","date":"2020-05-28T15:55:52.000Z","path":"2020/05/28/知识点汇总2/","text":"ioc原理、aop原理和应用 ioc原理 控制反转（依赖注入） 本质是，spring维护了一个实例的容器，在需要使用某个实例的地方，自动注入这个实例 主要运用了反射机制，通过反射来创建约定的实例，并维护在容器中 aop原理 面向切面编程 原理是动态代理 比如日志、监控等公共行为可以通过AOP来实现，避免大量重复代码 元素 大数据相关，MapReduce Docker的原理 Http协议 cookie session介绍一下 session表结构怎么设计，储存在哪里？ 你们的session cookie在项目里运用到哪里？ 算法题：[删除链表中重复的节点]在一个排序的链表中,存在重复的节点,请删除该链表中重复的节点,重复的节点不保留,返回链表头指针.例如,链表1-2-3-3-4-4-5处理后为1-2-5 TCP/UDP的区别介绍一下 TCP如何保证传输的有效性。 TCP的拥塞控制 TCP建立连接的三次握手 TCP四次挥手断开连接 Java的锁机制 死锁产生的原因 三次握手和四次挥手,time_wait是什么状态 B树 B+树区别说一下 数据库平时用到过什么 咱们先问MySQL MySQL索引原理知道吗，问了两种索引的区别 索引的分类和优缺点 innodb和myisam的区别 乐观锁悲观锁区别说一下 数据库四种隔离状态 分别有什么问题 redis的数据结构 sortset底层，原理，怎么保证有序 冯诺依曼计算机的结构 操作系统的虚拟内存 进程的调度 进程间的通讯方式 线程间的同步方式 进程和线程的区别 常见的排序算法 spring 事务实现 蚂蚁花呗一面（近一个小时左右），主要是看简历里的经历。面试官很谦和，会用比较委婉的方式了解面试者更方面的经验和能力。 Java容器有哪些？哪些是同步容器,哪些是并发容器？ ArrayList和LinkedList的插入和访问的时间复杂度？ java反射原理， 注解原理？ 说说一致性 Hash 原理 HashMap在什么情况下会扩容，或者有哪些操作会导致扩容？ 新生代分为几个区？使用什么算法进行垃圾回收？为什么使用这个算法？ HashMap push方法的执行过程？ HashMap检测到hash冲突后，将元素插入在链表的末尾还是开头？ https和http区别，有没有用过其他安全传输手段？ 线程池的工作原理，几个重要参数，然后给了具体几个参数分析线程池会怎么做，最后问阻塞队列的作用是什么？ 1.8还采用了红黑树，讲讲红黑树的特性，为什么人家一定要用红黑树而不是AVL、B树之类的？ linux怎么查看系统负载情况？ spring 一个bean装配的过程？ 请详细描述springmvc处理请求全流程？ 项目用 Spring 比较多，有没有了解 Spring 的原理？AOP 和 IOC 的原理 第一面能通过，后续被录用的可能性就比较高了，第一轮非常重要 蚂蚁花呗二面 Mysql主从同步的实现原理？ MySQL是怎么用B+树？ 谈谈数据库乐观锁与悲观锁? 有使用过哪些NoSQL数据库？MongoDB和Redis适用哪些场景？ 描述分布式事务之TCC服务设计？ Redis和memcache有什么区别？Redis为什么比memcache有优势？ Redis 的数据结构 查询中哪些情况不会使用索引？ 数据库索引，底层是怎样实现的，为什么要用B树索引？ 海量数据过滤，黑名单过滤一个 url。 讲一讲AtomicInteger，为什么要用CAS而不是synchronized？ 蚂蚁金服的面试非常严格，流程也很规范。 蚂蚁花呗三面 solr和mongodb的区别，存数据为什么不用solr？ 分布式 session 的共享方案有哪些，有什么优劣势 谈谈分布式锁、以及分布式全局唯一ID的实现比较？ 考虑redis的时候，有没有考虑容量？大概数据量会有多少？ Redis 的 list zset 的底层实现 集群监控的时候，重点需要关注哪些技术指标？这些指标如何优化？ 排序算法的复杂度，快速排序非递归实现。 消息中间件有哪些？他们之间的优劣势？ 从千万的数据到亿级的数据，会面临哪些技术挑战？你的技术解决思路？ 数据库分库分表需要怎样来实现？ 蚂蚁花呗四面 分布式架构设计哪方面比较熟悉 介绍你实践的性能优化案例，以及你的优化思路 介绍项目 谈一个你觉得你学到最多的项目，使用了什么技术，挑战在哪里 各种聊项目，从项目的架构设计到部署流程。 最近有没有学习过新技术？ 有什么想问我的？ 最近两年遇到的最大的挫折，从挫折中学到了什么？ 三年到五年的职业规划？ HR面 1.工作中遇到的最大挑战是什么，你如何克服的？ 2.你最大的优点和最大的缺点，各自说一个？ 3.未来的职业发展，短期和长期的规划是什么？ 最后，你懂的，主要就是HR走流程了，主要问了未来的职业规划。 以上就是蚂蚁花呗高级java面试题，以下最全蚂蚁花呗高级Java必考题范围和答案。 相信它会给大家带来很多收获。（更全的内容和资料，在文末获取）","tags":[]},{"title":"Spring-两种PropertyPlaceholderConfigurer","date":"2020-05-19T11:26:24.000Z","path":"2020/05/19/Spring-两种PropertyPlaceholderConfigurer/","text":"加载配置文件的地方 Case Study项目中注册了两个PropertyPlaceholderConfigurer，一个是xml中配置的PropertyPlaceholderConfigurer，显示设置ignoreUnresolvablePlaceholders为true（忽略找不到的占位符）；另一个是第三方组件注册的PropertySourcesPlaceholderConfigurer，默认配置。 12345678910&lt;bean id=\"propertyConfigurer\" class=\"org.springframework.beans.factory.config.PropertyPlaceholderConfigurer\"&gt; &lt;property name=\"ignoreUnresolvablePlaceholders\" value=\"true\"/&gt; &lt;property name=\"ignoreResourceNotFound\" value=\"true\"/&gt; &lt;property name=\"properties\" ref=\"prop\"/&gt; &lt;property name=\"locations\"&gt; &lt;list&gt; &lt;value&gt;classpath:config/*.properties&lt;/value&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt;","tags":[{"name":"PropertyPlaceholderConfigurer","slug":"PropertyPlaceholderConfigurer","permalink":"https://northernw.github.io/tags/PropertyPlaceholderConfigurer/"}]},{"title":"JVM堆与非堆","date":"2020-05-07T18:33:01.000Z","path":"2020/05/08/JVM堆与非堆/","text":"堆和非堆内存 Heap memory Code Cache Eden Space 新生代 Survivor Space 新生代 Tenured Gen 老年代 non-heap memory Perm Gen 永久代 native heap?(I guess) 内存监控方法 jamp -heap pid 查看堆内存使用情况 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647jmap -heap 212Attaching to process ID 212, please wait...Debugger attached successfully.Server compiler detected.JVM version is 25.20-b23using thread-local object allocation.Parallel GC with 43 thread(s)Heap Configuration: MinHeapFreeRatio = 0 MaxHeapFreeRatio = 100 MaxHeapSize = 4294967296 (4096.0MB) NewSize = 1431306240 (1365.0MB) MaxNewSize = 1431306240 (1365.0MB) OldSize = 2863661056 (2731.0MB) NewRatio = 2 # 老年代与新生代大小比值 SurvivorRatio = 8 # Eden与Survivor大小比值 MetaspaceSize = 21807104 (20.796875MB) CompressedClassSpaceSize = 1073741824 (1024.0MB) MaxMetaspaceSize = 17592186044415 MB G1HeapRegionSize = 0 (0.0MB)Heap Usage:PS Young GenerationEden Space: capacity = 1414529024 (1349.0MB) used = 496589416 (473.5845718383789MB) free = 917939608 (875.4154281616211MB) 35.10634335347508% usedFrom Space: capacity = 8388608 (8.0MB) used = 5778216 (5.510536193847656MB) free = 2610392 (2.4894638061523438MB) 68.8817024230957% usedTo Space: capacity = 8388608 (8.0MB) used = 0 (0.0MB) free = 8388608 (8.0MB) 0.0% usedPS Old Generation capacity = 2863661056 (2731.0MB) used = 125500192 (119.68630981445312MB) free = 2738160864 (2611.313690185547MB) 4.382508598112527% used40327 interned Strings occupying 4296152 bytes. JVM收集器设置1234–XX:+UseSerialGC–XX:+UseParallelGC–XX:+UseParallelOldGC–XX:+UseConcMarkSweepGC Serial Collector 大部分平台或者加java -client参数 young generation算法 = serial old generation算法 = serial (mark-sweep-compact) 缺点：串行，stop-the-world，速度慢，服务器不推荐使用 Parallel Collector linux x64平台，或加java -server参数 young = parallel，多个thread同时copy old = mark-sweep-compact = 1 优点：新生代回收更快，提高了吞吐量throughput（cpu用于非gc时间） 缺点：运行在8G/16G server上，old generation live obeject太多，pause time 过长（堆内存大-&gt;老年代大-&gt;能放的存活对象多） Parallel Compact Collector（ParallelOld） young = parallel = 2 old = parallel，分成多个独立单元，如果单元中存活对象少就回收，多则跳过 优点：old generation性能较2有提高 缺点：【大部分server系统old generation内存占用会达到60%-80%, 没有那么多理想的单元live object很少方便迅速回收，同时compact方面开销比起parallel并没明显减少。】 Concurrent Mark-Sweep(CMS) Collector young = parallel = 2 old = cms 【同时不做compact操作】 优点：pause time 会降低，pause敏感但cpu有空闲的场景建议使用4 缺点：cpu占用过多，cpu密集型服务不适合使用。另外碎片多，object的存储要通过链表连续跳n个地方，空间浪费问题也会增大。 参考 JVM堆内存和非堆内存 JVM参数设置","tags":[]},{"title":"SpringMVC请求处理流程","date":"2020-04-14T16:08:43.000Z","path":"2020/04/15/SpringMVC请求处理流程/","text":"小结 getHandler取到一个HandlerExecutionChain mappedHandler，包含URL对应的controller方法HandlerMethod，和一些interceptors HandlerMethod取到对应的handlerAdapter，数据绑定就再这个ha中做的 mappedHandler执行拦截器的preHandle handlerAdapter执行controller方法，包含请求前的数据绑定（数据转换），和请求后的数据转换 mappedHandler执行拦截器的postHandle 以上过程如果有抛出异常，由全局异常处理器来处理 mappedHandler触发拦截器的afterCompletion getHandler HandlerMapping是用来找Handler的 第一个是RequestMappingHandlerMapping，找到的Handler是HandlerMethod，也就是URL对应的controller的方法 寻找interceptorList（一系列拦截器，包括自定义的业务拦截器，系统预定义的一些拦截器？），与HandlerMethod一起分装在HandlerExecutionChain里 根据Handler获取HandlerAdapte","tags":[{"name":"SpringMVC","slug":"SpringMVC","permalink":"https://northernw.github.io/tags/SpringMVC/"},{"name":"web","slug":"web","permalink":"https://northernw.github.io/tags/web/"}]},{"title":"知识点汇总","date":"2020-04-07T21:15:01.000Z","path":"2020/04/08/知识点汇总/","text":"先是聊项目，从项目的架构设计到部署流程。 Java容器有哪些？哪些是同步容器,哪些是并发容器？ 容器分两个大类，Collection和Map。Collection又分List、Set、Queue、Vector几个大类，Map有HashMap、TreeMap、LinkedHashMap、HashTable，其中，Vector、HashTable是同步容器。 并发容器一般在juc包下，有ConcurrentHashMap、CopyOnWriteArrayList等。 ps: List: ArrayList、LinkedList Set: HashSet、LinkedHashSet、TreeSet Queue: LinkedList、PriorityQueue 引申：几个容器的主要方法的操作流程，容器体系结构 ArrayList和LinkedList的插入和访问的时间复杂度？ ArrayList：插入O(n) 访问O(1) LinkedList：插入O(1) 访问O(n) Java反射原理， 注解原理？ 反射原理：在运行状态下，对于任何一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能调用它的任意方法和属性，并能改变它的属性。总结来说，反射把Java类中的各个成分映射成为一个个Java对象，并且可以进行操作。 注解原理：注解的本质是一个继承了Annotation接口的接口。 解析一个类或者方法的注解有两种形式，一是编译期扫描，如@Override，编译器会检查方法是否真的重写了父类的某个方法；二是运行期发射，虚拟机规范定义了一系列和注解相关的属性表，字段、属性或类上有注解时（被注解修饰了），会写相应信息进字节码文件，Class类中提供了一些接口用于获取注解或判断是否被某个注解修饰。 延伸阅读：JAVA 注解的基本原理 ps: Java类执行的过程/类加载过程（2-6）/类的生命周期（2-8） – tbc 更准确的说法 编译：Java文件编译成.class字节码文件 加载：类加载器通过全限定名，将字节码加载进JVM，存储在方法区，将其转换为一个与目标类型对应的Class对象实例 验证：格式（.class文件规范）验证和语义（final不能继承等）验证？ 准备：静态变量赋初值与内存空间，final修饰的内存空间直接赋原值（？），不是开发人员赋的初值 解析：符号引用转换为直接引用，分配地址（?） 初始化：先初始化父类，再初始化自身；静态变量赋值，静态代码块执行。 使用 卸载 新生代分为几个区？使用什么算法进行垃圾回收？为什么使用这个算法？ 新生代有三个区，一个较大的Eden区，两个小的Survivor区。 使用复制算法。（也有标记过程，标记-复制） 一方面，针对算法本身，相对于标记-清除算法，不会有内存碎片的问题；相对于标记-整理算法，处理效率高很多（在整理时，还未进行对象清理，移动存活对象时需要将存活对象插入到待清理对象之前，有大量的移动操作，时间复杂度很高）。 复制算法主要问题在于内存利用率，而HotSpot的Eden和Survivor的默认比例是8:1，保证内存利用率达到了90%，所以影响也不是太大。 另一方面，新生代minor gc比较频繁，对gc效率有比较高的要求；对象生命周期比较短，小的survivor空间即可容纳大部分情况下的存活对象。 引申：jvm的几个知识点，算法，判断对象存活，GC roots有哪些，内存分配与回收策略，类加载机制 HashMap在什么情况下会扩容，或者有哪些操作会导致扩容？ java8中 放入新值（putValue–put/putMapEntries）后，元素个数size大于阈值threshold，会触发扩容。 链表树化时，如果表长table.length小于64，会用扩容代替树化。 put值前，如果表长为0，会触发扩容 HashMap put方法的执行过程？【见46】 如果table为空，或长度为0，初始化。默认loadFactor为0.75，默认capacity为16（capacity是table的长度），threshold一般为capacity*loadFactor。 通过hash定位槽，如果槽为空，构造新节点赋值给槽 若槽不为空，则在槽的链表或树中找到key相同的节点，替换节点值为新值；或是没有key相同的节点，就在树中或链表尾部加入新节点；若链表加入新节点后长度达到8（槽不算，aka槽下原有7个节点），则进行红黑树转化 如果是新加入节点，modCount、元素个数size自增1，如果元素个数超过阈值，则进行扩容 7.1 Java8扩容的执行过程？ 1. 计算新容量newCap和新阈值newThr（ps: 当容量已到最大值时，不再扩容；2倍扩容；） 2. 创建新的数组，赋值给table 3. 将键值对重新映射到新数组上 1. 如果无链表，直接根据`hash&amp;(newCap-1)`定位 2. 如果是树节点，委托红黑树来拆分和重新映射 3. 为链表，根据`hash&amp;oldCap`的值分成0、1两组，映射到j和j+oldCap（0低位，1高位）（**链表顺序不变**） HashMap检测到hash冲突后，将元素插入在链表的末尾还是开头？ Java8是加载链表末尾 Java7是开头 头插法会改变链表中元素原本的顺序，在并发情况下可能会产生链表成环的问题。 Java7到Java8的改变HashMap为何从头插入改为尾插入 java7的问题老生常谈，HashMap的死循环 1.8还采用了红黑树，讲讲红黑树的特性，为什么人家一定要用红黑树而不是AVL、B树之类的？ 插入、删除、查找的最坏时间复杂度都为 O(logn)。 红黑树特性： 每个节点要么是黑色，要么是红色 根节点是黑色的 每个叶节点是黑色的（Java实现中，叶子节点是null，遍历时看不到黑色的叶子节点，反而每个叶子节点是红色的） 如果一个节点是红色的，那么它的两个子节点是黑色的（意味着可以有连续的黑色节点，但不能有连续的红色节点。若给定N个黑色节点，最短路径情况是连续N个黑色，树高为N-1；最长路径情况是红黑相间，树高为2N-2） 对任一节点，从节点到它每个叶子节点的路径包含相同数量的黑色节点（最主要特性，插入、删除要调整以遵守这个规则） 面试旧敌之红黑树（直白介绍深入理解） 为什么用红黑树？ 红黑树的统计性能（理解为增删查平均性能）优于AVL树。 AVL：名字来源发明者G. M. Adelson-Velsky和E. M. Landis。本质是平衡二叉搜索树（查找树），任何节点的左右子树高度差不超过1，是高度平衡的二叉查找树。 B树：重温数据结构：理解 B 树、B+ 树特点及使用场景 平衡二叉树节点最多有两个子树，而 B 树每个节点可以有多个子树，M 阶 B 树表示该树每个节点最多有 M 个子树 AVL树高度平衡，查找效率高，但维护这个平衡的成本比较大，插入、删除要做的调整比较耗时。 红黑树的插入、删除、查找各种操作的性能比较平衡。 B树和B+树多用于数据存储在磁盘上的场景，比较矮胖，一次读取较多数据，减少IO。节点内是有序列表。列表的插入、删除成本比较高，如果是链表形式，则查找效率比较低（不能用二分查找提高查询效率）。 【自己的理解：B树节点内是有序列表，通过二分查找提高效率】 为什么STL和linux都使用红黑树作为平衡树的实现？ - Acjx的回答 - 知乎 https://www.zhihu.com/question/20545708/answer/58717264 https和http区别，有没有用过其他安全传输手段？ 区别： http明文传输，安全性低；HTTPS数据加密传输，安全性高 使用https协议需要到CA（Certificate Authority，数字证书认证机构）申请证书 http的响应速度比HTTPS快，因为HTTPS除了http三次握手的包，还要加上ssl的交互–具体是？ 端口不同，http80端口，https443端口 https本质是构建在ssl/tls之上的http协议 HTTP 与 HTTPS 的区别 其他安全传输手段：SSH SSH 协议原理、组成、认证方式和过程 延伸 https的特性：加密保证安全性防窃听、认证防伪装、完整性防篡改 加密方式：混合加密，用非对称加密传输对称秘钥，用对称秘钥进行要传输的数据的加解密 认证：使用证书来对通信双方认证。 完整性：ssl提供报文摘要功能来进行完整性保护。 http也可以通过md5验证完整性，但数据篡改后也可重新生成md5，因为是明文的。https是通过ssl的报文摘要来保证完整性的，结合了加密与认证，即使加密后数据被篡改，也很难再生成报文摘要，因为不知道明文是什么。 线程池的工作原理，几个重要参数，然后给了具体几个参数分析线程池会怎么做，最后问阻塞队列的作用是什么？ 线程池解决两个问题： 由于减少了每个任务的调度开销，通常在执行大量异步任务时提供优秀的性能。 提供了管理、调控资源的方式 Executors工厂方法： 1. newFixedThreadPool 固定size的线程池。为了满足资源管理的需求，需要限制当前线程数量的场景。适用于负载比较重的服务器。 1. corePoolSize == maximumPoolSize 2. keepAliveTimes = 0 3. LinkedBlockingQueue 队列大小Integer.MAX_VALUE，等价于无界 4. 当线程池中线程数达到corePoolSize后，新任务将在队列中等待 5. 由于使用无界队列，运行中的线程池不会拒绝任务 2. newSingleThreadExecotor 单个线程的线程池。需要保证顺序执行任务的场景，并且在任意时间点不会有多个线程是活动的。 1. corePoolSize = maximumPoolSize = 1 2. keepAliveTimes = 0 3. LinkedBlockingQueue 4. 如果当前线程池无线程，就创建一个线程来运行任务 5. 当线程数达到1后，新的任务都加入到队列中 3. newCachedThreadPool 大小无界的线程池（自动资源回收？），适用于有很多短期异步执行任务的小程序，或者是负载比较轻的服务器。 1. corePoolSize = 0, maximumPoolSize = Integer.MAX_VALUE 2. keepAliveTimes = 60s 3. SynchronousQueue 是一个没有容量的阻塞队列，一个插入操作必须等待另一个线程对应的移除操作 4. 提交任务时如果有空闲线程，就空闲线程取到这个任务执行；否则创建一个线程来执行任务 5. 适用于将主线程的任务传递给空闲线程执行 重要参数： 1. core and maximum pool sizes 1. corePoolSize 核心最大线程：新任务加入时，如果运行线程个数小于核心线程数，即使有其他工作线程是空闲的，也会创建新线程 -- 线程池预热 2. maximumPoolSize 线程池最大线程：阻塞队列满时，如果运行线程数小于maximumPoolSize，才可创建新线程运行任务 3. corePoolSize=maximumPoolSize时，等价于newFixedThreadPool 4. maximumPoolSize=本质上无限的数（比如Integer.MAX_VALUE），等价于newCachedThreadPool ？ 5. 一般只在构造时设置这两个参数，但也可以通过两个set方法改变 6. **这两个参数会自动调整么？** 2. On-demand construction 1. 默认情况下，只有任务提交时才会创建线程（包括核心线程） 2. 也可以通过prestartCoreThread或者prestartAllCoreTheads来预先创建线程。比如构建了一个阻塞队列不为空的线程池时，会想要这么做（预先创建线程）。 3. Creating new threads 1. 默认使用defaultThreadFactory来创建线程，相同的线程组ThreadGroup、优先级priority和非守护线程状态non-daemon status. 2. 也可以使用自定义的threadFactory，自定义线程名称、线程组、优先级等。 3. threadFactory创建线程失败的什么东西没看懂 4. Keep-alive times 1. keepAliveTime 如果线程数多于核心线程数，超过这个时间的空闲线程将会被停掉（指销毁掉？） 5. queuing 1. 入队规则 6. rejected tasks 四个拒绝策略 RejectedExecutionHandler 1. ThreadPoolExecutor.AbortPolicy 抛出RejectedExecutionException 2. CallerRunsPolicy 调用者自身来执行 3. DiscardPolicy 丢弃任务，任务不会被执行 4. DiscardOldestPolicy work queue的首个任务将会被丢弃，重试添加当前任务（可能再次失败，自旋执行） 7. hook methods 1. beforeExecute afterExecute 可用来设置运行环境，重新初始化本地线程，获取统计数据，添加日志。 2. terminated executor终止时提供的钩子方法 8. queue maintenance getQueue可用于监控和调试当前work queue，其他用途不建议。remove和purge可用于大量任务取消时候的存储清理。 9. reclamation （清除？）一个在程序中无引用、并且无剩余线程的线程池，即使无显式shutdown关闭，也可以被清除回收。可以通过这些方式设置线程池的线程在无使用时（最终）销毁：设置keep-alivet times；使用小的核心线程数比如0，或者设置allowCoreThreadTimeOut。 ScheduledThreadPoolExcutor 延迟运行命令，或周期执行命令 LinkedBlockingQueue和DelayQueue的实现原理 1. LinkedBlockingQueue 就是生产者消费者的实现 1. 应用了ReentrantLock（putLock &amp; tackLock）和lock的Condition（notEmpty &amp; notFull） 2. DelayQueue 1. 应用了PriorityQueue，时间小的在队头 2. ReentrantLock（lock）和Condition（available） FutureTask是用AQS实现的 get=acquireShared，run/cancel后=release Linux怎么查看系统负载情况？ uptime w top 查看linux系统负载情况 请详细描述springmvc处理请求全流程？ 通用的流程： 客户端提交请求到DispatcherServlet DispatcherServlet寻找Handler（HandlerExecutionChain）(包括handler , common interceptors和MappedInterceptor) DispatcherServlet调用controller controller调用业务逻辑，返回ModelAndView DispatcherServlet寻找ViewResolver，找到对应视图 渲染视图显示到客户端 2. restful的一些细节（上述2、3、4过程的细化，restful的mav一般是空的）： 1. getHandler取到一个HandlerExecutionChain mappedHandler，包含URL对应的controller方法HandlerMethod，和一些interceptors 2. HandlerMethod取到对应的handlerAdapter，数据绑定就再这个ha中做的 3. mappedHandler执行拦截器的preHandle 4. handlerAdapter执行controller方法，包含请求前的数据绑定（数据转换），和请求后的数据转换（转换后将数据按需要的格式写入response） 5. mappedHandler执行拦截器的postHandle 6. 以上过程如果有抛出异常，由全局异常处理器来处理 7. mappedHandler触发拦截器的afterCompletion 8. 讲一讲AtomicInteger，为什么要用CAS而不是synchronized？ 查询中哪些情况不会使用索引？ 使用or like以”%xx”开始匹配 联合（复合）索引，不符合最左匹配 索引列数据类型隐形转换，比如列是字符串，但用数值来查询就用不上索引 在where子句中，对索引列有数学运算、或者使用函数，用不了索引 MySQL估计全表扫描比查询索引快时（比如数据量非常少） [MYSQL 索引类型、什么情况下用不上索引、什么情况下不推荐使用索引](https://blog.csdn.net/kaka1121/article/details/53395628) [MySQL性能优化的最佳21条经验](https://www.cnblogs.com/hongfei/archive/2012/10/19/2731342.html) -- 没大用 [mysql explain执行计划详解](https://blog.csdn.net/kaka1121/article/details/53394426) -- 有错字之类的 type: const 命中唯一索引或主键的时候 数据库索引，底层是怎样实现的，为什么要用B+树索引？ MySQL底层使用B+树实现的。 MyISAM引擎，B+树主索引、辅助索引叶节点是数据记录的地址，称为非聚集索引（与InnoDB区分） InnoDB的主键索引是聚集索引，叶节点存的完整的数据记录；辅助索引，叶节点存的是主键的值。 为什么用B+树索引？ 1. 数据文件比较大，一般存储在磁盘上 2. 索引的组织结构要尽量减少查找过程中磁盘IO次数。 3. 数据库系统利用磁盘预读原理，将一个节点的大小设为一个页的大小，则只需要一次IO就可以将一个节点的数据都读入 B+树只有叶子节点存放数据，非叶子节点作为索引，这样树出度大，树高小，一般3层，查询目标数据的io次数比较少，效率高。 使用节点大小正好等于磁盘一页大小的B+树，可以减少io操作次数，提高查询效率。 从数组、哈希表、二叉树等数据结构的对比来回答，见下面这篇文章 [MySQL为什么不用数组、哈希表、二叉树等数据结构作为索引呢](https://juejin.im/post/5e920646e51d4546f5790713) [orderby底层执行过程](https://juejin.im/post/5e945b9651882573b7537c2a) Mysql主从同步的实现原理？ 原理：在主库上记录二进制日志，在备库重放日志的方式实现异步数据复制。 复制有三个步骤： 主库记录二进制日志，每次准备提交事务（完成数据库更新）前先记录二进制日志（记录日志完后，再执行数据库更新） 备库将主库的二进制文件复制到本地的中继日志中。 备库会启动一个工作线程，称为IO工作线程，负责和主库建立一个普通的客户端连接 如果该进程追赶上了主库，它将进入睡眠状态，直到主库有新的事件产生，会被唤醒，将接收到的事件记录到中继日志中 备库的SQL线程读取中继日志并在备库执行 中继日志一般在系统缓存中，开销低，也可以根据配置选项来决定是否写入自己的二进制日志中 常见复制架构： 1. 一主多从 2. 主主 3. 环型复制 ![wKioL1YKPpuzRxapAAKMyc9n3zQ218.jpg](../../image/wKioL1YKPpuzRxapAAKMyc9n3zQ218.jpg) [MySQL复制详解](https://blog.51cto.com/amyhehe/1699168) MySQL是怎么用B+树？ innodb引擎用B+树当索引，索引文件同时是数据文件。聚集索引，也就是主键索引，叶节点存储的完整行数据；辅助索引，也称为非聚集索引，叶节点存对应行记录的主键。 MyISAM引擎也是用B+树当索引，为非聚集索引，索引不是数据文件，叶节点存的是行记录的地址。 谈谈数据库乐观锁与悲观锁？ 悲观锁，认为操作会发生冲突，提前加锁，直到自己操作结束再释放锁。 MySQL的显式锁定 写锁 select .. for update &amp; 读锁 select .. lock in share mode 乐观锁，认为不会发生冲突，在提交更新的时候会判断一下期间数据有没有被修改。类似于CAS操作，常用方式有版本号、时间戳。 有使用过哪些NoSQL数据库？MongoDB和Redis适用哪些场景？ 工程中用过Redis，主要是小部分数据的缓存 其他不太了解 NoSql not only sql 非关系型数据库 [memcache、redis、mongoDB 如何选择？](https://zhuanlan.zhihu.com/p/32940868) 描述分布式事务之TCC服务设计？ 不了解 TCC分布式事务 try - confirm - cancel Redis和memcache有什么区别？Redis为什么比memcache有优势？ 同19 考虑redis的时候，有没有考虑容量？大概数据量会有多少？ 没有，公司维护的Redis组件 – redis &amp; nosql 需要再深入一点呀 谈谈分布式锁、以及分布式全局唯一ID的实现比较？ 分布式锁 为什么需要分布式锁？ 效率：避免不同节点重复相同的工作，这些工作会浪费资源。比如针对一个操作发多封邮件。 正确性：避免破坏正确性的发生，比如多个节点操作了同一条数据，其中一个操作结果被覆盖了，造成数据丢失。 常见实现方式 数据库 表的一行数据表示一个资源，select..for update来加锁，可同时存节点信息，支持重入。 理解简单，但需要自己实现，以及维护超时、事务和异常处理等，性能局限于数据库，性能相对比较低，不适合高并发场景。 zookeeper curator封装 – 具体怎么用还没看过呢 Zk 排他锁和共享锁有区别。排他锁，利用zk有序节点，序号最小的节点表示获取到锁，其他未获取到锁的节点监听自己的前一个节点。 （共享锁，能获取到资源都算？回家再看看。）还有个读写锁，一个节点获取读锁，只要序号小于他的都为读锁，就表示获取到读锁；一个节点获取写锁，需要自己的序号最小，才表示获取到写锁。可重入锁之类的，zk节点写值吧，原理和Java的reentrantLock类似，获取多少次，state自加多少次，解锁再一次次自减，直到state为0表示完全释放。 （文章里说zk分布式锁和数据库mysql差不多。。真的么） redis setNX 自己参照一篇文章实现的比较简单，主要利用setNX，不支持重入，非公平。有超时释放，有加锁身份，解锁原子性 下面的文章里介绍的Redission。不太了解，加锁原子性lua脚本，用了hset，hashmap的结构，key是资源，value是锁定次数，可重入。。还有公平锁的实现。。。 分布式锁 分布式全局唯一ID/分布式ID生成器 实现方式 1. uuid 1. 缺点：长，占用空间大，间接导致数据库性能下降 2. 缺点：不是有序的，导致索引在写的时候有过多的随机写 2. 数据库自增主键 1. 缺点：完全依赖于数据库，有性能瓶颈 2. 缺点：不易扩展 3. snowflake Twitter，Scala实现的... 1. 雪花算法，带有时间戳的全局唯一ID生成算法 2. 固定ID格式： 12341位的时间戳（精确到毫秒，41位的长度可使用69年）10位的机器ID（10位长度最多支持1024个服务器节点部署）12位的计数序列号（12位支持每节点每毫秒最多生成4096个序列号） 集群监控的时候，重点需要关注哪些技术指标？这些指标如何优化？ 系统指标：cpu使用率、内存使用率、机器连通性、系统负载（cpu.load） 网络指标：网络流入速度、网络流出速度、网络流入包数、网络流出包数、TCP连接数、TCP Active Opens（主动打开数）、IP接收包数、IP丢包数、TCP接收包数、TCP发送包数、TCP包传输错误数、TCP重传数 磁盘指标：磁盘使用率、iNode使用率、磁盘繁忙占比、磁盘读速度、磁盘写速度、磁盘读次数、磁盘写次数 容器指标？：线程数[容器指标]/平均到每核、进程数[容器指标]/平均到每核…. 监控知识体系 小米监控 从千万的数据到亿级的数据，会面临哪些技术挑战？你的技术解决思路？ https://zhuanlan.zhihu.com/p/51081227 最常见的数据库，如MySql、Oracle等，都采用行式存储，比较适合OLTP。如果用MySql等行数据库来实现OLAP，一般都会碰到两个瓶颈： 数据量瓶颈：mysql比较适合的数据量级是百万级，再多的话，查询和写入性能会明显下降。因此，一般会采用分库分表的方式，把数据规模控制在百万级。 查询效率瓶颈：mysql对于常用的条件查询，需要单独建立索引或组合索引。非索引字段的查询需要扫描全表，性能下降明显。 分表 垂直业务拆分=分库+微服务（可分库基础上再分表） https://zhuanlan.zhihu.com/p/54594681 最近两年遇到的最大的挫折，从挫折中学到了什么？ 胃炎？失眠？ 生病要看病，不要自己琢磨；看病也是要厚脸皮的，多问；运动，保持良好生活习惯。 – 这么回答..太坑了吧 最近有没有学习过新技术？ zookeeper kafka json:Gson/jackson/fastjson 设计模式：尽量在工作中合适地应用 docker简单使用 简单说一下面向对象的特征以及六大原则 特征： 封装：把客观事物封装成抽象的类 抽象继承！：（实现继承or接口继承）让某个类型的对象获得另一个类型的对象的属性和方法 多态：一个类实例的相同方法在不同情形有不同的表现形式。（编译多态与运行时多态）一般指运行时多态..? 多态存在的必要条件：继承、重写、父类引用指向子类对象 原则： 单一职责：一个类的功能要单一，不能包罗万象 开放封闭：一个模块，在扩展性方面应该是开发的，在更改性方面是封闭的 里氏替换：子类应当可以替换父类，并出现在父类能够出现的任何地方 依赖倒置：高层次的模块不应该依赖于低层次的模块，他们都应该依赖于抽象；抽象不应该依赖于具体实现，具体实现应该依赖于抽象。 接口分离：模块间要通过抽象接口隔离开，而不是通过具体的类强耦合起来，即面向接口编程。 迪米特原则：一个类对自己依赖的类知道的越少越好。类间解耦，低耦合、高内聚。 谈谈final、finally、finalize的区别 final 修饰类、方法或变量 修饰类：表明类不能被继承 方法：禁止在子类中被覆盖（private方法会隐式被指定为final） 变量： 基本数据类型的变量：数值在初始化后不能更改 引用类型的变量：初始化后不能再指向另一个对象（指向的地址不可变） finally： 一般与try catch一起使用，无论程序抛出异常或正常执行，finally块的内容一定会被执行。 最常用的地方：通过try-catch-finally来进行类似资源释放、保证解锁等动作。 finalize Object的protected方法，子类可以覆盖该方法以实现资源清理工作，GC在回收对象之前调用该方法。 日常开发中基本不用，也不推荐使用。Java9中被标记为deprecated! -- 不想多说 https://juejin.im/post/5b9bb81ef265da0ac2565a0f Java中==、equals与hashCode的区别和联系 https://juejin.im/entry/59b3897b5188257e733c24eb – 后面写的比较乱 https://juejin.im/post/5a4379d4f265da432003874c – equals与hashCode Java数据类型 8种基本数据类型 （整型）数值类型 byte short int long 1.2.4.8 （浮点）数值类型 float double 4.8 字符型 char 2 存储 Unicode 码，用单引号赋值。 布尔类型 boolean 1 3种引用类型：类、接口、数组 == 比较两个数据是否相等，基本类型比较数值是否相等，引用类型比较地址是否相等。 equals()方法 Object类型定义的，比较二者== 1234//object的equals方法public boolean equals(Object obj) &#123; return (this == obj);&#125; 想自定义对象逻辑“相等”（值相等、或内容相等）的含义时，重写equals方法。 重写equals准则： &gt; **自反性**：对于任何非空引用值 x，x.equals(x) 都应返回 true。 &gt; **对称性**：对于任何非空引用值 x 和 y，当且仅当 y.equals(x) 返回 true 时，x.equals(y) 才应返回 true。 &gt; **传递性**：对于任何非空引用值 x、y 和 z，如果 x.equals(y) 返回 true， 并且 y.equals(z) 返回 true，那么 x.equals(z) 应返回 true。 &gt; **一致性**：对于任何非空引用值 x 和 y，多次调用 x.equals(y) 始终返回 true 或始终返回 false， 前提是对象上 equals 比较中所用的信息没有被修改。 &gt; **非空性**：对于任何非空引用值 x，x.equals(null) 都应返回 false。 一般只判断同类型的对象 主要不要违反了对称性、传递性 hashCode()方法 1public native int hashCode(); equals的对象hashcode一定相等，hashcode相同的对象不一定equals 为什么对象的hashcode会相同？ hashcode的实现取决于jvm，比较典型的一种是基于内存地址进行哈希计算，也有基于伪随机的实现。 哈希计算会存在哈希碰撞。 https://juejin.im/entry/597937cdf265da3e114cd300 谈谈Java容器ArrayList、LinkedList、HashMap、HashSet的理解，以及应用场景 | | ArrayList | LinkedList | HashMap | HashSet || ————– | —————– | —————– | ———– | —————– || 数据结构 | （可变）数组 | （双向）链表 | 数组+红黑树 | 底层实现是HashMap || 插入时间复杂度 | o(n) | o(1) | | || 删除时间复杂度 | o(n) | o(1) | | || 访问时间复杂度 | o(1) 支持随机访问 | o(n) 不支持随机.. | | || 应用场景 | 经常访问 | 经常修改 | 映射..？ | 去重 | 谈谈线程的基本状态，其中的wait() sleep() yield()方法的区别。 线程的基本状态 新建、运行（运行中、就绪）、等待、超时等待、阻塞、终止 wait() Object的方法，在某个对象上等待，等待这个对象将它唤醒，释放锁。运行-&gt;等待/超时等待 sleep() Thread的静态方法，当前线程睡眠，不释放锁。运行-&gt;超时等待 yield() Thread的方法，让出当前cpu。还是运行这个大状态，从运行中变成就绪状态。 JVM性能调优的监控工具了解那些？ jps jstack jmap jps [option] 输出Java进程信息 123jps -ml111957 org.apache.catalina.startup.Bootstrap -config /export/Domains/testenv.jd.local/server1/conf/server.xml start136044 sun.tools.jps.Jps -ml jstack [option] pid 输出某个进行内的线程栈信息 123jstack 111957 | grep 1b6d0\"System_Clock\" #307 daemon prio=5 os_prio=0 tid=0x00007f71b53f3800 nid=0x1b6d0 runnable [0x00007f72606d9000] 12-l long listings，会打印出额外的锁信息，在发生死锁时可以用&lt;strong&gt;jstack -l pid&lt;/strong&gt;来观察锁持有情况 -m mixed mode，不仅会输出Java堆栈信息，还会输出C/C++堆栈信息（比如Native方法） jmap [option] pid 输出某个进程内的堆信息：JVM版本、使用的GC算法、堆配置、堆内存使用情况 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647jmap -heap 111957Attaching to process ID 111957, please wait...Debugger attached successfully.Server compiler detected.JVM version is 25.20-b23using thread-local object allocation.Parallel GC with 43 thread(s)Heap Configuration: MinHeapFreeRatio = 0 MaxHeapFreeRatio = 100 MaxHeapSize = 2147483648 (2048.0MB) NewSize = 357564416 (341.0MB) MaxNewSize = 715653120 (682.5MB) OldSize = 716177408 (683.0MB) NewRatio = 2 SurvivorRatio = 8 MetaspaceSize = 21807104 (20.796875MB) CompressedClassSpaceSize = 1073741824 (1024.0MB) MaxMetaspaceSize = 17592186044415 MB G1HeapRegionSize = 0 (0.0MB)Heap Usage:PS Young GenerationEden Space: capacity = 353370112 (337.0MB) used = 28186432 (26.88067626953125MB) free = 325183680 (310.11932373046875MB) 7.976461801047849% usedFrom Space: capacity = 2097152 (2.0MB) used = 1736768 (1.65631103515625MB) free = 360384 (0.34368896484375MB) 82.8155517578125% usedTo Space: capacity = 2097152 (2.0MB) used = 0 (0.0MB) free = 2097152 (2.0MB) 0.0% usedPS Old Generation capacity = 869793792 (829.5MB) used = 160875768 (153.42308807373047MB) free = 708918024 (676.0769119262695MB) 18.495851485681793% used36932 interned Strings occupying 3347024 bytes. 输出堆内存中对象个数、大小统计直方图 1jmap -histo:live 111957 | less 123456789B byte C char D double F float I int J long Z boolean [ 数组，如[I表示int[] [L+类名 其他对象 dump出堆信息，再使用jhat或其他工具分析 123jmap -dump:format=b,file=dump.dat 111957jhat -port 8888 dump.dat# 浏览器输入 ip:port可访问 jstat [ generalOption | outputOptions vmid [interval[s|ms] [count]] ] jvm统计信息 vmid是Java虚拟机ID，在Linux/Unix系统上一般就是进程ID。interval是采样时间间隔。count是采样数目。 123456789101112131415jstat -gc 111957 250 6 # gc信息 S0C S1C S0U S1U EC EU OC OU MC MU CCSC CCSU YGC YGCT FGC FGCT GCT2048.0 2048.0 0.0 0.0 345088.0 148448.5 1397248.0 119834.5 80640.0 78303.8 8448.0 7919.0 133 8.230 5 11.160 19.3902048.0 2048.0 0.0 0.0 345088.0 148457.6 1397248.0 119834.5 80640.0 78303.8 8448.0 7919.0 133 8.230 5 11.160 19.3902048.0 2048.0 0.0 0.0 345088.0 148457.6 1397248.0 119834.5 80640.0 78303.8 8448.0 7919.0 133 8.230 5 11.160 19.3902048.0 2048.0 0.0 0.0 345088.0 150425.8 1397248.0 119834.5 80640.0 78303.8 8448.0 7919.0 133 8.230 5 11.160 19.3902048.0 2048.0 0.0 0.0 345088.0 150425.8 1397248.0 119834.5 80640.0 78303.8 8448.0 7919.0 133 8.230 5 11.160 19.3902048.0 2048.0 0.0 0.0 345088.0 150427.8 1397248.0 119834.5 80640.0 78303.8 8448.0 7919.0 133 8.230 5 11.160 19.390 1234567S0C、S1C、S0U、S1U：Survivor 0/1区容量（Capacity）和使用量（Used） EC、EU：Eden区容量和使用量 OC、OU：年老代容量和使用量 PC、PU：永久代容量和使用量 YGC、YGT：年轻代GC次数和GC耗时 FGC、FGCT：Full GC次数和Full GC耗时 GCT：GC总耗时 简单谈谈JVM内存模型，以及volatile关键字 运行时数据区域包括堆、方法区（包括运行时常量池）、Java虚拟机栈、本地方法栈、程序计数器、直接内存。 堆：所有对象在这里分配内存【所有线程共享】 方法区：存放已被加载的类信息、常量、静态变量、即时编译器编译后的代码等信息【所有线程共享】 Java虚拟机栈：生命周期与线程相同，描述的是Java方法执行时候的内存模型，每个方法被执行的时候都会创建一个栈帧，存储局部变量表、操作数栈、常量池引用（动态链接）、方法出口等信息。【线程私有】 本地方法栈：与虚拟机栈类似，只不过方法是本地方法【线程私有】 程序计数器：记录正在执行的虚拟机字节码指令的地址（如果是本地方法则为空）【线程私有】 直接内存：JDK1.4引入NIO，可以使用native函数库分配堆外内存，然后通过堆内的DirectByteBuffer作为这部分内存的引用、进行操作。可以提高性能，避免堆外内存和堆内内存的来回拷贝。 **Java内存模型 JMM** **Java memory model** 用来屏蔽不同硬件和操作系统的内存访问差异，实现Java在各平台上一致的内存访问效果。 JMM规定，所有变量都存在主内存中（类似于操作系统的普通内存）；每个线程有自己的工作内存（=CPU的寄存器或高速缓存），保存了该线程使用的变量的主内存副本拷贝。线程只能操作工作内存。 存在缓存不一致问题。 &lt;img src=&quot;../../image/image-20200514161253557.png&quot; alt=&quot;image-20200514161253557&quot; style=&quot;zoom:30%;&quot; /&gt; **主内存与工作内存交互操作** &lt;img src=&quot;../../image/image-20200514163326711.png&quot; alt=&quot;image-20200514163326711&quot; style=&quot;zoom:50%;&quot; /&gt; **内存模型三大特性** 1. 原子性：上述8个操作是原子的（double&amp;long等64位变量的操作，JVM允许非原子），一系列操作合起来其实是非原子的 2. 可见性：指一个线程修改了共享变量的值，其他线程可以立即得知这个修改。JMM是通过变量修改后将新值同步到主内存（并使其他工作内存中的这个变量副本无效）、在变量读取前从主内存刷新变量值来实现的。 3. 有序性：从本线程来看，所有操作都是有序的；从线程外看，操作是无序的，因为发生了指令重排。JMM允许编译器和处理器进程指令重排，重排不会影响到单线程的执行结果，但会影响多线程的执行正确性。 volatile关键字通过添加内存屏障的方式来禁止指令重排（重排序时不能把屏障后的指令重排到屏障前） **先行发生原则** 1. 单一线程原则：在一个线程内，在程序前面的操作先行发生于后面的操作。 2. 管程锁定原则：一个 unlock 操作先行发生于后面对同一个锁的 lock 操作。 3. volatile变量规则：对一个 volatile 变量的写操作先行发生于后面对这个变量的读操作。 4. 线程启动规则：Thread 对象的 start() 方法调用先行发生于此线程的每一个动作。 5. 线程加入规则：Thread 对象的结束先行发生于 join() 方法返回。 6. 线程中断规则：对线程 interrupt() 方法的调用先行发生于被中断线程的代码检测到中断事件的发生。 7. 对象终结规则：一个对象的初始化完成(构造函数执行结束)先行发生于它的 finalize() 方法的开始。 8. 传递性：如果操作 A 先行发生于操作 B，操作 B 先行发生于操作 C，那么操作 A 先行发生于操作 C。 [volatile关键字](https://juejin.im/post/5a2b53b7f265da432a7b821c#heading-0) **volatile关键字** 1. 保证了不同线程对该变量操作的内存可见性 2. 禁止指令重排序，保证（volatile读写）有序性 垃圾收集器与内存分配策略【祥见JVM的几个大知识点】 垃圾收集器 内存分配策略 Minor GC 和 Full GC Minor GC:回收新生代，因为新生代对象存活时间很短，因此 Minor GC 会频繁执行，执行的速度一般也会比 较快。 Full GC:回收老年代和新生代，老年代对象其存活时间长，因此 Full GC 很少执行，执行速度会比 Minor GC 慢很多。 分配策略 1. 对象优先在 Eden 分配 大多数情况下，对象在新生代 Eden 上分配，当 Eden 空间不够时，发起 Minor GC。 2. 大对象直接进入老年代 大对象是指需要连续内存空间的对象，最典型的大对象是那种很长的字符串以及数组。 经常出现大对象会提前触发垃圾收集以获取足够的连续空间分配给大对象。 -XX:PretenureSizeThreshold，大于此值的对象直接在老年代分配，避免在 Eden 和 Survivor 之间的大量内存复制。 3. 长期存活的对象进入老年代 为对象定义年龄计数器，对象在 Eden 出生并经过 Minor GC 依然存活，将移动到 Survivor 中，年龄就增加 1 岁， 增加到一定年龄则移动到老年代中。 -XX:MaxTenuringThreshold 用来定义年龄的阈值。 4. 动态对象年龄判定 虚拟机并不是永远要求对象的年龄必须达到 MaxTenuringThreshold 才能晋升老年代，如果在 Survivor 中相同年龄 所有对象大小的总和大于 Survivor 空间的一半，则年龄大于或等于该年龄的对象可以直接进入老年代，无需等到 MaxTenuringThreshold 中要求的年龄。 5. 空间分配担保 在发生 Minor GC 之前，虚拟机先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果条件成立的话，那么 Minor GC 可以确认是安全的。 如果不成立的话虚拟机会查看 HandlePromotionFailure 的值是否允许担保失败，如果允许那么就会继续检查老年代 最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次 Minor GC;如果小 于，或者 HandlePromotionFailure 的值不允许冒险，那么就要进行一次 Full GC。 **Full GC 的触发条件** 对于 Minor GC，其触发条件非常简单，当 Eden 空间满时，就将触发一次 Minor GC。而 Full GC 则相对复杂，有以下条件: 1. 调用 System.gc() 只是建议虚拟机执行 Full GC，但是虚拟机不一定真正去执行。不建议使用这种方式，而是让虚拟机管理内存。 2. 老年代空间不足 老年代空间不足的常见场景为前文所讲的大对象直接进入老年代、长期存活的对象进入老年代等。 为了避免以上原因引起的 Full GC，应当尽量不要创建过大的对象以及数组。除此之外，可以通过 -Xmn 虚拟机参数 调大新生代的大小，让对象尽量在新生代被回收掉，不进入老年代。还可以通过 -XX:MaxTenuringThreshold 调大对 象进入老年代的年龄，让对象在新生代多存活一段时间。 3. 空间分配担保失败 使用复制算法的 Minor GC 需要老年代的内存空间作担保，如果担保失败会执行一次 Full GC。 4. JDK 1.7 及以前的永久代空间不足 在 JDK 1.7 及以前，HotSpot 虚拟机中的方法区是用永久代实现的，永久代中存放的为一些 Class 的信息、常量、静 态变量等数据。 当系统中要加载的类、反射的类和调用的方法较多时，永久代可能会被占满，在未配置为采用 CMS GC 的情况下也 会执行 Full GC。如果经过 Full GC 仍然回收不了，那么虚拟机会抛出 java.lang.OutOfMemoryError。 为避免以上原因引起的 Full GC，可采用的方法为增大永久代空间或转为使用 CMS GC。 5. Concurrent Mode Failure 执行 CMS GC 的过程中同时有对象要放入老年代，而此时老年代空间不足(可能是 GC 过程中浮动垃圾过多导致暂时 性的空间不足)，便会报 Concurrent Mode Failure 错误，并触发 Full GC。 垃圾收集算法【见Java虚拟机】 标记-清除 标记-整理 复制 分代收集 新生代：复制算法 老年代：标记-清除 or 标记整理 **虚拟机的几大问题** 1. 运行时数据区域 2. 垃圾收集 1. 对象可达 2. 引用类型 3. GC Roots 4. 算法 5. 收集器 3. 内存分配与回收策略（回收主要是老年代的触发条件） 4. 类加载机制 MySQL几种常用的存储引擎区别 InnoDB与MyISAM比较典型的几个区别： innodb支持事务、MVCC快照读、行级锁粒度、hash索引、聚集索引、支持外键 myisam支持全文索引、空间索引、数据压缩 innodb存储成本高、内存成本高、插入速度低，myisam反过来 来源：MySQL技术内幕 &lt;img src=&quot;../../image/image-20200515154754678.png&quot; alt=&quot;image-20200515154754678&quot; style=&quot;zoom:50%;&quot; /&gt; 数据库的隔离级别 未提交读 read uncommited 读提交 rc read commited 可重复读rr repeatable read 可串行化 serializable 5亿整数的大文件，怎么排？ 内存估算 假设每个数最多64位，8字节 5,0000,0000x8 ~ 500MBx8 = 4000MB ~ 4G 假设5亿数不重复 内存装的下： 直接快排，得算好久吧.. [5亿个整数排序](https://juejin.im/post/5e435f236fb9a07cbc268994) 内存装不下： 1. 读文件，数取模%4000存入4000个小文件，每个文件约1M 2. 读每个小文件，快排 3. 多路归并排序输出 [海量数据处理思路](https://juejin.im/entry/5a27cb796fb9a045104a5e8c) 1. 分治/hash映射 + hash统计 + 堆/快排/归并排序 1. hash分成n个小文件，满足内存要求：好处是，可以让相同的数或字符串进入同一个小文件 2. 小文件排序或统计，或没有本步骤，输出另一份小文件 3. 最终要求 1. 全排序：使用多路归并 2. 找top k：直接小顶堆（找最大top k）or大顶堆；或者每个小文件先找top k，再对比n个top k 3. 找两文件不同：两两小文件set对比 2. 数据结构 1. bitmap 可用于整数去重等 2. [trie树](https://juejin.im/post/5c2c096251882579717db3d2) 名字来源retrieval Java内存模型【重复】 full gc怎么触发？【见35】 gc算法【见36】 JVM回收策略【内存分配和回收策略，见Java虚拟机】 ​ ClassLoader原理和应用 ClassLoader的作用 加载class字节码文件到jvm 确认每个类应由那个类加载器加载，这也影响到两个类是否相等的判断，影响的方法有equals()、isAssignableFrom()、isInstance()以及instanceof关键字 加载的类存放在哪里？ jdk8之前在方法区，8之后在元数据区。 什么时候触发类加载？ 隐式加载 遇到new、getstatic、putstatic、invokestatic4条字节码指令时 对类进行反射调用时 当初始化一个类时，如果父类还没初始化，优先加载父类并初始化 虚拟机启动时，需指定一个包含main函数的主类，优先加载并初始化这个主类 显式加载 通过ClassLoader的loadClass方法 通过Class.forName 通过ClassLoader的findClass方法 有哪些类加载器ClassLoader？ Bootstrap ClassLoader：加载JVM自身工作需要的类，由JVM自己实现。加载JAVA_HOME/jre/lib下的文件 ExtClassLoader：是JVM的一部分，由sun.misc.Launcher$ExtClassLoader实现，会加载JAVA_HOME/jre/lib/ext下的文件，或由System.getProperty(&quot;java.ext.dirs&quot;)指定的目录下的文件 AppClassLoader：应用类加载器，由sun.misn.Launcher$AppClassLoader实现，加载System.getProperty(&quot;java.class.path&quot;)目录下的文件，也就是classpath路径。 双亲委派模型 原理：当一个类加载器收到类加载请求时，如果存在父类加载器，会先由父类加载器进行加载，当父类加载器找不到这个类时（根据类的全限定名称。找不到是由于，每个类有自己的加载路径。），当前类加载器才会尝试自己去加载。 为什么使用双亲委派模型？它可以解决什么问题？ 双亲委派模型能够保证类在内存中的唯一性。 假如没有双亲委派模型，用户自己写了个全限定名为java.lang.Object的类，并用自己的类加载器去加载，同时BootstrapClassLoader加载了rt.jar包中的jdk本身的java.lang.Object，这样内存中就存在两份Object类了，会出现很多问题，例如根据全限定名无法定位到具体的类。 高吞吐量的话用哪种gc算法 高吞吐量，如果指cpu多用于用户程序，需要停顿时间比较短的收集器，新生代在服务端一般用Parallel Scavenge，算法也是复制算法。 复制算法的性能比较高。 HashMap 查找 根据hash定位槽 在槽中查找给定key（hash相等、key相等），找到直接返回，否则最后返回null 若槽节点key相等，返回槽节点 若槽节点为树节点，委托给树查找 遍历链表查找 2. 遍历 从`index = 0, table[index]`开始，找到一个不为null的槽，遍历链表 3. 插入 1. 如果table为空，或长度为0，初始化。（默认loadFactor为0.75，默认capacity为16（capacity是table的长度），threshold一般为capacity*loadFactor。） 2. 通过hash定位槽，如果槽为空，构造新节点赋值给槽 3. 若槽不为空，则在槽的链表或树中找到key相同的节点，替换节点值为新值；或是没有key相同的节点，就在树中或链表尾部加入新节点；若链表加入新节点后长度达到8（槽不算，aka槽下原有7个节点），则进行红黑树转化 4. 如果是新加入节点，modCount、元素个数size自增1，如果元素个数超过阈值，则进行扩容 4. 扩容 1. 计算新容量newCap和新阈值newThr（ps: 当容量已到最大值时，不再扩容；2倍扩容；） 2. 创建新的数组，赋值给table 3. 将键值对重新映射到新数组上 1. 如果无链表，直接根据`hash&amp;(newCap-1)`定位 2. 如果是树节点，委托红黑树来拆分和重新映射 3. 为链表，根据`hash&amp;oldCap`的值分成0、1两组，映射到j和j+oldCap（0低位，1高位）（**链表顺序不变**） 5. 删除 1. 定位到槽 2. 找到删除节点 3. 删除节点，并修复链表或红黑树 6. 链表树化 1. 链表树化有两个条件，不满足采用扩容，满足再扩容 2. 树化时，将Node节点替换为TreeNode，保留next信息 3. 替换后，再从head开始，进行红黑树化（标记红黑节点、父子节点，如果root节点不是first节点，再修正next和prev？）【链表转成红黑树后，原链表的顺序仍然会被引用仍被保留了（红黑树的根节点会被移动到链表的第一位）】 在扩容过程中，树化要满足两个条件： 1. 链表长度大于等于 TREEIFY_THRESHOLD 8 2. 桶数组容量大于等于 MIN_TREEIFY_CAPACITY 64 7. 红黑树拆分（扩容时候） 红黑树中保留了next引用，拆分原理和链表相似 1. 根据hash拆分成两组（这时候会生成新的next关系） 2. 各组内根据情况，链化或者重新红黑树化 8. 红黑树链化 将TreeNode替换为Node ConcurrentHashMap 相比较HashMap，主要是增加了写操作时候的同步处理。扩容迁移时，多个线程帮助迁移。 1. 为什么要用synchronized代替ReentrantLock？ 1. 优化后的synchronized性能与ReentrantLock差不多，基于JVM也保证synchronized在各平台上的使用一致。 2. 锁粒度降低了；在大量数据操作下，基于api的ReentrantLock会有更大的内存开销。 2. sizeCtl 1. 默认为0 2. 当table为null时，持有一个initial table size用于初始化 3. 当sizeCtl&lt;0时 1. -1表示正在初始化 2. 非-1的负数 123（sizeCtl的低16位-1）表示有多少个线程参与扩容迁移 sizeCtl的高16位-(1 + the number of active resizing threads) sizeCtl&gt;0时，(n &lt;&lt; 1) - (n &gt;&gt;&gt; 1) = 0.75n （表示阈值，超过阈值需要扩容） 插入 计算hash 循环执行 如果数组为空，初始化initTable 如果hash定位到的槽为空，CAS替换为新节点，退出循环 如果槽不为空，节点hash为-1，说明正在迁移，helpTransfer 槽不为空，且不在迁移，那么，对头节点加锁，链表或红黑树形式插入或更新节点 addCount 迁移 transfer的第二个参数为空的时候，触发扩容，创建nextTable，在addCount和tryPresize中有这样的调用。 addCount是size不精确情况下，可能触发扩容；tryPresize是已知精确size的情况下做扩容。 计算步长stride 如果nextTab未创建，则创建之，并赋给nextTable 循环迁移 分配迁移区间i和bound（i从前往后，bound = i - stride + 1`，总之就是stride） 如果区间已达边界，将sc减1，表示本线程退出迁移。如果是最后一个迁移线程，标记finish和advance为true，进入下一循环recheck；非最后线程，直接退出方法。 如果finish为true，table = nextTab; sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1);，退出 若未达边界，且槽为空，CAS槽为fwd，进入下一循环 槽不为空，且槽已经是fwd，进入下一循环 最后一种情形，进行迁移 为链表，根据节点hash二进制第k位为0或1分成两组（n=2^k），1连接到高位槽上 为红黑树，分组同链表，分好的组根据节点个数判断是否链化或新生成红黑树 volatile的底层如何实现，怎么就能保住可见性了？ 见【34 JMM与volatile】 有参与过开源的项目吗？ 米有！ 线程池原理，拒绝策略，核心线程数 见【11】 ​ 1亿个手机号码，判断重复 不允许有误差的： hash到n个小文件中 每个文件做统计 个数大于1的是重复的 允许有误差的： 布隆过滤器 线程之间的交互方式有哪些？有没有线程交互的封装类？ 线程之间的协作 join() 在线程中调用另一个线程的join()方法，会将本线程挂起，直到目标线程结束 wait() notify() notifyAll() 调用 wait() 使得线程等待某个条件满足，线程在等待时会被挂起，当其他线程的运行使得这个条件满足时，其它线程会调用 notify() 或者 notifyAll() 来唤醒挂起的线程。 属于Object（不是Thread） await() signal() signalAll() java.util.concurrent 类库中提供了 Condition 类来实现线程之间的协调，可以在 Condition 上调用 await() 方法使线程等待，其它线程调用Condition的 signal() 或 signalAll() 方法唤醒等待的线程。 线程交互的封装类 CountDownLatch 用来控制一个线程等待多个线程。 维护了一个计数器 cnt，每次调用 countDown() 方法会让计数器的值减 1，减到 0 的时候，那些因为调用 await() 方 法而在等待的线程就会被唤醒。 12345678910111213141516 public class CountdownLatchExample &#123; public static void main(String[] args) throws InterruptedException &#123; final int totalThread = 10; CountDownLatch countDownLatch = new CountDownLatch(totalThread); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; totalThread; i++) &#123; executorService.execute(() -&gt; &#123; System.out.print(\"run..\"); countDownLatch.countDown();&#125;); &#125; countDownLatch.await(); System.out.println(\"end\"); executorService.shutdown();&#125; &#125;run..run..run..run..run..run..run..run..run..run..end 等待所有run结束 CyclicBarrier 用来控制多个线程互相等待，只有当多个线程都到达时，这些线程才会继续执行。 和 CountdownLatch 相似，都是通过维护计数器来实现的。线程执行 await() 方法之后计数器会减 1，并进行等待，直到计数器为 0，所有调用 await() 方法而在等待的线程才能继续执行。 CyclicBarrier 和 CountdownLatch 的一个区别是，CyclicBarrier 的计数器通过调用 reset() 方法可以循环使用，所以它才叫做循环屏障。 123456789101112131415161718192021public class CyclicBarrierExample &#123; public static void main(String[] args) &#123; final int totalThread = 10; CyclicBarrier cyclicBarrier = new CyclicBarrier(totalThread); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; totalThread; i++) &#123; executorService.execute(() -&gt; &#123; System.out.print(\"before..\"); try &#123; cyclicBarrier.await(); &#125; catch (InterruptedException | BrokenBarrierException e) &#123; e.printStackTrace(); &#125; System.out.print(\"after..\"); &#125;);&#125; executorService.shutdown(); &#125;&#125;before..before..before..before..before..before..before..before..before..before..after..after..after..after..after..after..after..after..after..after.. 等待所有before结束 3. Semaphore 1. Semaphore 类似于操作系统中的信号量，可以控制对互斥资源的访问线程数。 2. 123456789101112131415161718192021public class SemaphoreExample &#123; public static void main(String[] args) &#123; final int clientCount = 3; final int totalRequestCount = 10; Semaphore semaphore = new Semaphore(clientCount); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; totalRequestCount; i++) &#123; executorService.execute(()-&gt;&#123; try &#123; semaphore.acquire(); System.out.print(semaphore.availablePermits() + \" \"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; semaphore.release(); &#125;&#125;); &#125; executorService.shutdown(); &#125;&#125;1 0 1 1 1 2 2 2 0 1 有限个资源 两次点击，怎么防止重复下订单？ 两次点击的场景 没有刷新和前端控制，同一个按钮点了两次 网络问题以为失败（其实成功了）又提交了一次 rpc等重试服务 刷新前后各点一次（或者表单刷新又提交了一次） 点了后退按钮，再前进 处理方案： 前端 弹出确认界面，或disable入口并倒计时等 后端 约定【所谓重复订单，需要定义这是相同的订单】，需要和客户端配合实现 比如支付可以用订单ID作判断 如果是下单，可以用uuid或服务端先生成一个全局唯一的订单ID，客户端如果未接收到下单成功的响应，多次重试都用这一个订单ID来提交。（如果是刷新，需要客户端去服务端请求最新购物车数据，已成功下单的商品已被移除；如果是未刷新页面的重试，则使用同一个订单ID；或者提示用户刷新、提示是否重试） 后端的去重判断方式 https://www.cnblogs.com/jett010/articles/9056567.html – 本质上分布式锁的应用 基于数据库中对应订单ID的状态做判断，ID已存在（下单），或者状态已变更（修改订单，比如取消、退款等）。如果查询和更新是分开的两个操作，会存在时间差，比如查询完后状态被别的线程修改了，可以用加数据库锁的方式解决这个问题（悲观锁或乐观锁） 利用数据库唯一性索引，性能比较低 Redis分布式锁，key是订单ID，要点是加锁和解锁的原子性 ps redis的计数器是原子操作 https://redis.io/commands/incr 数据库表设计，索引 【explain分析也看看】 ​ Redis的缓存淘汰策略、更新策略 过期策略 定期删除：默认每隔100ms随机抽取一些设置了过期时间的key，检查是否过期，如果过期就删除（因为全表扫描非常耗时、耗性能，所以是随机，也因此要配合惰性删除） 惰性删除：在客户端要获取某个key时，判断key是否设置过期以及是否过期，如果过期先删除 内存淘汰策略 Redis在使用内存达到某个阈值（通过maxmemory配置)的时候，就会触发内存淘汰机制，选取一些key来删除。 12# maxmemory &lt;bytes&gt; 配置内存阈值# maxmemory-policy noeviction noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。默认策略 allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key。 allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个key。 volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key。 volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key。 volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先移除。 如何选取合适的策略？比较推荐的是两种lru策略。根据自己的业务需求。如果你使用Redis只是作为缓存，不作为DB持久化，那推荐选择allkeys-lru；如果你使用Redis同时用于缓存和数据持久化，那推荐选择volatile-lru。 redis过期策略和内存淘汰策略 java实现LRU dubbo、netty、RPC介绍原理 https://juejin.im/post/5e215783f265da3e097e9679 RPC remote procedure call 远程过程调用，是一种进程间的通信方式，是一种技术思想，而不是规范 一次完整的rpc调用流程。RPC的目标是把2-8封装起来，对用户透明。 (1):服务消费方(client)以本地调用方式调用服务。 (2):client stub接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体。 (3):client stub找到服务地址，并将消息发送到服务端。 (4):server stub收到消息后进行解码。 (5):server stub根据解码结果调用本地的服务。 (6):本地服务执行并将结果返回给server stub。 (7):server stub将返回结果打包成消息并发送至消费方。 (9):client stub接收到消息，并进行解码。 (9):服务消费方得到最终结果。 决定rpc效率的两个重要因素：通信效率，序列化和反序列化效率 常见rpc框架：dubbo、gRPC、Thrift、HSF（high speed service framework） 2. netty [理解netty](https://juejin.im/post/5bdaf8ea6fb9a0227b02275a) 1. netty是一个异步事件驱动的网络应用程序框架，是基于NIO的多路复用模型实现的。 2. 传统HTTP服务 【HTTP服务器之所以称为HTTP服务器，是因为编码解码协议是HTTP协议，如果协议是Redis协议，那它就成了Redis服务器，如果协议是WebSocket，那它就成了WebSocket服务器，等等。 使用Netty可以定制编解码协议，实现自己的特定协议的服务器。】 1. 创建一个ServerSocket，监听并绑定一个端口 2. 一系列客户端来请求这个端口 3. 服务器使用Accept，获得一个来自客户端的Socket连接对象 4. 启动一个新线程处理连接 1. 读Socket，得到字节流 2. 解码协议，得到HTTP请求对象 3. 处理HTTP请求，得到一个结果，封装成一个HTTPResponse对象 4. 编码协议，将结果序列化字节流写入Socket，发给客户端 5. 循环步骤3 3. NIO 1. 不是Java独有的概念，NIO代表IO多路复用。 2. 由操作系统提供的功能，早期select，后期linux-epoll/max-kqueue。一般就说是epoll（没人用mac当服务器） 3. Netty基于Java NIO进行了封装，提供易于操作的使用模式和接口。 4. BIO (Blocking IO)，如何理解blocking 1. 服务端监听时，accept是阻塞的，只有新连接来了，accept才会返回，主线程才能继续 2. 读写Socket时，read是阻塞的，只有请求消息来了（需要读完吗？），read才能返回，子线程才能继续处理 3. 读写Socket时，write是阻塞的，只有客户端把消息接收了（客户端把消息接收了是什么表现？），write才能返回，子线程才能继续 5. NIO利用**事件机制**（=事件驱动机制）实现非阻塞。【可以用一个线程把Accept，读写操作，请求处理的逻辑全干了。如果什么事都没得做，它也不会死循环，它会将线程休眠起来，直到下一个事件来了再继续干活，这样的一个线程称之为NIO线程。】 伪代码 123456789101112131415while true &#123; events = takeEvents(fds) // 获取事件，如果没有事件，线程就休眠 for event in events &#123; if event.isAcceptable &#123; doAccept() // 新链接来了 &#125; elif event.isReadable &#123; request = doRead() // 读消息 if request.isComplete() &#123; doProcess() &#125; &#125; elif event.isWriteable &#123; doWrite() // 写消息 &#125; &#125;&#125; 4. Reactor（基于事件驱动）线程模型 【netty可以基于以下模型灵活配置，比较常见的是用第三种。】 【在Netty里面，Accept连接可以使用单独的线程池去处理，读写操作又是另外的线程池来处理。】 【Accept连接和读写操作也可以使用同一个线程池来进行处理。请求处理逻辑既可以使用单独的线程池进行处理，也可以跟读写线程放在一块处理。】 【线程池中的每一个线程都是NIO线程。用户可以根据实际情况进行组装，构造出满足系统需求的高性能并发模型。】 1. Reactor单线程模型。一个NIO线程+一个accept线程。reactor线程负责分发，read、decode等操作都由其他线程处理。就和上面的伪代码差不多。 ![image-20200523181425912](../../image/image-20200523181425912.png) 2. Reactor多线程模型。相比上一种，【其他线程】由线程池来托管。 ![image-20200523181750089](../../image/image-20200523181750089.png) 3. Reactor主从模型。多个acceptor的NIO线程池用于接收客户端的连接。 ![image-20200523181851749](../../image/image-20200523181851749.png) 5. TCP粘包拆包 1. 现象 1. 假设使用netty在客户端重复写100次数据&quot;你好，我的名字是xxx!&quot;给服务端，用ByteBuf存放这个数据 2. 服务端接收后输出，一般存在三种情况 1. 完整的一个字符串 2. 字符串多了 3. 字符串少了 2. 原因：尽管client按照ByteBuf为单位发送数据，server按照ByteBuf读取，但操作系统底层是tcp协议，按照字节发送和接收数据，在netty应用层，重新拼装成的ByteBuf与客户端发送过来的ByteBuf可能不是对等的。 因此，我们**需要自定义协议来封装和解封应用层的数据包**。 3. netty中定义好的拆包器 1. 固定长度的拆包器 FixedLengthFrameDecoder 2. 行拆包器 LineBasedFrameDecoder 3. 分隔符拆包器 DelimiterBasedFrameDecoder （行拆包器的通用版本，可自定义分隔符） 4. 长度域拆包器 LengthFieldBasedFrameDecoder （最通用，在协议中包含长度域字段） 6. 零拷贝 1. 传统方式的拷贝 `File.read(bytes)` `Socket.send(bytes)` 需要四次数据拷贝和四次上下文切换 1. 数据从磁盘读取到内核的read buffer 2. 数据从内核缓冲区拷贝到用户缓冲区 3. 数据从用户缓冲区拷贝到内核的socket buffer 4. 数据从内核的socket buffer拷贝到网卡接口（硬件）的缓冲区 2. 零拷贝的概念 1. 上面的第二步和第三步是没有必要的，通过java的FileChannel.transferTo方法，可以避免上面两次多余的拷贝（需要操作系统支持） 2. 调用transferTo,数据从文件由DMA引擎拷贝到内核read buffer 接着DMA从内核read buffer将数据拷贝到网卡接口buffer 上面的两次操作都不需要CPU参与，达到了零拷贝。 3. Netty中的零拷贝 体现在三个方面： 1. bytefuffer Netty发送和接收消息主要使用bytebuffer，bytebuffer使用直接内存（DirectMemory）直接进行Socket读写。 原因：如果使用传统的堆内存进行Socket读写，JVM会将堆内存buffer拷贝一份到直接内存中然后再写入socket，多了一次缓冲区的内存拷贝。DirectMemory中可以直接通过DMA发送到网卡接口 2. Composite Buffers 传统的ByteBuffer，如果需要将两个ByteBuffer中的数据组合到一起，需要先创建一个size=size1+size2大小的新的数组，再将两个数组中的数据拷贝到新的数组中。 使用Netty提供的组合ByteBuf，就可以避免这样的操作。CompositeByteBuf并没有真正将多个Buffer组合起来，而是保存了它们的引用，从而避免了数据的拷贝，实现了零拷贝。 3. 对FileChannel.transferTo的使用 Netty中使用了FileChannel的transferTo方法，该方法依赖于操作系统实现零拷贝。 3. dubbo 1. 简介与特性：dubbo是一款高性能、轻量级的开元Java RPC框架，提供三大核心能力：**面向接口的远程方法调用**、**智能容错和负载均衡**、**服务自动注册和发现**。 1. 【以下几点是官网上的特性介绍...】 2. 面向接口的远程方法调用：提供高性能的基于代理的远程调用能力，服务以接口为粒度，为开发者屏蔽远程调用底层细节。 3. 智能负载均衡：内置多种负载均衡策略（有哪些？），感知下游节点的健康状况，显著减少调用延迟，提高系统吞吐量。 4. 服务自动注册于发现：支持多种注册中心服务（有哪些？），服务实例上下线实时感知（具体实现是什么？）。 5. 高度可扩展能力：遵循微内核+插件的设计原则，所有核心能力如Protocol、Transport、Serialization被设计为可扩展点，平等的对待内置实现和第三方实现。（SPI设计模式？） 6. 运行期流量调度：内置条件、脚本等路由策略，通过配置不同的路由规则，实现灰度发布、同机房优先等功能。 7. 可视化的服务治理与运维：提供丰富服务治理、运维工具：随时查看服务元数据、服务健康状态以及调用统计，实时下发路由策略、调度配置参数。 2. dubbo架构 &lt;img src=&quot;../../image/image-20200523161523850.png&quot; alt=&quot;image-20200523161523850&quot; style=&quot;zoom:30%;&quot; /&gt; &lt;img src=&quot;../../image/image-20200523161846866.png&quot; alt=&quot;image-20200523161846866&quot; style=&quot;zoom:50%;&quot; /&gt; 以上两张图说明dubbo执行流程： 1. dubbo容器启动后，provider将自己提供的服务注册到注册中心（注册中心便知道有哪些服务上线了） 2. consumer启动后，从注册中心订阅需要的服务。 3. 注册中心以长连接的方式向consumer发送服务变更通知。 4. consumer同步调用provider的服务（如果服务有多个节点，可通过负载均衡算法选择一个节点进行调用） 5. consumer和provider会定期将调用信息（调用时间、调用服务信息）发送给监控中心 6. Dubbo容器启动、服务生产者注册自己的服务、服务消费者从注册中心中订阅服务是在Dubbo应用启动时完成的；consumer调用provider是同步过程；注册中心向consumer发送服务变更通知是异步的；consumer和provider向监控中心发送信息是异步的。 调用链整体展开： ![image-20200523162137463](../../image/image-20200523162137463.png) 下面这张图看起来有点复杂了.. ![image-20200523162039246](../../image/image-20200523162039246.png) 3. Dubbo配置的覆盖关系 (1):方法级优先、接口级次之，全局配置优先级最低。 (2):如果级别一样，则消费者优先，提供方次之。 4. dobbo高可用 1. 注册中心Zookeeper宕机，还可以消费Dubbo暴露的服务。 2. Dubbo的监控中心宕机，不会影响Dubbo的正常使用，只是丢失了部分采样数据。 3. 数据库宕机后，注册中心仍然可以通过缓存提供服务列表查询，但是不能注册新的服务。 4. 注册中心集群的任意一个节点宕机，将自动切换到另外一台。 5. 注册中心全部宕机，服务提供者和消费者可以通过本地缓存通讯。 6. 服务提供者无状态，任意一台宕机后，不影响使用。 7. 服务提供者全部宕机，服务消费者应用将无法使用，并且会无限次重连等待服务提供者恢复。 5. 负载均衡策略 1. 【默认为随机】 2. 基于**权重的随机**负载均衡：Random LoadBalance，比如orderService想要远程调用userService，而userService分别在三台机器上，我们可以给每台机器设置权重，比如三台机器的权重依次为100、200、50，则总权重为350，则选择第一台的概率就是100/350. 3. 基于**权重的轮询**负载均衡：RoundRobin LoadBalance（可以理解为按照权重占比进行轮询。占比少的，当权重比较低时就不会再去权重低的机器上请求。如果某台机器性能一般，但权重占比高，就很可能卡在这里） 4. 最少活跃数负载均衡：LeastActive LoadBalance，比如三台服务器上一次处理请求所花费的时间分别为100ms、1000ms、300ms，则这一次请求回去上一次处理请求时间最短的机器，所以这次一号服务器处理这次请求。 5. 一致性Hash负载均衡：ConsistentHash LoadBalance 原文：https://blog.csdn.net/revivedsun/java/article/details/71022871 一致性Hash负载均衡涉及到两个主要的配置参数为hash.arguments 与hash.nodes。 hash.arguments ： 当进行调用时候根据调用方法的哪几个参数生成key，并根据key来通过一致性hash算法来选择调用结点。例如调用方法invoke(String s1,String s2); 若hash.arguments为1(默认值)，则仅取invoke的参数1（s1）来生成hashCode。 hash.nodes： 为结点的副本数。 12345缺省只对第一个参数Hash，如果要修改，请配置&lt;dubbo:parameter key=\"hash.arguments\" value=\"0,1\" /&gt;缺省用160份虚拟节点，如果要修改，请配置&lt;dubbo:parameter key=\"hash.nodes\" value=\"320\" /&gt; 6. 降级服务 当服务器压力剧增的情况下，根据实际业务及流量，对一些服务和页面有策略地不处理或者换种简单的方式处理，从而释放服务器资源以保证核心交易正常或高效运行。 1. mock=force:return+null:表示消费方对该服务的方法都返回null值，不发起远程调用。用来屏蔽不重要的服务不可用时对调用方的影响，可以直接在Dubbo客户端(localhost:7001)对服务消费者设置，屏蔽掉即可。 2. mock=fall:return+null:表示消费方对该服务的方法调用在失败后，再返回null，不抛出异常。用来容忍不重要服务不稳定时对调用方的影响，可以直接在Dubbo客户端(localhost:7001)对服务消费者设置，容错掉即可。 7. 集群容错 1. Failover Cluster:失败自动切换，当出现失败，重试其他服务器。通常用于读操作，但重试会带来更长延迟。可通过retries=n来设置重试次数。 2. Failfast Cluster:快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增操作。 3. Forking Cluster:并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作，但需要浪费更多的服务资源。通过过fork=n设置最大并行数。 4. Broadcast Cluster:广播调用所有提供者，逐个调用，任意一台报错则报错，通常用于通知所有服务提供者更新缓存或日志等本地资源信息。 限流算法 缓存：提升系统访问速度和增大系统处理容量 降级：当服务器压力剧增的情况下，根据当前业务情况对一些服务和页面有策略地降级，以此释放服务器资源以保证核心任务的正常运行 限流：限流的目的是通过对并发访问/请求进行限速，或者对一个时间窗口内的请求进行限速来保护系统，一旦达到限制速率则可以拒绝服务、排队或等待、降级等处理 限流算法： 固定窗口法 实现：固定时间内限定个数，比如限定每分钟100个请求 缺点：无法应对两个时间边界内的突发流量。比如在计数器清零的前一秒和后一秒都进来100个请求，那么系统短时间内就收到了两倍（200个）请求，有可能超负荷。 原因：统计精度不够 滑动窗口法 实现：简单来说就是随着时间的推移，时间窗口也会持续移动，有一个计数器不断维护着窗口内的请求数量，这样就可以保证任意时间段内，都不会超过最大允许的请求数。例如当前时间窗口是0s~60s，请求数是40，10s后时间窗口就变成了10s~70s，请求数是60。 可以用Redis有序集合实现.. 缺点：还是不能解决细粒度请求过于集中的问题，比如限制一分钟60个请求，但在59s时发送了60个请求过来。 漏桶算法 算法思想：与令牌桶算法有点相反。不限制流入速率，但以固定的速度将水流出。如果流入速度太大会导致水满溢出，溢出的请求被丢弃。 实现一：基于queue。queue的大小表示桶的大小，queue满了请求会被拒绝；另维护一个定时器，根据设定的出水速度去queue中取一个任务，比如限定一秒钟5个请求，就200ms去取一个任务，取到就执行，取不到就轮空。 实现二：基于meter，计数器。【…写的不是很清楚，看起来和固定窗口法很像了，没有体现固定的出水，只表示时间粒度比较细】 令牌桶法 算法思路：以固定的速率生成令牌，把令牌放到固定容量的桶里，超过桶容量的令牌则丢弃。每来一个请求获取一次令牌，只有获得令牌的请求才能放行，没有获得令牌的请求丢弃。 【令牌是匀速生成的，如果请求超高频，则完全被限制成令牌的生成速率；如果请求突发，也最多只允许令牌数的上限。】 Guava RateLimiter 令牌桶与漏桶的比较 漏桶能够强行限制数据的传输速率 令牌桶限制数据的平均传输速率，允许某种程度的突发传输 【看起来比较喜欢令牌桶】 zk挂了怎么办？ todo 指zk集群挂了其中一台机器？ – 集群自己可以处理 挂的是master 挂的是follower 挂的是.. 集群全挂了？—那就是全挂了啊 趁早加入监控和降级策略 分布式锁的实现方式，zk实现和Redis实现的比较 实现方式：CAP的应用 MySQL唯一索引 实现：锁名称建立唯一索引，先插入数据的线程获得锁 缺点：完全依赖数据库的可用性（单点问题，无主从切换）和性能 Redis 实现：setnx key value expire_time 优缺点：为解决无主从切换的问题，可以使用Redis集群，或者sentinel哨兵模型。当master节点出现故障，哨兵从slave中选取节点称为新master节点。文章说，Redis集群的复制是AP模式，可能存在数据不一致，导致存在两个线程获得到锁的情况。（一个线程在原master获得锁，另一个线程在新master获得锁） 对数据一致性非常敏感的场景，建议使用CP模型（比如zk） zk 实现： 线程向zk的锁目录，申请创建有序的临时节点 如果建成的节点序号最小，表明获得到锁 如果序号非最小，监听自己的前一个节点 删除节点表示释放锁；当获取锁的客户端异常、无心跳，临时节点会被删除，也表示释放锁 优缺点：CP模式，zk的分布式锁比Redis的可靠，但Redis的性能更高。要根据自己的业务场景，再选型。 秒杀场景设计，应付突然的爆发流量 一个秒杀系统的设计思考 两个核心问题：并发读、并发写 对应到架构设计，就是高可用、一致性和高性能的要求 1. 高性能：涉及高读和高写。 核心理念：高读-&gt;尽量“少读”或“读少”，高写-&gt;数据拆分 1. 动静分离：将页面拆分，静态部分做CDN缓存（秒级失效，若干CDN节点），动态部分异步请求。 数据拆分、静态缓存、数据整合（指动态数据、静态数据怎么整合在一起，一种服务端将动态数据插入到静态页面，另一种前端异步调用） 2. 热点优化 1. 热点操作：零点刷新、零点下单、零点加购物车等，属于用户行为不好改变，但可以做一些限制，比如用户频繁刷新页面时进行提示阻断。 2. 热点数据： 1. 热点识别：分为**静态热点**（可以提前预测的。大促前夕，可以根据大促的行业特点、活动商家等纬度信息分析出热点商品，或者通过卖家报名的方式提前筛选；另外，还可以通过技术手段提前预测，例如对买家每天访问的商品进行大数据计算，然后统计出 TOP N 的商品，即可视为热点商品）和**动态热点**（无法提前预测的。比如直播临时做了个广告可能导致一件商品短期内被大量购买）。 动态热点的识别实现思路：1. **异步采集交易链路各个环节的热点key信息**，比如nginx采集访问url或agent采集热点日志（一些中间件本身已具备热点发现能力），提前识别潜在的热点数据。2. **聚合分析热点数据，达到一定规则的热点数据，通过订阅分发推送到链路系统**，各系统根据自身需求决定如何处理热点数据，或限流或缓存，从而实现热点保护 最好做到秒级实时，这样动态发现才有意义。实际上也是对核心节点的数据采集和分析能力提出了较高的要求。 2. 热点隔离：将热点数据隔离出来，不影响非热点数据的访问。 -- 【我怎么觉得参与秒杀的商品都可以直接作为热点呢？？】 1. 业务隔离。秒杀作为一种营销活动，卖家需要单独报名，从技术上来说，系统可以提前对已知热点做缓存预热 -- 【静态热点吧..】 2. 系统隔离。系统隔离是运行时隔离，通过分组部署和另外 99% 进行分离，另外秒杀也可以申请单独的域名，入口层就让请求落到不同的集群中 -- 【也是静态热点吧..】 3. 热点数据，可以启用单独的缓存集群或者DB服务组，从而更好的实现横向或纵向能力扩展 -- 【可以是动态的，假如一个商品被动态标记为热点后】 3. 热点优化：对这1%的数据做针对性的优化 1. 缓存：热点缓存是最为有效的办法。 2. 限流：流量限制更多是一种保护机制。--属于有损服务。 3. 系统优化：提升硬件水平、调优JVM性能、代码层面优化 1. 代码层面优化：1. **减少序列化**（减少RPC调用，一种可行的方案是将多个关联性较强的应用进行 “合并部署”，从而减少不同应用之间的 RPC 调用（微服务设计规范））2. **直接输出流数据**（只要涉及字符串的I/O操作，无论是磁盘 I/O 还是网络 I/O，都比较耗费 CPU 资源，因为字符需要转换成字节，而这个转换又必须查表编码。所以对于常用数据，比如静态字符串，推荐提前编码成字节并缓存，具体到代码层面就是通过 OutputStream() 类函数从而减少数据的编码转换；另外，热点方法toString()不要直接调用ReflectionToString实现，推荐直接硬编码，并且只打印DO的基础要素和核心要素-- 这整段不是很懂，toString懂啊哈哈）3. **裁剪日志异常堆栈**，超大流量下频繁地输出完整堆栈，会加剧系统当前负载（可以通过日志配置文件控制异常堆栈输出的深度）4. **去组件框架**：极致优化要求下，可以去掉一些组件框架，比如去掉传统的 MVC 框架，直接使用 Servlet 处理请求。这样可以绕过一大堆复杂且用处不大的处理逻辑，节省毫秒级的时间，当然，需要合理评估你对框架的依赖程度 2. 一致性：秒杀场景下的一致性问题，主要是库存扣减的准确性问题 1. 减库存的方式： 1. 下单减库存（用户体验好，但存在恶意下单不付款的问题） 2. 付款减库存（用户体验差，很多人下单成功但有人不能付款） 3. **预扣库存**：缓解了以上两种方式的问题。预扣库存实际就是“下单减库存”和 “付款减库存”两种方式的结合，将两次操作进行了前后关联，下单时预扣库存，付款时释放库存。 劣势：并没有彻底解决以上问题。比如针对恶意下单的场景，虽然可以把有效付款时间设置为 10 分钟，但恶意买家完全可以在 10 分钟之后再次下单。 实际业界也多用这种方式，下单后一般都有个 “有效付款时间”，超过该时间订单自动释放，是典型的预扣库存方案。 4. 恶意下单的解决方案主要还是结合安全和反作弊措施来制止。比如，识别频繁下单不付款的买家并进行打标，这样可以在打标买家下单时不减库存；再比如为大促商品设置单人最大购买件数，一人最多只能买 N 件商品；又或者对重复下单不付款的行为进行次数限制阻断等 5. 超卖分为两种：1. 商家可以补货的，允许超卖；2. 不允许超卖，限定库存字段不能为负数：1）一是在通过事务来判断，即保证减后库存不能为负，否则就回滚；2）直接设置数据库字段类型为无符号整数，这样一旦库存为负就会在执行 SQL 时报错 2. 一致性性能的优化 1. 高并发读：“分层校验”。即在读链路时，不做一致性校验，只做不影响性能的检查（如用户是否具有秒杀资格、商品状态是否正常、用户答题是否正确、秒杀是否已经结束、是否非法请求等），**在写链路的时候，才对库存做一致性检查，在数据层保证最终准确性。** 不同层次尽可能过滤掉无效请求，只在“漏斗” 最末端进行有效处理，从而缩短系统瓶颈的影响路径。 2. 高并发写 1. 更换DB选型：换用缓存系统（带有持久化功能的缓存，如Redis，适合减库存操作逻辑单一的，无事务要求的） 2. 优化DB性能：库存数据落地到数据库实现其实是一行存储（MySQL），因此会有大量线程来竞争 InnoDB 行锁。但并发越高，等待线程就会越多，TPS 下降，RT 上升，吞吐量会受到严重影响。 两种方法： 1. 应用层排队。加入分布式锁，控制集群对同一行记录进行操作的并发度，也能控制单个商品占用数据库连接的数量 2. 数据层排队。（应用层排队是有损性能的，数据层排队是最为理想的。）业界中，阿里的数据库团队开发了针对InnoDB 层上的补丁程序（patch），可以基于DB层对单行记录做并发排队，从而实现秒杀场景下的定制优化。另外阿里的数据库团队还做了很多其他方面的优化，如 COMMIT_ON_SUCCESS 和 ROLLBACK_ON_FAIL 的补丁程序，通过在 SQL 里加入提示（hint），实现事务不需要等待实时提交，而是在数据执行完最后一条 SQL 后，直接根据 TARGET_AFFECT_ROW 的结果进行提交或回滚，减少网络等待的时间（毫秒级）。目前阿里已将包含这些补丁程序的 MySQL 开源：AliSQL。 3. 高可用 1. 流量削峰 1. 答题：通过提升购买的复杂度，达到两个目的：防止作弊&amp;延缓请求。本质是通过在入口层削减流量，从而让系统更好地支撑瞬时峰值。 2. 排队：最为常见消息队列，通过把同步的直接调用转换成异步的间接推送，缓冲瞬时流量。（弊端：请求积压、用户体验）（排队本质是在业务层将一步操作转变成两步操作，从而起到缓冲的作用，但鉴于此种方式的弊端，最终还是要基于业务量级和秒杀场景做出妥协和平衡。） 3. 过滤：过滤的核心目的是通过减少无效请求的数据IO 保障有效请求的IO性能。 1. 读限流：对读请求做限流保护，将超出系统承载能力的请求过滤掉 2. 读缓存：对读请求做数据缓存，将重复的请求过滤掉 3. 写限流：对写请求做限流保护，将超出系统承载能力的请求过滤掉 4. 写校验：对写请求做一致性校验，只保留最终的有效数据 2. Plan B 1. 架构阶段：考虑系统的可扩展性和容错性，避免出现单点问题。例如多地单元化部署，即使某个IDC甚至地市出现故障，仍不会影响系统运转 编码阶段：保证代码的健壮性，例如RPC调用时，设置合理的超时退出机制，防止被其他系统拖垮，同时也要对无法预料的返回错误进行默认的处理 测试阶段：保证CI的覆盖度以及Sonar的容错率，对基础质量进行二次校验，并定期产出整体质量的趋势报告 发布阶段：系统部署最容易暴露错误，因此要有前置的checklist模版、中置的上下游周知机制以及后置的回滚机制 运行阶段：系统多数时间处于运行态，最重要的是运行时的实时监控，及时发现问题、准确报警并能提供详细数据，以便排查问题 故障发生：首要目标是及时止损，防止影响面扩大，然后定位原因、解决问题，最后恢复服务 2. 预防：建立常态压测体系，定期对服务进行单点压测以及全链路压测，摸排水位 管控：做好线上运行的降级、限流和熔断保护。需要注意的是，无论是限流、降级还是熔断，对业务都是有损的，所以在进行操作前，一定要和上下游业务确认好再进行。就拿限流来说，哪些业务可以限、什么情况下限、限流时间多长、什么情况下进行恢复，都要和业务方反复确认 监控：建立性能基线，记录性能的变化趋势；建立报警体系，发现问题及时预警 恢复：遇到故障能够及时止损，并提供快速的数据订正工具，不一定要好，但一定要有 在系统建设的整个生命周期中，每个环节中都可能犯错，甚至有些环节犯的错，后面是无法弥补的或者成本极高的。所以高可用是一个系统工程，必须放到整个生命周期中进行全面考虑。同时，考虑到服务的增长性，高可用更需要长期规划并进行体系化建设。 ![image-20200525191748562](../../image/image-20200525191748562.png) ​ 分布式数据一致性 https://juejin.im/post/5ce7b325e51d45772a49ac9d 数据不一致性的情形 主库、从库和缓存的数据一致性：相同数据冗余。为保证数据库的高可用和高性能，会采用主从（备）架构并引入缓存。数据不一致存在于数据冗余的时间窗口内。 多副本数据之间的数据一致性：相同数据副本。一份数据有多个副本存储到不同节点上，客户端可以访问任一节点进行读写。常用协议包括Paxos、ZAB、Raft、Quorum、Gossip等。 分布式服务之间的数据一致性：微服务架构下，不同服务操作不同的库表，要求库表间要保持一致（等价于分布式事务） – 【感觉题目问的是这个】 对CAP理论的理解 https://www.zhihu.com/question/54105974/answer/139037688 C代表一致性，A代表可用性（在一定时间内，用户的请求都会得到应答），P代表分区容忍性。 一个分布式系统里面，节点组成的网络本来应该是连通的。然而可能因为一些故障，使得有些节点之间不连通了，整个网络就分成了几块区域。数据就散布在了这些不连通的区域中。这就叫分区。 当你一个数据项只在一个节点中保存，那么分区出现后，和这个节点不连通的部分就访问不到这个数据了。这时分区就是无法容忍的。 提高分区容忍性的办法就是一个数据项复制到多个节点上，那么出现分区之后，这一数据项就可能分布到各个区里。容忍性就提高了。 然而，要把数据复制到多个节点，就会带来一致性的问题，就是多个节点上面的数据可能是不一致的。要保证一致，每次写操作就都要等待全部节点写成功，而这等待又会带来可用性的问题。 总的来说就是，数据存在的节点越多，分区容忍性越高，但要复制更新的数据就越多，一致性就越难保证。为了保证一致性，更新所有节点数据所需要的时间就越长，可用性就会降低。 3. 理解数据库本地事务 [分布式事务](https://juejin.im/post/5b5a0bf9f265da0f6523913b) 1. ACID 1. 原子性 atomicity：一个事务(transaction)中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。 2. 一致性 consistency：事务的执行前后，是从一个一致性状态转移到另一个一致性状态。【是通过原子性和隔离性保证的。】 3. 隔离性 isolation：事务并发执行时，每个事务有各自完整的数据空间。有不同的隔离级别，大部分通过锁实现。 4. 持久性 durability：事务只要成功执行，对数据库所做的更新会永久保存下来。 2. 隔离级别 3. innodb实现原理：主要通过锁和日志来保证ACID 1. 通过锁机制和mvcc实现隔离性 2. redo log（重做日志）实现持久性 3. undo log实现原子性和一致性【可以回滚】 4. 分布式事务 -- **主要是要保证原子性** 1. 分布式事务 指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于不同的分布式系统的不同节点之上。 一次大操作由不同的小操作组成，小操作分布在不同的服务器上，且属于不同的应用，**分布式服务需要保证这些小操作要么全部成功，要么全部失败**。 2. 分布式事务的场景 1. Service多个节点 -- 指微服务等，比如一个交易平台，订单、库存、余额等在不同的服务下，一次交易需要原子性得更新。 2. resource多个节点 -- 指分库分表了，比如转账双方的余额在不同的表里，一次转账双方都要正确更新。 3. 理论 1. CAP 2. BASE 4. 解决方案 1. 2PC 1. 第一阶段：预提交，并反映是否可以提交。【事务管理器要求每个涉及到事务的数据库预提交(precommit)此操作，并反映是否可以提交.】 2. 第二阶段：提交，或者回滚。【事务协调器要求每个数据库提交数据，或者回滚数据。】 3. 优点：实现成本低 4. 缺点：单点问题（事务管理器单点，可能引起资源管理器一直阻塞），同步阻塞（precommit后，资源管理器一直处于阻塞中，直到提交、释放资源），可能存在数据不一致（比如协调者发出了commit通知，但只有部分参与者收到通知并执行了commit，其余参与者则没收到通知处于阻塞状态，就产生不一致了） 2. TCC 1. try - confirm - cancel 2. 协调者变成多点，引入进群 3. 引入超时，超时后进行补偿，并且不会锁定整个资源，缓解同步阻塞 4. 数据一致性：通过补偿机制，由业务活动管理器控制一致性 3. 本地消息表：核心是将需要分布式处理的任务通过消息日志的方式来异步执行。消息日志可以存储到本地文本、数据库或消息队列，再通过业务规则自动或人工发起重试。 4. MQ事务 5. SAGA 6. seata 数据库隔离级别 读未提交 读提交 可重复读 可串行化 一致性哈希 一致性哈希算法 求出各节点的哈希值，将其配置到0~2^32的圆（continuum）上 用同样方法求出存储数据的哈希值，映射到相同的圆上 从数据映射到的位置开始顺时针查找，将数据保存到找到的第一个节点上。如果超过2^32仍然找不到节点，就保存到第一个节点上 【如果添加一个节点node5，只会影响该节点的逆时针方向的第一个节点node4会受到影响（原来在node4上的数据要重新分配一些到node5上）】 一致性哈希算法对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性 在服务节点太少时，容易因节点分部不均匀而造成数据倾斜问题，可引入虚拟节点机制，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。同时数据定位算法不变，只是多了一步虚拟节点到实际节点的映射。 消息队列原理介绍 作用 解耦 异步 削峰/限流 注解的原理 【见4】 数据库原理，数据库中间件，索引优化 索引优化 如果MySQL评估使用索引比全表扫描还慢，则不会使用索引 前导模糊查询（like ‘%xx’）不会使用索引，可以优化为非前导模糊查询（like ‘xx%’） 数据类型出现隐式转换的时候不会命中索引，特别是当列类型是字符串，一定要将字符常量值用引号引起来 复合索引，要满足最左匹配原则 union、in、or 都能够命中索引，建议使用 in 查询的CPU消耗：or (id=1 or id=2)&gt; in (id in (1,2)) &gt;union(id = 1 union id = 2) 用or分割开的条件，如果or前的条件中列有索引，而后面的列中没有索引，那么涉及到的索引都不会被用到 因为or后面的条件列中没有索引，那么后面的查询肯定要走全表扫描，在存在全表扫描的情况下，就没有必要多一次索引扫描增加IO访问。 负向条件查询不能使用索引，可以优化为 in 查询 负向条件有：!=、&lt;&gt;、not in、not exists、not like 等。 范围条件查询可以命中索引 范围条件有：&lt;、&lt;=、&gt;、&gt;=、between等（返回数据的比例超过30%，会不使用索引） 查询条件（带有计算函数）执行计算不会命中索引 利用覆盖索引进行查询，避免回表 建议索引的列设置为非null 更新十分频繁的字段上不宜建立索引 区分度不大的字段上不宜建立索引 业务上具有唯一特性的字段，建议建立唯一索引 多表关联时，关联字段建议有索引 创建索引时避免以下错误观念 索引越多越好，认为一个查询就需要建一个索引。 宁缺勿滥，认为索引会消耗空间、严重拖慢更新和新增速度。 抵制唯一索引，认为业务的唯一性一律需要在应用层通过“先查后插”方式解决。 过早优化，在不了解系统的情况下就开始优化。","tags":[]},{"title":"docker创建kafka环境","date":"2020-04-06T16:52:20.000Z","path":"2020/04/07/docker创建kafka环境/","text":"下载镜像 1docker pull zookeeper:3.5.6 1docker pull wurstmeister/kafka 启动 1docker run -d --name zookeeper -p 2181:2181 -t zookeeper:3.5.6 1docker run -d --name kafka --publish 9092:9092 --link zookeeper --env KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 --env KAFKA_ADVERTISED_HOST_NAME=localhost --env KAFKA_ADVERTISED_PORT=9092 wurstmeister/kafka:latest 可通过docker ps查看启动状态 测试 docker ps找到container id，进入容器 12# docker exec -it $&#123;CONTAINER NAME&#125; /bin/bashdocker exec -it kafka /bin/bash 进入kafka默认目录 1cd /opt/kafka_2.12-2.4.1/bin 创建主题 1bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic test 1bin/kafka-topics.sh --list --bootstrap-server localhost:9092 发送消息 1bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test 消费消息 1bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning 多个kafka节点 1bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 3 --partitions 1 --topic my-replicated-topic1 1docker run -d --name kafka -p 9092:9092 -e KAFKA_BROKER_ID=0 -e KAFKA_ZOOKEEPER_CONNECT=192.168.199.246:2181 -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://192.168.199.246:9092 -e KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092 -t wurstmeister/kafka 1docker run -d --name kafka1 -p 9093:9093 -e KAFKA_BROKER_ID=1 -e KAFKA_ZOOKEEPER_CONNECT=192.168.199.246:2181 -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://192.168.199.246:9093 -e KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9093 -t wurstmeister/kafka 1docker run -d --name kafka2 -p 9094:9094 -e KAFKA_BROKER_ID=2 -e KAFKA_ZOOKEEPER_CONNECT=192.168.199.246:2181 -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://192.168.199.246:9094 -e KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9094 -t wurstmeister/kafka 删除主题1bin/kafka-topics.sh --delete --topic my-replicated-topic --bootstrap-server localhost:9092 查看主题 1bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic 1bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test1 1bin/kafka-topics.sh --describe --zookeeper 192.168.199.246:2181 --topic test1 参考文章 在Docker环境下部署Kafka Mac 使用 docker 搭建 kafka 集群 + Zookeeper + kafka-manager apache kafka quickstart","tags":[]},{"title":"Java-JUC","date":"2020-04-01T18:15:34.000Z","path":"2020/04/02/Java-JUC/","text":"Java juc源码笔记合集[TOC] 这是一个Java juc源码笔记合集。 队列同步器AQSAbstractQueuedSynchronizer抽象队列同步器。 Lock接口提供加锁、解锁的统一入口，是面向锁使用者的。 自定义锁实现时，实现Lock接口，内部类继承AQS，实现AQS的tryAcquire/tryRelease（或tryAcquireShared/tryReleaseShared），完成具体的锁获取逻辑，比如是否公平、对同步状态的变更等，是面向锁开发者的。 AQS封装了排队（什么时候阻塞、什么时候去获取锁）的逻辑。 公平与非公平公平与非公平，是针对非排队的线程而言的。 对排队中的线程，都是公平的，需要按照队列顺序，当前驱节点是head时，才有资格tryAcquire，即尝试获取锁。 非公平是指，在队列中的线程刚被唤醒、还没来得及获取到锁时，不在排队中的、其他的线程可能先获取到了锁。以ReentrantLock的nonfaireSync为例，只要当前线程看到state为0、且cas成为1，当前线程就获取到了锁。 acquire流程acquire是AQS提供给Lock的加锁入口。 ConditionAQS-同步队列 sync queue condition-条件队列 CountDownLatch怎么用？ 首先设置一个状态n，latch被countDown1次，状态就自减1。状态为0，表示”栅栏“。 假设多个线程要到同一个状态前再同时开始工作，那么可以调用latch.await()，等latch状态为0，就会被唤醒同时工作。","tags":[]},{"title":"kafka学习笔记","date":"2020-01-08T18:31:02.000Z","path":"2020/01/09/kafka学习笔记/","text":"深入理解Kafka——核心设计与实践原理 朱忠华 第4章 主题与分区主题作为消息的归类，可以再细分为一个或多个分区，分区可看做对消息的二次归类。 分区为kafka提供了可伸缩性、水平扩展的功能，还通过副本机制提供数据冗余来提高数据可靠性。 主题和分区都是逻辑上的概念。分区有一个至多个副本，每个副本对应一个日志文件，每个日志文件对应一个或多个日志分段（LogSegment），每个日志分段细分为索引文件、日志存储文件和快照文件等。 4.1 主题的管理1bin/kafka-topics.sh --zookeeper locakhost:2181/kafka --create --topic topic-create --partitions 4 --replication-factor 2 （启动三个broker/节点node的集群后）创建分区数为4、副本因子为2的主题，会在3个节点上共创建8个文件夹=分区数4*副本因子2 副本与日志一一对应，每个副本对应一个命名形式如&lt;topic&gt;-&lt;partition&gt;的文件夹 同一个分区的多个副本必须分布在不同的broker中，才能提供有效的数据冗余 4.2 初识KafkaAdminClient4.3 分区的管理只有leader副本对外提供读写服务，follower副本只负责在内部进行消息的同步。 第5章 日志存储5.1 文件目录布局物理上的形式： 日志（对应于一个副本）Log是一个文件夹 日志段LogSegment是一个日志文件和两个索引文件，以及可能的其他文件（比如以“.txnindex”为后缀的事务索引文件） 5.2 日志格式的演变### v1比v0在RECORD中加了timestamp字段，占8B(字节) v0 V1 LOG_OVERHEAD日志头部 12B 12B RECORD_OVERHEAD_V0 消息最小长度 14B RECORD_OVERHEAD_V1 22B 5.3 日志索引5.3.1 偏移量索引（1）relativeOffset：相对偏移量，相对于baseOffset（索引文件文件名）的偏移量，4个字节 （2）position：物理地址，消息在日志分段文件中对应的物理地址，4个字节 查找时，先通过二分法在偏移量索引中找到不大于给定偏移量的最大索引项，再从对应的日志分段的物理位置开始顺序查找给定偏移量对应的消息。 用跳跃表的结构，查找对应的偏移量索引文件。 5.3.2时间戳索引每个索引项12字节 （1）timestamp：当前日志分段最大的时间戳。—–没太理解？ （2）relativeOffset：时间戳所对应的消息的相对偏移量。 时间戳索引文件中包含若干时间戳索引项，每个追加的时间戳索引项中的timestamp 必须大于之前追加的索引项的 timestamp，否则不予追加。 这部分没太看懂 = = 5.4 日志清理5.4.1 日志删除有专门的日志删除任务，定期（默认为5分钟，300000ms）检测和删除不符合保留条件的日志分段文件 3种保留策略： 基于时间：默认情况下日志分段文件的保留时间为7天 基于日志大小 基于日志起始偏移量 5.4.2 日志压缩LogCompaction对于有相同key的不同value值，只保留最后一个版本。 如果应用只关心key对应的最新value值，则可以开启Kafka的日志清理功能。 5.5 磁盘存储顺序写盘的速度比随机写盘快，也比随机写内存快。 5.5.1 页缓存 当一个进程准备读取磁盘上的文件内容时，操作系统会先查看待读取的数据所在的页（page）是否在页缓存（pagecache）中，如果存在（命中）则直接返回数据，从而避免了对物理磁盘的 I/O 操作；如果没有命中，则操作系统会向磁盘发起读取请求并将读取的数据页存入页缓存，之后再将数据返回给进程。同样，如果一个进程需要将数据写入磁盘，那么操作系统也会检测数据对应的页是否在页缓存中，如果不存在，则会先在页缓存中添加相应的页，最后将数据写入对应的页。被修改过后的页也就变成了脏页，操作系统会在合适的时间把脏页中的数据写入磁盘，以保持数据的一致性。 使用页缓存的原因： 对象的内存开销非常大，通常是实际数据的几倍或者更多，空间使用率低； Java垃圾回收会随着堆数据的增多而越来越慢。 因此，使用文件系统并依赖于页缓存的做法优于维护一个进程内缓存或者其他结构： 可以省去进程内的缓存消耗 可以通过结构紧凑的字节码（比之对象）来节省空间 即使服务器重启，页缓存还是有效的，无需重建（进程内缓存需要重建） 整体上简化逻辑，将维护页缓存和文件之间的一致交由操作系统来负责，比进程内维护更加安全有效 5.5.2 磁盘I/O流程 4种IO调度策略： NOOP: no operation, FIFO队列+相邻IO请求合并 CFQ：Completely Fair Queuing，按照IO请求的地址进行排序。先来的io请求可能会被“饿死”。 DEADLINE：在CFQ基础上解决饿死情况，分别为读IO和写IO提供FIFO队列，有等待时长（读500ms，写5s）和优先级区别，FIFO(read)&gt;fifo(write)&gt;cfq ANTICIPATORY：在DEADLINE的基础上，为每个读I/O都设置了6ms的等待时间窗口。如果在6ms内OS收到了相邻位置的读I/O请求，就可以立即满足 从文件系统层面分析，Kafka 操作的都是普通文件，并没有依赖于特定的文件系统，但是依然推荐使用EXT4或XFS。 5.5.3 零拷贝 所谓的零拷贝是指将数据直接从磁盘文件复制到网卡设备中，而不需要经由应用程序之手。 零拷贝技术通过DMA（Direct Memory Access）技术将文件内容复制到内核模式下的Read Buffer 中。不过没有数据被复制到 Socket Buffer，相反只有包含数据的位置和长度的信息的文件描述符被加到Socket Buffer中。DMA引擎直接将数据从内核模式中传递到网卡设备（协议引擎）。 零拷贝是针对内核模式而言的，数据在内核模式下实现了零拷贝。 第6章 深入服务端6.1 协议设计Apache Kafka官网：https://kafka.apache.org/intro 中文官网：http://kafka.apachecn.org/intro.html 介绍Apache kafka是一个分布式流处理平台。 特性： 发布订阅流式记录，类似于消息队列或企业级消息系统。 存储流式的记录，有较好容错性。 在流式记录产生时就可以进行处理。 应用： 构造实时流数据管道（消息队列） 构建实时流式应用程序，转换或响应流数据（流处理，kafka stream topic和topic—流处理不太了解） 概念： kafka运行在集群（一个或多个服务器）上，可跨越多个数据中心 kafka集群通过topic对消息分类 每条消息包含一个key，一个value，一个timestamp 四个核心api producer api 允许发布一串流式的数据到一个或多个主题 Consumer api 允许应用程序订阅一个或多个主题，并对发布给他们的流式数据进行处理 streams api 允许一个应用程序作为流处理器，消费一个或多个主题产生的输入流，然后生产一个输出流到一个或多个主题中去。在输入输出流中进行有效的转换。 connector api–不太了解 原文：允许构建并运行可重用的生产者或者消费者，将Kafka topics连接到已存在的应用程序或者数据系统。比如，连接到一个关系型数据库，捕捉表（table）的所有变更内容。 Topics和日志kafka中的topics总是多订阅者模式，一个topic可以有一个或多个消费者来订阅数据。 对每个主题，kafka都会维护一个分区日志： 每一个topic有多个分区，每个分区都是有序且顺序不可变的记录集。 分区表示记录顺序的标号叫offset（偏移量），唯一标识分区中的每一条记录。 记录不会因为被消费过就被集群删掉，而是通过保留期限这个参数控制的。保留期限内可随时被消费，过期后会被抛弃并释放磁盘空间。kafka的性能在数据大小方面是稳定的，所以长时间存储数据不是问题。 另一反面，消费者唯一保持的元数据是offset，即消费在log中的位置，由消费者自己控制。（可以任意修改offset的值，消费已消费的、或者跳过某些消息） 日志分区的作用： 当日志大小超过单台服务器的限制，允许日志进行扩展。（可以处理无限量的数据） 可以作为并行的单元集 Apache Kafka工作原理介绍–博客原文地址：https://www.ibm.com/developerworks/cn/opensource/os-cn-kafka/index.htmlhttps://www.ibm.com/developerworks/cn/opensource/os-cn-kafka/index.html 术语Broker: kafka集群包含一个或多个服务器，服务器称为broker Topic: 每条发布到kafka集群的消息都有一个类别，这个类别称为topic（物理上可能分开存储，用户不必关心） Partition：物理上的概念，每个topic包含一个或多个Partition Producer：生产者，发布消息到broker Consumer：消费者，从broker读取消息 Consumer Group：每个Consumer属于一个特定的组（可为每个Consumer指定group name，不指定group name则属于默认的group） kafka交互流程 每个消息只会发送给群组中的一个消费者，有相同键值的消息都会被确保发给这一个消费者。 问题 Consumer group做什么用？为什么要设计组这个概念？","tags":[]},{"title":"复习规划","date":"2020-01-08T17:49:28.000Z","path":"2020/01/09/规划/","text":"整理的问题都准备下回答 还有不熟悉的内容 spring beanDefinitions的解析 mybatis和事务 kafka","tags":[]},{"title":"读书笔记-Designing Data-Intensive Applications","date":"2019-12-18T17:57:24.000Z","path":"2019/12/19/读书笔记-Designing-Data-Intensive-Applications/","text":"Part II. Distributed Datareasons to distribute database accross multiple machines Scalability Fault tolerance/ hive avaliability Latency 第5章 复制领导者与追随者 同步复制与异步复制 设置新从库","tags":[]},{"title":"读书笔记-分布式系统常用技术及案例分析","date":"2019-12-18T16:11:18.000Z","path":"2019/12/19/读书笔记-分布式系统常用技术及案例分析/","text":"","tags":[]},{"title":"架构学习笔记","date":"2019-12-13T14:12:33.000Z","path":"2019/12/13/架构学习笔记/","text":"【以下都是原文摘录。著作权归原作者所有。更多内容请看原文。】 系统架构第三篇之架构(上) - gaofla的文章 - 知乎 https://zhuanlan.zhihu.com/p/50290236 架构一般分为：业务架构、技术架构、应用架构、部署架构，不同的架构关注面不一样。 业务架构：业务提供的能力，业务能做什么； 技术架构：技术领域解决问题，如存储、计算等； 应用架构：应用之间的关系； 部署架构：应用实际是怎样部署的，不同机房等。 那么问题来了，架子是怎么出来的呢？还是回到前面两篇文章中，系统架构前面有系统二字，那么具有系统的特性，系统是分层，所以架子的导出按照之前提到的三把斧：分解、抽象、层次特性。 软件架构之分离关注点 - gaofla的文章 - 知乎 https://zhuanlan.zhihu.com/p/54625074 “关注点=职责”—这句是我加的 分离关注点是我们经常听到的一个词汇，一说到分离关注点一般情况下有两种场景立即浮现在我们的眼前：一是分层；二是面向接口编程。这两个都只是分离关注点的具体实现，但并不是分离关注点的本质思想。分离关注点本身包含三层含义：一是如何分离； 二是关注点是什么；三是关注点如何实现的。 二、关注点是什么 从分层出发，往上抽象，就是4个字：单一职责。 我们在学习面向对象时，首先会讲到封装，讲封装最低层次就是讲封装私密性(数据保护)；中层次就是对象功能的封装；高层次就是封装的关注点有哪些。面向对象的三大特性继承和多态都是为了封装服务的。 三、如何分离关注点 小到一个函数，大到一个类，再或者是一个包，甚至更大的是一个层，都可以看作是一个关注点，关注点常见划分的手段有两种：功能(职责)和业务语义。平时说的边界也是在分离各自己的关注点，划分边界也是体现了单一职责。 业务语义在领域建模中经常使用到，根据业务语义进行拆分，不同的对象放在不同的域内，如有订单域、商品域、交易域、结算域等等，它们的业务含义是不一样的。 系统架构设计（通用型），推荐给苦于写文档的同学们，干货分享！ - 摆渡人的文章 - 知乎 https://zhuanlan.zhihu.com/p/36989243 阿里巴巴的技术专家，是如何画好架构图的？ - 芋道源码的文章 - 知乎 https://zhuanlan.zhihu.com/p/64881017 4+1视图，分别为场景视图、逻辑视图、物理视图、处理流程视图和开发视图 场景视图用于描述系统的参与者与功能用例间的关系，反映系统的最终需求和交互设计，通常由用例图表示。 逻辑视图用于描述系统软件功能拆解后的组件关系，组件约束和边界，反映系统整体组成与系 统如何构建的过程,通常由UML的组件图和类图来表示。 物理视图用于描述系统软件到物理硬件的映射关系，反映出系统的组件是如何部署到一组可 计算机器节点上，用于指导软件系统的部署实施过程。 处理流程视图用于描述系统软件组件之间的通信时序，数据的输入输出，反映系统的功能流程 与数据流程,通常由时序图和流程图表示。 开发视图用于描述系统的模块划分和组成，以及细化到内部包的组成设计，服务于开发人员，反映系统开发实施过程。 知乎的技术架构是什么样的？ - 姚钢强的回答 - 知乎 https://www.zhihu.com/question/314356555/answer/625772570 简述下知乎在线部分的技术架构，先看下整体的架构图，整体比较清晰不再赘述。 偏流程的图求一张MongoDB的系统架构图？ - Michael wang的回答 - 知乎 https://www.zhihu.com/question/29052377/answer/43378275 作者：Michael wang链接：https://www.zhihu.com/question/29052377/answer/43378275来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 在我的笔记中找了找，发现了几副，比较老了，对付看吧。 sharding相关的图 Replication相关的 ! 在最新的3.0中增加了很多新东西，值得继续跟进。 业务架构图的例子，仅供参考零售系统架构图 - summer86的文章 - 知乎 https://zhuanlan.zhihu.com/p/80270660 产品架构图到底是怎么“画”出来的？ - 杜松的文章 - 知乎 https://zhuanlan.zhihu.com/p/67038314","tags":[]},{"title":"读书笔记【持续演进的Cloud Native：云原生架构下微服务最佳实践】","date":"2019-11-25T11:58:04.000Z","path":"2019/11/25/读书笔记【持续演进的Cloud-Native：云原生架构下微服务最佳实践】/","text":"摘录DevOps简介原文地址：https://www.cnblogs.com/liufei1983/p/7152013.html 实现DevOps需要什么？ 硬性要求：工具上的准备 上文提到了工具链的打通，那么工具自然就需要做好准备。现将工具类型及对应的不完全列举整理如下： 代码管理（SCM）：GitHub、GitLab、BitBucket、SubVersion 构建工具：Ant、Gradle、maven 自动部署：Capistrano、CodeDeploy 持续集成（CI）：Bamboo、Hudson、Jenkins 配置管理：Ansible、Chef、Puppet、SaltStack、ScriptRock GuardRail 容器：Docker、LXC、第三方厂商如AWS 编排：Kubernetes、Core、Apache Mesos、DC/OS 服务注册与发现：Zookeeper、etcd、Consul 脚本语言：python、ruby、shell 日志管理：ELK、Logentries 系统监控：Datadog、Graphite、Icinga、Nagios 性能监控：AppDynamics、New Relic、Splunk 压力测试：JMeter、Blaze Meter、loader.io 预警：PagerDuty、pingdom、厂商自带如AWS SNS HTTP加速器：Varnish 消息总线：ActiveMQ、SQS 应用服务器：Tomcat、JBoss Web服务器：Apache、Nginx、IIS 数据库：MySQL、Oracle、PostgreSQL等关系型数据库；cassandra、mongoDB、redis等NoSQL数据库 项目管理（PM）：Jira、Asana、Taiga、Trello、Basecamp、Pivotal Tracker 在工具的选择上，需要结合公司业务需求和技术团队情况而定。（注：更多关于工具的详细介绍可以参见此文：51 Best DevOps Tools for #DevOps Engineers） 概念CNCF（Cloud Native Computing Foundation）云原生计算基金会 Istio 2017.05.24 Google&amp;IBM 第二代Service Mesh conduit 201712.05 Buoyant 第二代Service Mesh DevOps is a set of practices that combines software development and information-technology operations which aims to shorten the systems development life cycle and provide continuous delivery with high software quality. DevOps 是一个完整的面向IT运维的工作流，以 IT 自动化以及持续集成（CI）、持续部署（CD）为基础，来优化程式开发、测试、系统运维等所有环节。 Service Mesh 服务网格，服务间通信的基础设施层。可以将它比作是应用程序或者说微服务间的 TCP/IP，负责服务之间的网络调用、限流、熔断和监控。 Serviceless Serverless不代表再也不需要服务器了，而是说：开发者再也不用过多考虑服务器的问题，计算资源作为服务而不是服务器的概念出现。 Serverless是一种构建和管理基于微服务架构的完整流程，允许在服务部署级别而不是服务器部署级别来管理应用部署，甚至可以管理某个具体功能或端口的部署，这就能让开发者快速迭代，更快速地开发软件。 Sofa SOFAStack™（Scalable Open Financial Architecture Stack）是一套用于快速构建金融级分布式架构的中间件，也是在金融场景里锤炼出来的最佳实践。","tags":[]},{"title":"计算机网络学习笔记","date":"2019-11-12T19:29:29.000Z","path":"2019/11/13/计算机网络学习笔记/","text":"零、重点 概述的【五层协议】 一、概述 网络把主机联系起来，互联网把网络联系起来。 电路交换与分组交换 电路交换用于电话通信系统，通信过程中占用整个信道（物理链路），利用率低 分组交换 每个分组都有首部和尾部，包含了源地址和目的地址等控制信息，在同一个传输线路上同时传输多个分组互相不会影响，因此在同一条传输线路上允许同时传输多个分组，也就是说分组交换不需要占用传输线路。在一个邮局通信系统中，邮局收到一份邮件之后，先存储下来，然后把相同目的地的邮件一起转发到下一个目的地，这个过程就是存储转发过程，分组交换也使用了存储转发过程。 时延 总时延 = 排队时延 + 处理时延 + 传输时延 + 传播时延 排队时延：在路由器的输入、输出队列等待 处理时延 ：主机或路由器处理分组的时间，例如分析首部、从分组中提取数据、进行差错检验或查找适当的路由等。 传输时延：主机或路由器传输数据帧所需要的时间，其中 l 表示数据帧的长度，v 表示传输速率。 传播时延：电磁波在信道中传播所需要花费的时间，电磁波传播的速度接近光速。 其中 l 表示信道长度，v 表示电磁波在信道上的传播速度。 计算机网络体系结构 网络协议 为进行网络中的数据交换而建立的规则、标准或约定称为网络协议（network protocol） 要素 语法（数据与控制信息的格式） 语义（需要发出何种控制信息，完成何种动作以及做出何种响应） 同步（事件实现顺序的说明） 五层协议 应用层：为应用进程提供数据传输服务。有http、DNS等协议，数据单位为报文。 传输层：为进程提供数据传输服务。包括两种协议，传输控制协议TCP，提供面向连接、可靠的shu句传输服务，数据单位为报文段；用户数据报协议UDP，提供无连接、尽最大努力的数据传输服务，数据单位为用户数据报 。 网络层：为主机提供数据传输服务。网络层把传输层传递下来的报文段或者用户数据报封装成分组。 数据链路层：网络层针对的还是主机之间的数据传输服务，而主机之间可以有很多链路，链路层协议就是为同 一链路的主机提供数据传输服务。数据链路层把网络层传下来的分组封装成帧。 物理层：考虑的是怎样在传输媒体上传输数据比特流，而不是指具体的传输媒体。物理层的作用是尽可能屏蔽传输媒体和通信手段的差异，使数据链路层感觉不到这些差异。 OSI ：开放系统互联基本参考模型OSI/RM (Open System Interconnection Reference Model) 表示层：数据压缩、加密以及数据描述，这使得应用程序不必关心在各台主机中数据内部格式不同的问题。 会话层：建立及管理会话。 五层协议没有表示层和会话层，而是将这些功能留给应用程序开发者处理。 TCP/IP 它只有四层，相当于五层协议中数据链路层和物理层合并为网络接口层。 TCP/IP 体系结构不严格遵循 OSI 分层概念，应用层可能会直接使用 IP 层或者网络接口层。 数据在各层之间的传递过程 在向下的过程中，需要添加下层协议所需要的首部或者尾部，而在向上的过程中不断拆开首部和尾部。 路由器只有下面三层协议，因为路由器位于网络核心中，不需要为进程或者应用程序提供服务，因此也就不需要传输层和应用层。 二、物理层 三、链路层 四、网络层 五、传输层拥塞控制 慢开始与拥塞避免 cwnd: congestion window 拥塞窗口 ssthresh: slow start threshold 慢开始门限 快重传与快恢复","tags":[]},{"title":"Service中使用@Validated注解","date":"2019-11-06T16:32:52.000Z","path":"2019/11/07/Service中使用-Validated注解/","text":"网上找到的资料大多介绍在controller中的使用，在工作中呢，往往需要在service中做校验，便想着是否可以实现呢。 顺着@Validated在controller中起作用的源码，可以发现校验的工作是交给了LocalValidatorFactoryBean，是spring做的封装接口，实现类是hibernate的ValidatorImpl. 也可以直接用hibernate的ValidatorImpl的，单纯的进行validate. 不严谨的性能比较： 比手工校验不用说是比较慢了 盲猜有部分缓存实现，validate的第一次校验比较慢，之后的校验触发比较快，大约是100倍 比较匆忙，都是相同的入参出参、同一个方法，没有更多横向比较 在配置文件中注入bean： 1234&lt;bean id=\"validator\" class=\"org.springframework.validation.beanvalidation.LocalValidatorFactoryBean\"&gt; &lt;property name=\"providerClass\" value=\"org.hibernate.validator.HibernateValidator\" /&gt; &lt;/bean&gt; 业务代码示例： 1234567891011121314151617181920212223242526272829303132333435@Slf4j@Servicepublic class UserServiceImpl implements UserService &#123; @Autowired private LocalValidatorFactoryBean localValidatorFactoryBean; @Override public Boolean add(User user) &#123; StopWatch stopWatch = new StopWatch(); stopWatch.start(); DataBinder dataBinder = new DataBinder(user); dataBinder.addValidators(localValidatorFactoryBean);// dataBinder.validate(); dataBinder.validate(User.Insert.class); BindingResult bindingResult = dataBinder.getBindingResult(); log.info(\"validate(), error size = &#123;&#125;\", bindingResult.getAllErrors().size()); stopWatch.stop(); log.info(\"cost time = &#123;&#125;\", stopWatch.getLastTaskTimeNanos());// if (bindingResult.hasErrors()) &#123;// log.info(\"errors = &#123;&#125;\", bindingResult.getAllErrors());// throw new IllegalArgumentException(bindingResult.getAllErrors().get(0).getDefaultMessage());// &#125; try &#123; stopWatch.start(); String name = user.getName(); Assert.notNull(name, \"not null\"); Integer age = user.getAge(); Assert.notNull(age, \"年龄不能为空\"); &#125; catch (RuntimeException e) &#123; stopWatch.stop(); log.info(\"cost time 2 = &#123;&#125;\", stopWatch.getLastTaskTimeNanos()); &#125; return user != null; &#125;&#125;","tags":[]},{"title":"mybatis学习笔记","date":"2019-10-28T17:43:17.000Z","path":"2019/10/29/mybatis学习笔记/","text":"MyBatis框架架构图 参考 [MyBatis的工作原理以及核心流程介绍]: http://www.mybatis.cn/archives/706.html [mybatis原理，配置介绍及源码分析]: https://juejin.im/post/5bd02b3d6fb9a05cee1e2478 [mybatis官网]: https://mybatis.org/mybatis-3/zh/configuration.html","tags":[]},{"title":"spring-mvc笔记","date":"2019-10-25T16:55:41.000Z","path":"2019/10/26/spring-mvc笔记/","text":"request处理与找到methodgetHandler返回需要的method，在这里就是UserController的query方法。 入参和出参都是在进入method invoke后才处理的 真正执行数据绑定的地方 invoke与response处理在反射执行完controller中的方法后，通过selectHandler找到匹配的处理返回结果的Processor。例如返回结果是json或xml格式的，都是在这个processor中。和@ResponseBody的数据绑定是同一个。","tags":[]},{"title":"设计模式笔记","date":"2019-10-24T17:20:04.000Z","path":"2019/10/25/设计模式/","text":"更多细节见原文：设计模式 UML类图 依赖关系 Dependency：耦合度最弱。见于方法里的局部变量、方法的入参、对静态方法的调用。 关联关系 Association：对象之间的引用关系，表示一类对象与另一类对象之间的联系，比如老师与学生。 聚合关系 Aggregation：关联关系的一种，强关联。部分和整体可以独立存在。 组合关系 Composition：关联关系的一种，是一种更强烈的聚合关系。部分依赖于整体存在。 泛化关系 Generalization：父类与子类的关系。（父类是子类的泛化。。） 实现关系 Realization：接口与实现类之间的关系。 7条开发原则 开闭原则（Open Closed Principle，OCP） 软件实体应当对扩展开放，对修改关闭（Software entities should be open for extension，but closed for modification） 里氏替换原则（Liskov Substitution Principle，LSP） 继承必须确保超类所拥有的性质在子类中仍然成立（Inheritance should ensure that any property proved about supertype objects also holds for subtype objects） 也就是，子类可以扩展父类的功能，但不能改变父类原有的功能。 如果违背，就需要重新设计类之间的关系，比如定义更一般的父类。 依赖倒置原则（Dependence Inversion Principle，DIP） 高层模块不应该依赖低层模块，两者都应该依赖其抽象；抽象不应该依赖细节，细节应该依赖抽象（High level modules shouldnot depend upon low level modules.Both should depend upon abstractions.Abstractions should not depend upon details. Details should depend upon abstractions） 核心思想是，面向接口编程，不要面向实现。 主要作用是降低类之间的耦合性。 单一职责原则（Single Responsibility Principle） 接口隔离原则：拆分成更小、更具体的接口 迪米特法则：不和陌生人说话 合成复用原则：多用组合，少用继承 创建型设计模式 单例模式 原型模式 原型（Prototype）模式的定义如下：用一个已经创建的实例作为原型，通过复制该原型对象来创建一个和原型相同或相似的新对象。 例如Java对象的clone() 工厂方法模式 抽象工厂模式 建造者模式","tags":[]},{"title":"spring design pattern","date":"2019-10-23T19:03:20.000Z","path":"2019/10/24/spring-design-pattern/","text":"设计模式简单工厂模式（非GoF）简单工厂是指，只用一个具体工厂就可以完成所有产品的生产。 比如BeanFactory生产出各种类型的bean。 通俗的说，就是创建产品的接口里，if else判断要哪种类型的产品，就创建哪种类型的产品。 BeanFactory是用一个map维护了产品和产品的类型。 ps：简单工厂不属于GoF，原因是会破坏开闭原则，说来就是增加产品时需要增加if判断。但用map和自动的类型注册，就不会破坏啦。也是很好用的模式。 ##工厂方法模式 FactoryBean spring中，当bean的初始化比较复杂，有大量工作要做时使用。 一个抽象工厂有多个具体工厂，对应着抽象产品的多个具体产品创建。 缺点，也是特点，每多一种产品，就要加一个具体工厂。 工厂方法和简单工厂如何区分呢？ 删掉抽象工厂，只用具体工厂就能生产所有产品，这时就可退化成简单工厂。 工厂方法一般是有一个抽象工厂，多个具体工厂的。 抽象工厂模式抽象工厂是工厂方法的升级版，工厂提供多类产品的创建，就是说，抽象产品有多个。 新增一个具体工厂时，不修改原代码，满足开闭原则。 要新增一类产品时，就要在每个工厂里增加创建这类产品的接口，不满足开闭原则。 抽象工厂模式（详解版） 抽象工厂模式最早的应用是用于创建属于不同操作系统的视窗构件。如 java 的 AWT 中的 Button 和 Text 等构件在 Windows 和 UNIX 中的本地实现是不同的。 抽象工厂和工厂方法又怎么区分呢？ 工厂方法提供一类/同类产品的生产，抽象工厂提供多类产品。 Ps: Spring框架中的设计模式（一）这篇文章提到BeanFactory是抽象工厂，我理解不太对。BeanFactory是工厂方法，因为BeanFactory只提供了一类产品。 在BeanFactory的实现中，我们可以区分：ClassPathXmlApplicationContext，XmlWebApplicationContext，StaticWebApplicationContext，StaticPortletApplicationContext，GenericApplicationContext，StaticApplicationContext。 12345678910111213141516@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations=&#123;\"file:test-context.xml\"&#125;)public class TestProduct &#123; @Autowired private BeanFactory factory; @Test public void test() &#123; System.out.println(\"Concrete factory is: \"+factory.getClass()); assertTrue(\"Factory can't be null\", factory != null); ShoppingCart cart = (ShoppingCart) factory.getBean(\"shoppingCart\"); assertTrue(\"Shopping cart object can't be null\", cart != null); System.out.println(\"Found shopping cart bean:\"+cart.getClass()); &#125;&#125; 在这种情况下，抽象工厂由BeanFactory接口表示。具体工厂是在第一个System.out中打印的，是org.springframework.beans.factory.support.DefaultListableBeanFactory的实例。它的抽象产物是一个对象。在我们的例子中，具体的产品就是被强转为ShoppingCart实例的抽象产品（Object）。","tags":[]},{"title":"spring aop","date":"2019-10-19T15:20:42.000Z","path":"2019/10/19/spring-aop/","text":"疑问 aspectJ、cglib、asm分别是什么？有什么联系？ aspectJ和cglib是代理框架，asm是字节码框架。 cglib用到了asm。 spring aop或者其他的aop，有多个before、around、after时，需要定义先后顺序吗？如何实现？ order或者实现Order接口，定义顺序数值。值越小越先执行（越在同心圆外圈）。 spring的aspectJ，是spring aop代理？用的动态代理，还是cglib？ 用的动态代理。 spring只用到了aspectJ的注解，没用到aspectJ的各种织入。 稍稍了解下，cglib怎么修改字节码。 小结 spring aop 基于动态代理实现，在容器启动的时候生成代理实例（cglib起什么作用？） 作用于容器中的所有bean 提供了对asjectJ的支持（怎么理解）？ 沿用（使用）了AsjectJ @AsjectJ、@Pointcut等注解，但注解的实现是spring自己实现的一套 AspectJ是AOP编程的完全解决方案。 静态织入，通过修改代码实现 织入时机包括 编译期织入 compile-time weaving 编译后织入 post-compile weaving 已经生成.class文件了 加载时织入 load-time weaving 在加载类的时候织入，方式包括自定义加载类，或在JVM启动时指定AspectJ提供的agent -javaagent:xxx/xxx/aspectjweaver.jar Spring aop 源码配置DefaultAdvisorAutoProxyCreator的方式，使所有的advisor都生效。 DefaultAdvisorAutoProxyCreator继承了BeanPostProcessor，在每个bean实例化后被调用，doCreateBean..initializeBean..applyBeanPostProcessorsAfterInitialization(..)..调用每个BeanPostProcessor的postProcessAfterInitialization。 在DefaultAdvisorAutoProxyCreator（父类AbstractAutoProxyCreator）的postProcessAfterInitialization中，生成bean的代理，将这个代理返回，放在容器中。 1234567891011121314151617181920212223242526272829303132333435363738// AbstractAutowireCapableBeanFactory.javaprotected Object initializeBean(final String beanName, final Object bean, RootBeanDefinition mbd) &#123; ... Object wrappedBean = bean; if (mbd == null || !mbd.isSynthetic()) &#123; // 1. 执行每一个 BeanPostProcessor 的 postProcessBeforeInitialization 方法 wrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); &#125; try &#123; // 调用 bean 配置中的 init-method=\"xxx\" invokeInitMethods(beanName, wrappedBean, mbd); &#125; ... if (mbd == null || !mbd.isSynthetic()) &#123; // 我们关注的重点是这里！！！ // 2. 执行每一个 BeanPostProcessor 的 postProcessAfterInitialization 方法 wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); &#125; return wrappedBean;&#125; //AbstractAutoProxyCreator.java /** * Create a proxy with the configured interceptors if the bean is * identified as one to proxy by the subclass. * @see #getAdvicesAndAdvisorsForBean */ @Override public Object postProcessAfterInitialization(@Nullable Object bean, String beanName) throws BeansException &#123; if (bean != null) &#123; Object cacheKey = getCacheKey(bean.getClass(), beanName); if (!this.earlyProxyReferences.contains(cacheKey)) &#123; return wrapIfNecessary(bean, beanName, cacheKey); &#125; &#125; return bean; &#125; DefaultAopProxyFactory有两种默认实现方式，JDK动态代理和cglib代理。 设置optimize、proxyTargetClass或当前类没有实现接口是，使用cglib代理（如果类本身是接口、或是代理类，也使用jdk动态代理）。 其他情况下用JDK动态代理。 一般情况下，指定proxyTargetClass来使用cglib，指定一个或多个接口来使用JDK代理。 jdk代理原理基于反射机制。 JDK利用反射机制生成一个实现代理接口的匿名类，然后重写方法，实现方法的增强。 生成匿名类很快，但后续调用栈比较深，执行速度会相对慢一些。 流程： 为接口创建代理类的字节码文件（重写的方法里调用了InvokeHandler的invoke方法） 使用ClassLoader加载类文件进JVM 附录1给出了一个反编译出来的匿名类的代码。 代码123456789101112131415161718192021// JdkDynamicAopProxy.java// JdkDynamicAopProxy也是一个InvocationHandler，实现了invoke方法，在调用到代理时，执行的就是invoke方法// invoke中实现了对advisor的调用@Overridepublic Object getProxy(@Nullable ClassLoader classLoader) &#123; if (logger.isDebugEnabled()) &#123; logger.debug(\"Creating JDK dynamic proxy: target source is \" + this.advised.getTargetSource()); &#125; Class&lt;?&gt;[] proxiedInterfaces = AopProxyUtils.completeProxiedInterfaces(this.advised, true); findDefinedEqualsAndHashCodeMethods(proxiedInterfaces); return Proxy.newProxyInstance(classLoader, proxiedInterfaces, this);&#125;// Proxy.java@CallerSensitivepublic static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) &#123; // ... return newProxyInstance(caller, cons, h);&#125; 生成的JDK动态代理实例如图中所式示 是一个Proxy类型的实例，有一个名称为h的成员变量，h为JdkDynamicAopProxy类型（父类未InvocationHandler），这里都和jdk动态代理是一致的 JdkDynamicAopProxy主要有advised变量，内部有advisors（理解为增强方法）和targetSource（封装了被代理实例） 调用proxy时，实际上是调用了InvocationHandler的invoke方法，简化了说，先执行advisors的方法，再反射调用method.invoke(target, args)，或是链式执行。 cglib代理原理cglib基于继承机制，继承被代理类，通过字节码重写，增强父类的方法。 底层基于asm第三方框架，加载被代理类的class文件，修改字节码生成子类。 可以用于接口，也可以用于类。 设计模式代理模式 责任链模式 切面执行顺序更确切的说，是advisor的执行顺序。 每个@Aspect类，根据@Before、@After可生成相应个数的advisor（也有order值）。 可为Aspect指定order，order值越小，越先执行。 内容来自：基于AspectJ的Spring AOP Advice执行顺序 参考文档:aop-ataspectj-advice-ordering When two pieces of advice defined in different aspects both need to run at the same join point, unless you specify otherwise the order of execution is undefined. You can control the order of execution by specifying precedence. This is done in the normal Spring way by either implementing the org.springframework.core.Ordered interface in the aspect class or annotating it with the Order annotation. Given two aspects, the aspect returning the lower value from Ordered.getValue() (or the annotation value) has the higher precedence. 上面的内容简单的说就是,当对于同一个Join Point有两个Advice定义在不同的Aspect中的时候,他们的执行顺序是根据Aspect类的@Order注解的值,或者通过实现Order并重写getValue方法的值来决定的.同时,Order的值越小,优先级越高. When two pieces of advice defined in the same aspect both need to run at the same join point, the ordering is undefined 当同一个Aspect中对同一个Join Point有两个Advice的话,这两个Advice的顺序是不固定的. AOP应用全局日志、异常、事务管理等 参考文献 aop源码分析 AspectJ使用介绍 spring aop 使用介绍 Spring AOP,AspectJ, CGLIB JDK动态代理实现原理 基于AspectJ的Spring AOP Advice执行顺序 附录反编译出的Proxy0成员变量是原接口的方法，在重写的相应的方法中被调用。 例如Method m1是原接口实例的equals方法，Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;equals&quot;, Class.forName(&quot;java.lang.Object&quot;))，在新的equals方法中被反射调用，super.h.invoke(this, m1, new Object[]{var1})).booleanValue();。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576//// Source code recreated from a .class file by IntelliJ IDEA// (powered by Fernflower decompiler)//package com.sun.proxy;import com.lnjecit.proxy.Subject;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;import java.lang.reflect.UndeclaredThrowableException;public final class $Proxy0 extends Proxy implements Subject &#123; private static Method m1; private static Method m3; private static Method m2; private static Method m0; public $Proxy0(InvocationHandler var1) throws &#123; super(var1); &#125; public final boolean equals(Object var1) throws &#123; try &#123; return ((Boolean)super.h.invoke(this, m1, new Object[]&#123;var1&#125;)).booleanValue(); &#125; catch (RuntimeException | Error var3) &#123; throw var3; &#125; catch (Throwable var4) &#123; throw new UndeclaredThrowableException(var4); &#125; &#125; public final void doSomething() throws &#123; try &#123; super.h.invoke(this, m3, (Object[])null); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; public final String toString() throws &#123; try &#123; return (String)super.h.invoke(this, m2, (Object[])null); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; public final int hashCode() throws &#123; try &#123; return ((Integer)super.h.invoke(this, m0, (Object[])null)).intValue(); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; static &#123; try &#123; m1 = Class.forName(\"java.lang.Object\").getMethod(\"equals\", Class.forName(\"java.lang.Object\")); m3 = Class.forName(\"com.lnjecit.proxy.Subject\").getMethod(\"doSomething\"); m2 = Class.forName(\"java.lang.Object\").getMethod(\"toString\"); m0 = Class.forName(\"java.lang.Object\").getMethod(\"hashCode\"); &#125; catch (NoSuchMethodException var2) &#123; throw new NoSuchMethodError(var2.getMessage()); &#125; catch (ClassNotFoundException var3) &#123; throw new NoClassDefFoundError(var3.getMessage()); &#125; &#125;&#125; 补充spring aop代理每个bean的流程小结 每个advisor是一个bean，连接了一个advice和一个pattern 在DefaultAdvisorAutoProxyCreator这个BeanPostProcessor中，warpIfNessary 在容器中查询出所有advisor，常用的是基于正则的advisorRegexpMethodPointcutAdvisor（还会对不同类型的advisor做一些处理，比如用DefaultPointcutAdvisor封装MethodInterceptor类型的advisor） 将bean与advisors放入proxyFactory，创建代理对象 这里缓存着@Aspect注解的类。它又是怎么生成的呢？遍历beanDefinitions，取className对应的Class，看有没有注解 bpp判断要不要代理：先获取候选advisor，对每个Advisor判断是不是当前bean需要的（通过正则等），如果都没有需要的Advisor，则不代理；否则生成代理。 获得xml配置的advisor比较古老的通过配置文件定义切面的方式。 可以看出，每个advisor连接了一个advice和一个pattern。advice是方法维度的，例如是一个MethodBeforeAdvice。 这里DefaultAdvisorAutoProxyCreator是一个BeanPostProcessor，在bean实例化后的回调方法中，触发代理的创建。 ps: DefaultAdvisorAutoProxyCreator与下面@Aspect要用的AnnotationAwareAspectJAutoProxyCreator同宗啊。 default方式的查找advisors，是直接通过获取Advisor类型的bean实现的。 从XML配置中可以看到，advisor实现了RegexpMethodPointcutAdvisor接口，是Advisor类型的。 获得@Aspect注解的advisor那通过@Aspect注解的切面是怎么组装的呢？ 首先，@Aspect注解的类需要在xml中配置成bean、或者加@Component注解等，能够托管在IOC容器中。 其次，需要开启@Aspect注解的自动解析。 AopNamespaceHandler会注册一个AspectJAutoProxyBeanDefinitionParser。【在什么时候使用到这个parser？】parser在parse方法中，注册了AnnotationAwareAspectJAutoProxyCreator.class（是SmartInstantiationAwareBeanPostProcessor的一个实现），就是这个类起着寻找并创建advisor的重要作用。从截图的2步骤开始。 12&lt;!--开启 @AspectJ 配置--&gt;&lt;aop:aspectj-autoproxy/&gt; 寻找并创建advisor的流程： 从beanFactory中取出所有的beanNames 对每一个beanName判断是否为aspect，其中一种判断就是看是否有Aspect注解，如截图步骤4. 如果是aspect，由ReflectiveAspectJAdvisorFactory生成类中的各advisors。 几种proxyCreator第四个没见过 beanName，就是在xml文件中直接写出哪些beanName是Advice，同时bean会实现MethodBeforeAdvice等接口。可能是比较古老的方式。","tags":[]},{"title":"Dubbo学习笔记","date":"2019-10-16T15:53:29.000Z","path":"2019/10/16/Dubbo学习笔记/","text":"架构图 dubbo关键功能 基于rpc的远程调用 容错与负载均衡 服务注册与发现 学习笔记《深度剖析ApacheDubbo核心技术内幕》翟陆续.2019.电子工业出版社 《深入理解Apache Dubbo与实战》诣极 林琳@著 第4章 Dubbo扩展点加载机制注解一、@SPI 在某个接口上加上@SPI注解后，表明该接口为可扩展接口。 @SPI的value是一个key，指定这个接口的默认实现是什么。 用户在配置文件中，可指定自己想要的实现。如Protocal可通过&lt;dubbo:protocol /&gt;指定（这样就不一定是用@SPI里的key对应的实现了）。 二、@Adaptive 保证dubbo在内部调用具体实现的时候不是硬编码来指定引用哪个实现，适配一个接口的多种实现 如果注解在方法上，动态获取键值对指定的实现，再执行实现的相应方法。编译时，为这个接口生成一个实现类（策略适配器），带有@Adaptive注解的方法里会增加一段类似策略选择的代码，比如Transporter接口的bind方法，将注解的server作为key去URL中找相应value，value值对应的类是本次调用需要的真正实现。 如果注解在类上，就以这个类作为接口的策略适配器。 可以理解为，@Adaptive标识或产生一个策略适配器。用户在调用接口时，传入key指定需要的实现是哪一个。 三、@Activate 自动激活带有这个注解的所有实现。 @SPI在接口上，@Adaptive在接口的方法或者一个类上，@Activate在实现类上。 ExtensionLoader每一个SPI接口，都有一个ExtensionLoader? 用来获取普通扩展类、自适应扩展类、自动激活的扩展类。 一般调用方是谁？—-要用到某个接口的实现的地方，可以获取指定的一个实现、自适应的实现（适配器）、或者多个实现（比如filter等）。 ExtensionLoaderFactory目测没有太大作用。在ExtensionLoader的injectExtension中有调用，依赖注入时利用factory去getExtension。 其实注入是用ExtensionLoader的getExtension也能实现相同功能，用factory是可以注入spring容器托管的bean。 dubbo也有自己的一个spi容器。 ExtensionLoaderFactory有三个实现，分别是adaptive、spi、spring. 参考：","tags":[]},{"title":"消息中间件","date":"2019-10-12T15:19:30.000Z","path":"2019/10/12/消息中间件/","text":"概述 ActiveMQ apache维护 rabbitMQ erlang语言，低时延 rocketmq 阿里开源，支持大量topic kafka 吞吐量高 Kafka在Kafka中实现消费的方式是将日志中的分区划分到每一个消费者实例上，以便在任何时间，每个实例都是分区唯一的消费者。 Kafka 只保证分区内的记录是有序的，而不保证主题中不同分区的顺序。 消费者组中的消费者实例个数不能超过分区的数量。","tags":[{"name":"消息中间件","slug":"消息中间件","permalink":"https://northernw.github.io/tags/消息中间件/"}]},{"title":"netty","date":"2019-10-09T15:58:49.000Z","path":"2019/10/09/netty/","text":"java nio buffer ByteBuffer position, limit, capacity, flip切换读写模式 channel 与buffer交互，读操作：将channel数据填充到buffer，channel.read(buffer)，写操作：将buffer数据写入channel，channel.write(buffer) selector 选择器or多路复用器 非阻塞 一个线程管理多个 Channel tomcat nio流程： 指定 Protocol，初始化相应的 Endpoint，我们分析的是 NioEndpoint； init 过程：在 NioEndpoint 中做 bind 操作； start 过程：启动 worker 线程池，启动 1 个 Acceptor 和 2 个 Poller，当然它们都是默认值，可配； Acceptor 获取到新的连接后，getPoller0() 获取其中一个 Poller，然后 register 到 Poller 中； Poller 循环 selector.select(xxx)，如果有通道 readable，那么在 processKey 中将其放到 worker 线程池中。","tags":[{"name":"nio","slug":"nio","permalink":"https://northernw.github.io/tags/nio/"},{"name":"netty","slug":"netty","permalink":"https://northernw.github.io/tags/netty/"},{"name":"通信","slug":"通信","permalink":"https://northernw.github.io/tags/通信/"}]},{"title":"待看内容","date":"2019-09-25T17:56:00.000Z","path":"2019/09/26/待看内容/","text":"优先 HashMap中，红黑树化时，next信息是否有变更？– 有变更 concurrentHashMap整理简短描述 AbstractQueuedSynchronizer 源码 解读 Java 并发队列 BlockingQueue 深入分析 Java 8 编程语言规范：Threads and Locks 深度解读 Java 线程池设计思想及源码实现 几位大牛博客 田小波 javadoop spring-ioc 非优先 Java 读写锁 ReentrantReadWriteLock 源码分析","tags":[]},{"title":"Java-Java容器","date":"2019-09-24T11:54:31.000Z","path":"2019/09/24/Java-Java容器/","text":"概览Collection List: ArrayList 基于动态数组（动态扩容），支持随机访问 Vector 线程安全 LinkedList 基于双向链表，只能顺序访问，但可快速插入和删除，还可用作栈、队列和双向链表 Set HashSet 哈希表实现，快速查找，不支持有序性操作 TreeSet 红黑树实现，支持有序性操作 LinkedHashSet 具有hashSet的查找效率，内部使用双向链表维护插入顺序 Queue LinkedList ，双向队列，双向链表实现 PriorityQueue 优先队列，堆实现 Map HashMap 基于哈希表实现 HashTable 线程安全，实现同HashMap TreeMap 红黑树实现 LinkedHashMap 哈希表实现，使用双向链表来维护元素的顺序，顺序为插入顺序或者最近最少使用(LRU)顺序 容器中的设计模式迭代器Collection 继承了 Iterable 接口，其中的 iterator() 方法能够产生一个 Iterator 对象，通过这个对象就可以迭代遍历Collection 中的元素。 适配器java.util.Arrays#asList() 可以把数组类型转换为 List 类型。 源码分析ArrayList概览ArrayList 是基于数组实现的，所以支持快速随机访问。RandomAccess 接口标识着该类支持快速随机访问。 数组默认大小是10. 扩容添加元素时使用 ensureCapacityInternal() 方法来保证容量足够，如果不够时，需要使用 grow() 方法进行扩容，新容量的大小为 oldCapacity + (oldCapacity &gt;&gt; 1) ，也就是旧容量的 1.5 倍。 扩容操作需要调用 Arrays.copyOf() 把原数组整个复制到新数组中，这个操作代价很高，因此最好在创建ArrayList 对象时就指定大概的容量大小，减少扩容操作的次数。 12345678910111213141516171819202122232425262728 public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125;private void ensureCapacityInternal(int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; ensureExplicitCapacity(minCapacity);&#125;private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) grow(minCapacity);&#125;private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125; 删除元素需要调用 System.arraycopy() 将 index+1 后面的元素都复制到 index 位置上，该操作的时间复杂度为 O(N)，可以看出 ArrayList 删除元素的代价是非常高的。 12345678910public E remove(int index) &#123; rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work return oldValue;&#125; Fail-FastmodCount 用来记录 ArrayList 结构发生变化的次数。结构发生变化是指添加或者删除至少一个元素的所有操作，或者是调整内部数组的大小，仅仅只是设置元素的值不算结构发生变化。 在进行序列化或者迭代等操作时，需要比较操作前后 modCount 是否改变，如果改变了需要抛出ConcurrentModificationException。 123456789101112131415private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException&#123; // Write out element count, and any hidden stuff int expectedModCount = modCount; s.defaultWriteObject(); // Write out size as capacity for behavioural compatibility with clone() s.writeInt(size); // Write out all elements in the proper order. for (int i=0; i&lt;size; i++) &#123; s.writeObject(elementData[i]); &#125; if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125; &#125; 序列化保存元素的数组 elementData 使用 transient 修饰，该关键字声明数组默认不会被序列化。 ArrayList 实现了 writeObject() 和 readObject() 来控制只序列化数组中有元素填充那部分内容。 使用 123ArrayList list = new ArrayList();ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(file));oos.writeObject(list); ### Vector与ArrayList实现相似，但用了synchronized进行同步。 与 ArrayList 的比较 Vector 是同步的，因此开销就比 ArrayList 要大，访问速度更慢。最好使用 ArrayList 而不是 Vector，因为同步 操作完全可以由程序员自己来控制; Vector 每次扩容请求其大小的 2 倍空间，而 ArrayList 是 1.5 倍。 替代方案 Collections.synchronizedList(); 得到一个线程安全的 ArrayList. 使用 concurrent 并发包下的 CopyOnWriteArrayList 类。 CopyOnWriteArrayList读写分离写操作在一个复制的数组上进行，读操作还是在原始数组中进行，读写分离，互不影响。写操作需要加锁，防止并发写入时导致写入数据丢失。写操作结束之后需要把原始数组指向新的复制数组。 适用场景 CopyOnWriteArrayList 在写操作的同时允许读操作，大大提高了读操作的性能，因此很适合读多写少的应用场景。 但是 CopyOnWriteArrayList 有其缺陷: 内存占用:在写操作时需要复制一个新的数组，使得内存占用为原来的两倍左右; 数据不一致:读操作不能读取实时性的数据，因为部分写操作的数据还未同步到读数组中。 所以 CopyOnWriteArrayList 不适合内存敏感以及对实时性要求很高的场景。 LinkedLsit基于双向链表实现，使用 Node 存储链表节点信息。 12345private static class Node&lt;E&gt; &#123; E item; Node&lt;E&gt; next; Node&lt;E&gt; prev;&#125; 每个链表存储了 first 和 last 指针: 12transient Node&lt;E&gt; first;transient Node&lt;E&gt; last; 与ArrayList比较 ArrayList 基于动态数组实现，LinkedList 基于双向链表实现; ArrayList 支持随机访问，LinkedList 不支持; LinkedList 在任意位置添加删除元素更快。 HashMap概览hashMap使用基于拉链的散列算法，jdk1.8起使用红黑树优化过长的链表。底层结构为数组+链表+红黑树。 源码分析构造函数比较关键的在于一些属性值得设置，初始容量、负载因子、阈值。 threshold = capacity * loadFactor MAXIMUM_CAPACITY 名称 用途 默认值 initialCapacity HashMap 初始容量 16 loadFactor 负载因子 0.75 threshold 当前 HashMap 所能容纳键值对数量的最大值，超过这个值，则需扩容 通过计算 MAXIMUM_CAPACITY 最大容量 1&lt;&lt;30 TREEIFY_THRESHOLD 链表树化阈值 8 UNTREEIFY_THRESHOLD 链表化的阈值（小于这个值，用链表形式） 6 MIN_TREEIFY_CAPACITY 最小容量 64 查找原理 1. 根据hash定位槽 2. 查找给定key（hash相等，key相等），依次判断，找到直接返回，否则最后返回null 1. 桶元素 2. 若桶元素为树节点类型，委托给树查找元素 3. 在链表中查找 code 12345678910111213141516171819202122232425262728public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; // 1. 定位键值对所在桶的位置 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) &#123; // 2. 如果 first 是 TreeNode 类型，则调用黑红树查找方法 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); // 2. 对链表进行查找 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; 遍历从index = 0, table[index]开始，找到一个不为null的slot，遍历链表，用到的是next引用。 插入原理：根据hash定位到槽，如果槽为空，直接插入，否则连接到链表尾部、或更新键值对 涉及到table 初始化、扩容、树化。 当桶数组 table 为空时，通过扩容的方式初始化 table 查找要插入的键值对是否已经存在，如果不存在，则将键值对链入链表中，并根据链表长度决定是否将链表转为红黑树 如果已存在，根据条件判断是否用新值替换旧值 判断键值对数量是否大于阈值，大于的话则进行扩容操作 扩容原理 计算新容量newCap和新阈值newThr 创建新的数组 将键值对重新映射到新数组上 如果无链表，直接根据hash&amp;(newCap-1)定位 如果是树节点，委托红黑树来拆分和重新映射 为链表，根据hash&amp;oldCap的值分成两组，映射到j和j+oldCap 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; // 如果 table 不为空，表明已经初始化过了 if (oldCap &gt; 0) &#123; // 当 table 容量超过容量最大值，则不再扩容 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 按旧容量和阈值的2倍计算新容量和阈值的大小 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold /* * 初始化时，将 threshold 的值赋值给 newCap， * HashMap 使用 threshold 变量暂时保存 initialCapacity 参数的值 */ newCap = oldThr; else &#123; // zero initial threshold signifies using defaults /* * 调用无参构造方法时，桶数组容量为默认容量， * 阈值为默认容量与默认负载因子乘积 */ newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; // newThr 为 0 时，按阈值计算公式进行计算 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; // 创建新的桶数组，桶数组的初始化也是在这里完成的 Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; // 如果旧的桶数组不为空，则遍历桶数组，并将键值对映射到新的桶数组中 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) // 重新映射时，需要对红黑树进行拆分 ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; // 遍历链表，并将链表节点按原顺序进行分组 do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 将分组后的链表映射到新桶中 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; ###### 链表树化 链表树化有两个条件，不满足采用扩容，满足再扩容 树化时，将Node节点替换为TreeNode，保留next信息 替换后，再从head开始，进行红黑树化（标记红黑节点、父子节点，如果root节点不是first节点，再修正next和prev？）【链表转成红黑树后，原链表的顺序仍然会被引用仍被保留了（红黑树的根节点会被移动到链表的第一位）】 在扩容过程中，树化要满足两个条件： 链表长度大于等于 TREEIFY_THRESHOLD 8 桶数组容量大于等于 MIN_TREEIFY_CAPACITY 64 红黑树拆分红黑树中保留了next引用，拆分原理和链表相似 根据hash拆分成两组（这时候会生成新的next关系） 各组内根据情况，链化或者重新红黑树化 红黑树链化将TreeNode替换为Node 删除原理 定位到槽 找到删除节点 删除节点，并修复链表或红黑树 ConcurrentHashMap相比较HashMap，主要是新增了写操作时候的同步处理 扩容迁移时，可以多个线程帮助迁移 Q：ConcurrentHashMap 在 JDK 1.8 中，为什么要使用内置锁 synchronized 来代替重入锁 ReentrantLock？ ①、粒度降低了；②、优化后的synchronized性能与ReentrantLock不相上下，基于JVM也保证synchronized在各平台上都可使用。③、在大量的数据操作下，对于 JVM 的内存压力，基于 API 的 ReentrantLock 会开销更多的内存。 插入 计算hash 循环执行 如果数组为空，初始化initTable 如果hash定位到的槽为空，CAS替换为新节点，退出循环 如果槽不为空，节点hash为-1，说明正在迁移，helpTransfer 槽不为空，且不在迁移，那么，对头节点加监控器锁，链表或红黑树形式插入或更新节点 addCount 计数【扩容==迁移】主要为putVal情形下的addCount，主要逻辑： 更新as的值，如果有竞争，退出方法，无竞争，根据as和baseCount统计当前节点个数s 如果s大于扩容阈值 如果sizeCtl&lt;0，说明正在迁移。如果未到迁移上限、或者迁移结束、或者新表未创建，退出本次循环，否则更新迁移线程数后进入迁移。 如果sizeCtl&gt;=0，由当前线程启动迁移，CAS更新sizeCtl为(rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)，进入迁移。 再sunCount统计s个数 12345678910111213141516171819202122232425262728293031323334353637383940private final void addCount(long x, int check) &#123; CounterCell[] as; long b, s; if ((as = counterCells) != null || !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) &#123; CounterCell a; long v; int m; boolean uncontended = true; if (as == null || (m = as.length - 1) &lt; 0 || (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null || // 如果CAS CELLVALUE失败，则进入fullAddCount，然后退出方法 // CAS成功，说明不存在对a的竞争，继续 !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) &#123; fullAddCount(x, uncontended); return; &#125; // check的值为putVal的binCount时，在链表新增节点或红黑树形式下，值都&gt;=2 if (check &lt;= 1) return; s = sumCount(); &#125; if (check &gt;= 0) &#123; Node&lt;K,V&gt;[] tab, nt; int n, sc; while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123; int rs = resizeStamp(n); if (sc &lt; 0) &#123; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); &#125; else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); s = sumCount(); &#125; &#125;&#125; 迁移transfer的第二个参数为空的时候，触发扩容，创建nextTable，在addCount和tryPresize中有这样的调用。 addCount是size不精确情况下，可能触发扩容；tryPresize是已知精确size的情况下做扩容。 总结下流程 计算步长stride 如果nextTab未创建，则创建之，并赋给nextTable 循环迁移 分配迁移区间i和bound（i从前往后，bound = i - stride + 1`，总之就是stride） 如果区间已达边界，将sc减1，表示本线程退出迁移。如果是最后一个迁移线程，标记finish和advance为true，进入下一循环recheck；非最后线程，直接退出方法。 若未达边界，且槽为空，CAS槽为fwd，进入下一循环 槽不为空，且槽已经是fwd，进入下一循环 最后一种情形，进行迁移 为链表，根据节点hash二进制第k位为0或1分成两组（n=2^k），1连接到高位槽上 为红黑树，分组同链表，分好的组根据节点个数判断是否链化或新生成红黑树 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151/** * Moves and/or copies the nodes in each bin to new table. See * above for explanation. */private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; int n = tab.length, stride; // (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n // cpu个数&gt;1的话，stride = 表长n/8/cpu个数 // cpu个数为1，stride = 表长n // 最小步长为16 if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range // 如果传入的nextTab是空的，先创建新数组 if (nextTab == null) &#123; // initiating try &#123; @SuppressWarnings(\"unchecked\") Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; &#125; nextTable = nextTab; transferIndex = n; &#125; int nextn = nextTab.length; ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); boolean advance = true; boolean finishing = false; // to ensure sweep before committing nextTab for (int i = 0, bound = 0;;) &#123; Node&lt;K,V&gt; f; int fh; // 分配本次循环需要迁移的区间 // 一般i=transferIndex-1，表示本次循环的上界，bound=i-stride，表示下界 while (advance) &#123; int nextIndex, nextBound; if (--i &gt;= bound || finishing) advance = false; else if ((nextIndex = transferIndex) &lt;= 0) &#123; i = -1; advance = false; &#125; else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; // bound = i - stride // bound = nextBound = nextIndex - stride = i + 1 - stride bound = nextBound; i = nextIndex - 1; advance = false; &#125; &#125; // 已到达边界，标识当前size*2的迁移finish if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; int sc; if (finishing) &#123; nextTable = null; table = nextTab; sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); return; &#125; if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return; finishing = advance = true; i = n; // recheck before commit &#125; &#125; // 如果槽为null，CAS替换为fwd else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); // 如果槽已被标记为迁移，则进入下一个分配循环 while (advance) else if ((fh = f.hash) == MOVED) advance = true; // already processed // 执行本区间的迁移 // 槽节点加监视器锁，如果为链表，分成高低位两组；或者为红黑树，也是利用next引用，分成两组，根据长度判断是否要链表化；结束后将advance标记为true，循环继续 else &#123; synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; Node&lt;K,V&gt; ln, hn; if (fh &gt;= 0) &#123; int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; else &#123; hn = lastRun; ln = null; &#125; for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; else if (f instanceof TreeBin) &#123; TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) &#123; int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) &#123; if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; &#125; else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; // new TreeBin&lt;K,V&gt;(lo) 中，调整红黑树 ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; &#125; &#125; &#125; &#125;&#125; #### 查找LinkedHashMap概览 Entry继承自HashMap.Entry，增加before和after引用 增加head和tail引用 默认维护插入顺序，accessOrder = false。构造函数中允许传入accessOrder = true，维护访问顺序（最近访问的移动到tail），实现LRU（Least recently used,最近最少使用）策略的缓存。 新增节点newNode时，若tail为空，head指向当前节点，tail指向当前节点。一般在put时候调用newNode方法。 123456Node&lt;K,V&gt; newNode(int hash, K key, V value, Node&lt;K,V&gt; e) &#123; LinkedHashMap.Entry&lt;K,V&gt; p = new LinkedHashMap.Entry&lt;K,V&gt;(hash, key, value, e); linkNodeLast(p); return p;&#125; 12345678910private void linkNodeLast(LinkedHashMap.Entry&lt;K,V&gt; p) &#123; LinkedHashMap.Entry&lt;K,V&gt; last = tail; tail = p; if (last == null) head = p; else &#123; p.before = last; last.after = p; &#125;&#125; 删除节点时，将当前节点的before和after建立连接 维护访问顺序时，构造函数中传入accessOrder = true，在调用get/getOrDefault/replace等方法时，会将访问节点移动到链表末尾。实现方式是，先连接起当前节点的before和after，再将当前节点连接到原tail后面。 基于LinkedHashMap实现LRU缓存时，还可通过覆写removeEldestEntry（移除最老节点的策略，比如设定个最大链表大小，超过这个大小就返回true，移除最老节点）方法，实现自定义策略的LRU缓存。 继承自 HashMap，具有和 HashMap 一样的快速查找特性。 内部维护了一个双向链表，用来维护插入顺序或者 LRU 顺序。 12345678/** * The head (eldest) of the doubly linked list. */transient LinkedHashMap.Entry&lt;K,V&gt; head;/** * The tail (youngest) of the doubly linked list. */transient LinkedHashMap.Entry&lt;K,V&gt; tail; accessOrder 决定了顺序，默认为 false，此时维护的是插入顺序。 1final boolean accessOrder; LinkedHashMap 最重要的是以下用于维护顺序的函数，它们会在 put、get 等方法中调用。 afterNodeAccess()当一个节点被访问时，如果 accessOrder 为 true，则会将该节点移到链表尾部。也就是说指定为 LRU 顺序之后，在每次访问一个节点时，会将这个节点移到链表尾部，保证链表尾部是最近访问的节点，那么链表首部就是最近最久未使用的节点。 123456789101112131415161718192021222324void afterNodeAccess(Node&lt;K,V&gt; e) &#123; // move node to last LinkedHashMap.Entry&lt;K,V&gt; last; if (accessOrder &amp;&amp; (last = tail) != e) &#123; LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; p.after = null; if (b == null) head = a; else b.after = a; if (a != null) a.before = b; else last = b; if (last == null) head = p; else &#123; p.before = last; last.after = p; &#125; tail = p; ++modCount; &#125;&#125; afterNodeInsertion()在 put 等操作之后执行，当 removeEldestEntry() 方法返回 true 时会移除最晚的节点，也就是链表首部节点 first。evict 只有在构建 Map 的时候才为 false，在这里为 true。 1234567void afterNodeInsertion(boolean evict) &#123; // possibly remove eldest LinkedHashMap.Entry&lt;K,V&gt; first; if (evict &amp;&amp; (first = head) != null &amp;&amp; removeEldestEntry(first)) &#123; K key = first.key; removeNode(hash(key), key, null, false, true); &#125;&#125; 每当有新键值对节点插入，新节点最终会接在 tail 引用指向的节点后面。而 tail 引用则会移动到新的节点上，这样一个双向链表就建立起来了。 WeekHashMap","tags":[{"name":"Java容器","slug":"Java容器","permalink":"https://northernw.github.io/tags/Java容器/"}]},{"title":"Java-Java并发","date":"2019-09-23T14:44:23.000Z","path":"2019/09/23/Java-Java并发/","text":"线程状态转换同JVM中记录 使用线程 实现Runnable接口 实现Callable接口 继承Thread类 Condition的概念Condition主要是为了在J.U.C框架中提供和Java传统的监视器风格的wait，notify和notifyAll方法类似的功能。 JDK的官方解释如下： 条件（也称为条件队列 或条件变量）为线程提供了一个含义，以便在某个状态条件现在可能为 true 的另一个线程通知它之前，一直挂起该线程（即让其“等待”）。因为访问此共享状态信息发生在不同的线程中，所以它必须受保护，因此要将某种形式的锁与该条件相关联。等待提供一个条件的主要属性是：以原子方式 释放相关的锁，并挂起当前线程，就像 Object.wait 做的那样。 Condition实质上是被绑定到一个锁上。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273/** * 生产者、消费者示例 */@Slf4jpublic class ConditionTest &#123; private int storage; private int putCounter; private int getCounter; private Lock lock = new ReentrantLock(); private Condition putCondition = lock.newCondition(); private Condition getCondition = lock.newCondition(); public void put() throws InterruptedException &#123; try &#123; lock.lock(); if (storage &gt;= 100) &#123; putCondition.await(); &#125; storage++; log.info(\"put =&gt; storage = &#123;&#125;, putCounter = &#123;&#125;\", storage, putCounter++); getCondition.signal(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void get() throws InterruptedException &#123; try &#123; lock.lock(); if (storage &lt;= 0) &#123; getCondition.await(); &#125; storage--; log.info(\"get =&gt; storage = &#123;&#125;, getCounter = &#123;&#125;\", storage, getCounter++); putCondition.signal(); &#125; finally &#123; lock.unlock(); &#125; &#125; public class PutThread extends Thread &#123; @Override public void run() &#123; for (int i = 0; i &lt; 10000; i++) &#123; try &#123; put(); &#125; catch (InterruptedException e) &#123; &#125; &#125; &#125; &#125; public class GetThread extends Thread &#123; @Override public void run() &#123; for (int i = 0; i &lt; 10000; i++) &#123; try &#123; get(); &#125; catch (InterruptedException e) &#123; &#125; &#125; &#125; &#125; public static void main(String[] args) &#123; final ConditionTest test = new ConditionTest(); Thread put = test.new PutThread(); Thread get = test.new GetThread(); put.start(); get.start(); &#125;&#125; Java内存模型内存间交互操作Java 内存模型定义了 8 个操作来完成主内存和工作内存的交互操作。 read:把一个变量的值从主内存传输到工作内存中load:在 read 之后执行，把 read 得到的值放入工作内存的变量副本中use:把工作内存中一个变量的值传递给执行引擎assign:把一个从执行引擎接收到的值赋给工作内存的变量store:把工作内存的一个变量的值传送到主内存中write:在 store 之后执行，把 store 得到的值放入主内存的变量中lock:作用于主内存的变量unlock 内存模型特性 原子性 Java 内存模型保证了 read、load、use、assign、store、write、lock 和 unlock 操作具有原子性，例如对一个 int类型的变量执行 assign 赋值操作，这个操作就是原子性的。 i++ 不是线程安全的操作 可见性 可见性指当一个线程修改了共享变量的值，其它线程能够立即得知这个修改。 三种实现方式：volatile，synchronized（对一个变量执行 unlock 操作之前,必须把变量值同步回主内存），final 有序性：单线程内有序 先行发生原则 单一线程原则：在一个线程内，在程序前面的操作先行发生于后面的操作。 管程锁定原则：一个 unlock 操作先行发生于后面对同一个锁的 lock 操作。 Volatile变量原则：对一个 volatile 变量的写操作先行发生于后面对这个变量的读操作。 线程启动原则：Thread 对象的 start() 方法调用先行发生于此线程的每一个动作。 线程加入原则：Thread 对象的结束先行发生于 join() 方法返回。 线程中断原则：对线程 interrupt() 方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过 interrupted() 方法检测到是否有中断发生。 对象终结原则：一个对象的初始化完成(构造函数执行结束)先行发生于它的 finalize() 方法的开始。 传递性：如果操作 A 先行发生于操作 B，操作 B 先行发生于操作 C，那么操作 A 先行发生于操作 C。 线程安全同JVM 不可变：final、 String、 Number子类、Collections.unmodifiableXX等 互斥同步：synchronized、ReentrantLock 非阻塞同步：CAS、juc原子类、AtomicStampedReference 无同步方案：栈封闭（方法内临时变量）、线程本地存储（ThreadLocal）、可重入代码（不依赖堆上的对象和系统公共资源、用到的状态参数都由变量传入、不调用不可重入方法） 其他","tags":[{"name":"Java并发","slug":"Java并发","permalink":"https://northernw.github.io/tags/Java并发/"}]},{"title":"Java-Java虚拟机","date":"2019-09-17T11:28:38.000Z","path":"2019/09/17/Java-Java虚拟机/","text":"运行时数据区域 程序计数器： 记录正在执行的虚拟机字节码指令地址 Java虚拟机栈： Java方法执行时创建的栈帧，用于存储临时变量表、操作数栈、常量池引用等信息。 -Xss指定栈大小，jdk1.4默认256k，jdk1.5+默认1M. 1java -Xss2M HackTheJava 该区域可能抛出以下异常: 当线程请求的栈深度超过最大值，会抛出 StackOverflowError 异常; 栈进行动态扩展时如果无法申请到足够内存，会抛出 OutOfMemoryError 异常。 本地方法栈： 与 Java 虚拟机栈类似，它们之间的区别是本地方法栈为本地方法服务。 堆： 所有对象都在这里分配内存，是垃圾收集的主要区域(“GC 堆”)。 现代的垃圾收集器基本都是采用分代收集算法，主要思想是针对不同类型的对象采取不同的垃圾回收算法。 可以将堆分成两块: 新生代(Young Generation) 老年代(Old Generation) 堆不需要连续内存，并且可以动态增加其内存，增加失败会抛出 OutOfMemoryError 异常。 通过 -Xms 和 -Xmx 这两个虚拟机参数来指定一个程序的堆内存大小，第一个参数设置初始值，第二个参数设置最大值。 1java -Xms1M -Xmx2M HackTheJava 方法区 用于存放已被加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 和堆一样不需要连续的内存，并且可以动态扩展，动态扩展失败一样会抛出 OutOfMemoryError 异常。 从 JDK1.8 开始，移除永久代，并把方法区移至元空间，它位于本地内存中，而不是虚拟机内存中。 运行时常量池 运行时常量池是方法区的一部分。 Class 文件中的常量池（编译生成的字面量和符号引用）会在类加载后被放入这个区域。 除了在编译期生成的常量，还允许动态生成，例如 String 类的 intern()。 直接内存 在 JDK 1.4 中新引入了 NIO 类，它可以使用 Native 函数库直接分配堆外内存，然后通过 Java 堆里的DirectByteBuffer 对象作为这块内存的引用进行操作。这样能在一些场景中显著提高性能，因为避免了在堆内存和堆外内存来回拷贝数据。 垃圾收集判断一个对象是否可被回收 引用计数法 为对象添加一个引用计数器，当对象增加一个引用时计数器加 1，引用失效时计数器减 1。引用计数为 0 的 对象可被回收。 存在循环引用的问题。 可达性分析 以 GC Roots 为起始点进行搜索，可达的对象都是存活的，不可达的对象可被回收。 GC Roots 一般包含以下内容: 1. 虚拟机栈中局部变量表中引用的对象 2. 本地方法栈中 JNI 中引用的对象 3. 方法区中类静态属性引用的对象 4. 方法区中的常量引用的对象 方法区的回收方法区主要存放永久代对象，回收率比较低 主要是对常量回收和对类卸载 类卸载的条件： 1. 该类的所有实例都已回收 2. 加载该类的classloader已回收 3. 该类对应的Class对象没有被引用，意味着不能通过反射生成该类实例、访问该类方法 引用类型1. 强引用类型被强引用关联的对象不会被回收。 使用new一个新对象的方式来创建强引用。 Object obj = new Object(); 2. 软引用类型被软引用关联的对象只有在内存不够的情况下才会被回收。使用 SoftReference 类来创建软引用。 123Object obj = new Object();SoftReference&lt;Object&gt; sf = new SoftReference&lt;Object&gt;(obj);obj = null; // 使对象只被软引用关联 3. 弱引用类型被弱引用关联的对象一定会被回收，也就是说它只能存活到下一次垃圾回收发生之前。使用 WeakReference 类来创建弱引用。 123Object obj = new Object();WeakReference&lt;Object&gt; wf = new WeakReference&lt;Object&gt;(obj);obj = null; 4. 虚引用类型又称为幽灵引用或者幻影引用，一个对象是否有虚引用的存在，不会对其生存时间造成影响，也无法通过虚引用得到一个对象。 为一个对象设置虚引用的唯一目的是能在这个对象被回收时收到一个系统通知。 使用 PhantomReference 来创建虚引用。 123Object obj = new Object();PhantomReference&lt;Object&gt; pf = new PhantomReference&lt;Object&gt;(obj, null);obj = null; 垃圾收集算法1. 标记 - 清除在标记阶段，程序会检查每个对象是否为活动对象，如果是活动对象，则程序会在对象头部打上标记。 在清除阶段，会进行对象回收并取消标志位 。 回收对象就是把对象作为分块，连接到被称为 “空闲链表” 的单向链表，之后进行分配时只需要遍历这个空闲链表，就可以找到分块。 不足: 标记和清除过程效率都不高; 会产生大量不连续的内存碎片，导致无法给大对象分配内存。 2. 标记 - 整理让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。 优点: 不会产生内存碎片 不足: 需要移动大量对象，处理效率比较低。 3. 复制将内存划分为大小相等的两块，每次只使用其中一块，当这一块内存用完了就将还存活的对象复制到另一块上面，然后再把使用过的内存空间进行一次清理。 4. 分代收集现在的商业虚拟机采用分代收集算法，它根据对象存活周期将内存划分为几块，不同块采用适当的收集算法。 一般将堆分为新生代和老年代。 新生代使用:复制算法 老年代使用:标记 - 清除 或者 标记 - 整理 算法 垃圾收集器 stop the world：停顿所有Java执行线程。 Safepoint：并非所有地方都能停顿下来开始GC，只有在到达安全点时才能停顿。安全点选取“长时间执行”的代码片段，例如方法调用、循环跳转、异常跳转等。 抢占式中断：先挂起线程，发现不是安全点就恢复线程，让线程跑到安全点。不采用。 主动式中断：线程执行到安全点时，主动去轮询中断标志，触发自身中断。 安全区域：指一段代码片段之中，引用关系不会发生变化。Safe Region可以看成是Safepoint的扩展。 为了解决“不执行”–没有分配CPU时间，sleep或blocked等–的线程的无法主动式中断。线程执行到安全区域时，会标识自己的安全区域状态，虚拟机GC时忽略这些线程。如果线程运行到要离开安全区域，但GC还未结束，则需要等待到GC结束。 单线程与多线程:单线程指的是垃圾收集器只使用一个线程，而多线程使用多个线程; 串行与并行:串行指的是垃圾收集器与用户程序交替执行，这意味着在执行垃圾收集的时候需要停顿用户程序;并行指的是垃圾收集器和用户程序同时执行。除了 CMS 和 G1 之外，其它垃圾收集器都是以串行的方式执行。 Serial 和 Serial old ParNew 和 Serial Old Parallel Scavenge 和 Parallel Old CMS 1. Serial单线程，复制，新生代，简单高效，需要暂停用户线程，client端可接受 2. ParNewserial多线程版本，server端默认新生代收集器，与CMS配合 3. Parallel Scavenge多线程，新生代，目标是可控吞吐量，称为“吞吐量优先”收集器（吞吐量：CPU用于用户程序的时间占总时间比值） 有参数可打开 GC 自适应的调节策略，就不需要手工指定新生代的大小(-Xmn)、Eden 和 Survivor 区的比例、晋升老年代对象年龄等细节参数了。虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量。 4. Serial Oldserial的老年代收集器，标记清除-整理。CMS的后背预案，在并发收集发生 Concurrent Mode Failure 时使用。 5. Parallel OldParallel Scavenge的老年代收集器，注重吞吐量。 6. CMS老年代收集器。 CMS(Concurrent Mark Sweep)，Mark Sweep 指的是标记 - 清除算法。分为以下四个流程: 初始标记:仅仅只是标记一下 GC Roots 能直接关联到的对象，速度很快，需要停顿。 并发标记:进行 GC Roots Tracing 的过程，它在整个回收过程中耗时最长，不需要停顿。 重新标记:为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，需要 停顿。 并发清除:不需要停顿。 在整个过程中耗时最长的并发标记和并发清除过程中，收集器线程都可以与用户线程一起工作，不需要进行停顿。具有以下缺点: 吞吐量低：低停顿时间是以牺牲吞吐量为代价的，导致 CPU 利用率不够高。CPU换取低停顿。 无法处理浮动垃圾，可能出现 Concurrent Mode Failure。浮动垃圾是指并发清除阶段由于用户线程继续运行 而产生的垃圾，这部分垃圾只能到下一次 GC 时才能进行回收。由于浮动垃圾的存在，因此需要预留出一部分 内存，意味着 CMS 收集不能像其它收集器那样等待老年代快满的时候再回收。如果预留的内存不够存放浮动垃 圾，就会出现 Concurrent Mode Failure，这时虚拟机将临时启用 Serial Old 来替代 CMS。 标记 - 清除算法导致的空间碎片，往往出现老年代空间剩余，但无法找到足够大连续空间来分配当前对象，不得 不提前触发一次 Full GC。 7. G1意在替换 CMS 收集器 G1 把堆划分成多个大小相等的独立区域(Region)，新生代和老年代不再物理隔离。 有优先级的区域回收方式。维护了优先队列。 整体上是标记-整理，局部（两个region间）是复制。—-没有具体说明 每个 Region 都有一个 Remembered Set，用来记录该 Region 对象的引用对象所在的 Region。避免可达性分析时全堆扫描。存储着其他分区中的对象对本分区对象的引用。minor gc的时候，只要扫描RSet中的其他old区对象对于本young区的引用，不需要扫描所有old区。 G1收集器 Young GC 新生代。 MixGC 混合收集，回收部分老年代对象，G1特有 FullGC 老年代收集，JDK1.8不提供。 如果不计算维护 Remembered Set 的操作，G1 收集器的运作大致可划分为以下几个步骤: 初始标记 并发标记 最终标记:为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将 这段时间对象变化记录在线程的 Remembered Set Logs 里面，最终标记阶段需要把 Remembered Set Logs 的数据合并到 Remembered Set 中。这阶段需要停顿线程，但是可并行执行。 筛选回收:首先对各个 Region 中的回收价值和成本进行排序，根据用户所期望的 GC 停顿时间来制定回收计 划。此阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分 Region，时间是用户可控制的， 而且停顿用户线程将大幅度提高收集效率。 具备如下特点: 空间整合:整体来看是基于“标记 - 整理”算法实现的收集器，从局部(两个 Region 之间)上来看是基于“复制”算法实现的，这意味着运行期间不会产生内存空间碎片。 —-没有具体说明，不理解啊。每个region里是复制？？ 可预测的停顿:能让使用者明确指定在一个长度为 M 毫秒的时间片段内，消耗在 GC 上的时间不得超过 N 毫 秒。 内存分配与回收策略MinorGC与FullGCminorGC: 回收新生代对象。 fullGC: 回收新生代与老年代对象。 内存分配策略 新对象优先分配在Eden。 大对象直接进入老年代。 超过一定年龄的对象进入老年代。（长期存活的对象） 动态对象年龄判定。（某个年龄对象的总和超过survivor容量的一半） 空间分配担保。（会触发fullGC，参数HandlePromotionFailure设置是否允许担保失败。） MinorGC的触发条件Eden区空间不足 FullGC的触发条件 System.gc() 并未真正执行full gc 老年代空间不足 空间分配担保失败 jdk1.7以前的永久代空间不足 Concurrent Mode Failure, CMS gc过程中，有对象进入老年代 类加载机制类的生命周期加载-验证-准备-解析-初始化-使用-卸载 类加载过程加载-验证-准备-解析-初始化 类初始化时机 new/getstatic/putstatic/invokestatic 指令 reflect包反射调用时，若类未初始化，则初始化 初始化一个类时，若类的父类未初始化，则先初始化父类 虚拟机启动时，初始化主类 Jdk1.7动态语言，遇到方法句柄REF_putstatic/REF_getstatic/REF_invokestatic（方法句柄所对应的类没有进行过初始 化，则需要先触发其初始化） 类与类加载器类加载器分类虚拟机层面，分两类： 启动类加载器，cpp实现，Bootstrap ClassLoader Java程序里的加载器，Java实现 BootstrapLoader ExtClassLoader AppClassLoader 双亲委派模型模型要求除了顶层的启 动类加载器外，其它的类加载器都要有自己的父类加载器。这里的父子关系一般通过组合关系(Composition)来实 现，而不是继承关系(Inheritance)。 工作原理： 一个类加载器首先将类加载请求转发到父类加载器，只有当父类加载器无法完成时才尝试自己加载。 好处： 使得基础类得到统一 为什么有三个加载器？ 保证唯一性 通过双亲委派模型，保证同一个类只会被一个加载器加载，保证类的唯一性。 保证安全性 自定义的同名类不会覆盖基础类，保证JVM的安全性 为什么java里面会用三种classloader，这样设计目的是什么？ JVM调优类文件结构无关性的基石可以运行在各种不同平台上的虚拟机，这些虚拟机可以载入和执行同一种平台无关的字节码，从而实现程序的“一次编写，到处运行”。 实现语言无关性的基础是虚拟机和字节码存储格式。 Class类文件的结构—-看不懂！ 魔数与class文件的版本号 常量池 访问标志 类索引、父类索引、接口索引集合 字段表集合 方法表集合 属性表集合 字节码指令简介—-看不懂！+1虚拟机字节码执行引擎可能会有解释执行和编译执行两种选择，也可能两者兼备，甚至还可能包含几个不同级别的编译期执行引擎。 从外观（Facade）看来，所有的Java虚拟机的执行引擎都是一致的：输入字节码文件，处理过程是字节码解析的等效过程，输出的是执行结果。 运行时栈帧结构 局部变量表 操作数栈 动态连接 方法返回地址 一些附加信息，例如调试相关的信息等，取决于具体的虚拟机实现 方法调用 解析：静态过程，在编译期就完全确定，在类加载的解析阶段把涉及的符号引用全部转变为可确定的直接引用 分派：可能静态，也可能动态。 注意编译期多态（重载 同一个类里方法名相同）与运行期多态（重写override 父子类里方法签名相同）。 基于栈的字节码解释执行引擎采用基于栈架构指令集的方式，依赖操作数栈进行工作 （另一种是基于寄存器的指令集，主流物理机采用） 优点：可移植，平台无关 缺点：相对于寄存器（硬件直接提供）方式，速度稍慢一些 编译优化早期javac编译时 解析与填充符号表 词法、语法分析 填充符号表 注解处理：读取、修改、添加抽象语法树中的任意元素 语义分析与字节码生成：保证源程序是符合逻辑的 标注检查：变量使用前是否被声明、数据类型匹配等 数据及控制流分析：局部变量使用前是否有赋值、方法的每条路径是否有返回值、所有受查异常都被处理 解语法糖 字节码生成：生成构造器、代码替换以优化实现逻辑（字符串+替换为stringbuild或stringbuffer的append等，取决于目标代码的版本大于或等于jdk1.5） Java语法糖 泛型与类型擦除 自动装箱、拆箱与遍历循环 条件编译 晚期jvm 即时编译器 （just in time compiler JIT编译期） C1 (client compiler)和C2 (server compiler) 热点代码：某个方法或代码块的运行特别频繁。 在运行时，虚拟机会将热点代码编译成本地平台相关的机器码，并进行各种层次的优化。 jvm内会同时存在解释器与编译器，可指定执行模式（只解释，只编译，或混合） 编译优化技术比较好理解的。。。 公共子表达式消除 数据边界检查消除（假如数组访问发生在循环内，编译期通过数据流分析判定循环变量的取值在[0,length)之内，整个循环中就可以把数组的上下界检查消除，可以减少很多的条件判定操作） 方法内联 逃逸分析 高效并发Java内存模型与线程Java内存模型 主内存与工作内存 volatile 保证可见性 防止指令重排序 原子性、可见性与有序性 先行发生原则 Java与线程抢占式调度 基于操作系统原生线程模型来实现 线程状态 新建 new 运行 runnable 无限期等待 waiting Object.wait()/ Thread.join() 需要被其他线程显式唤醒 限期等待 timed waiting thread.sleep() / Object.wait(time) / Thread.join(time) / LockSupport.parkNanos()/ LockSupport.partUntil() 过一定时间后由系统自动唤醒 阻塞 blocked： 等待一个排它锁 结束 terminated 线程安全与锁优化线程安全如果一个对象可以安全地被多个线程同时使用，那它就是线程安全地。 代码本身封装了所有必要的正确性保障手段（如互斥同步等） 调用者无须关心多线程的问题，也无需自己采用任何措施保证多线程的正确使用 Java语言中的线程安全 不可变 final 绝对线程安全 相对线程安全 对对象的单独的操作是线程安全的 某些特定顺序的连续调用，需要调用端额外采用同步手段 vector hashtable等 线程兼容 对象本身不是线程安全地 调用端正确地使用同步手段来保证对象在并发环境中可以安全地使用 arraylist hashmap等 线程对立 无论是否采用同步手段，都不能在并发环境中使用 不常见 线程安全的方式 互斥同步 互斥：临界区、互斥量、信号量 Java中，synchronized ReentrantLock 非阻塞同步 CAS (compare and swap, 基于系统指令支持） 无同步方案 可重入代码 线程本地存储 锁优化 自旋锁与自适应自旋 基于共享数据的锁定状态只会持续很短一段时间的经验数据 同一线程进入，无需加锁、解锁 竞争线程自旋等待，超过限定时间没有获得锁，触发锁膨胀 自适应自旋，是指竞争线程在争用锁时，动态判定自旋次数 Jdk1.6起，默认开启 锁消除 锁粗化 轻量级锁 获取锁 替换Mark Word。如果成功，线程拥有锁。如果失败，先检查Mark Word是否指向当前线程，是的话说明获得锁，继续执行。否则说明被其他线程抢占了。如果有两条以上线程争用同一个锁，轻量级锁不再有效，膨胀为重量级锁（Mark Word存储变为重量级锁的指针，后面等待锁的线程进入阻塞状态）。 释放锁 CAS替换Mark Word，如果成功，则释放成功 失败，说明存在竞争（Mark Word已指向重量级锁），释放锁时，唤醒其他的在等待的线程。 偏向锁 如果无竞争，整个同步都消除掉，不需要执行CAS操作 偏向第一个获得它的线程，如果锁没有被其他线程获取，则持有偏向锁的线程永远不需要再进行同步 其他this逃逸12345678910111213public class ThisEscape &#123; public final int id; public final String name; public ThisEscape(EventSource&lt;EventListener&gt; source) &#123; id = 1; source.registerListener(new EventListener() &#123; //内部类是可以直接访问外部类的成员变量的（外部类引用this被内部类获取了） public void onEvent(Object obj) &#123; System.out.println(\"id: \"+ThisEscape.this.id); System.out.println(\"name: \"+ThisEscape.this.name); &#125; &#125;); name = \"flysqrlboy\"; &#125; &#125; copyright@cs-notes","tags":[{"name":"Java虚拟机","slug":"Java虚拟机","permalink":"https://northernw.github.io/tags/Java虚拟机/"}]},{"title":"swagger源码笔记","date":"2019-09-16T15:17:01.000Z","path":"2019/09/16/swagger源码/","text":"配置swagger可生成controller的接口文档，比较好奇example value是如何生成的，来一探究竟。 从UI方面看，主要信息来自于definitions，给出了出入参数（汇总为model）的属性，引用了已存在实体。有了type和相关信息，既可以mock数据，也可以实例化对象。 初步看源码，Model和ModelProperty中的type为ResolvedType，是jackson的实现（还没看过jackson源码，哭泣）。 总结调用过程@EnableSwagger2注解引入了swagger的各类plugin，在spring容器finish之后，开始这些plugin的工作。在swagger中就是DocumentationPluginsBootstrapper。 接下去以Documentation-ApiListing(controller)-ApiDescription(method)/Model等维度，生成文档数据。 Ps:一个docket看做一个文档，可设置是否启用，group分组，默认分组为default。 解析结果ApiListing：一个controller 从description=user controller可以看出 ApiDescription：接口 Model：实体，模型 示例中有2个接口（/list和/list/no/type），3个模型（一个入参，2个出参） ApiDescription（method的维度）-Operation中，出入参对model的引用 response有4个，200status里有所需model 疑问ResolvedType在哪里生成？解析依赖 1234567891011121314151617181920212223242526272829303132333435363738394041DefaultModelDependencyProvider.javaline: 177 private List&lt;ResolvedType&gt; resolvedDependencies(ModelContext modelContext) &#123; ResolvedType resolvedType = modelContext.alternateFor(modelContext.resolvedType(typeResolver)); if (isBaseType(ModelContext.fromParent(modelContext, resolvedType))) &#123; LOG.debug(\"Marking base type &#123;&#125; as seen\", resolvedType.getSignature()); modelContext.seen(resolvedType); return newArrayList(); &#125; // 依赖 List&lt;ResolvedType&gt; dependencies = newArrayList(resolvedTypeParameters(modelContext, resolvedType)); dependencies.addAll(resolvedArrayElementType(modelContext, resolvedType)); dependencies.addAll(resolvedMapType(modelContext, resolvedType)); // 大部分是在这里产生的最终ResolvedType，其他几个resolve会调用到当前方法resolvedDependencies dependencies.addAll(resolvedPropertiesAndFields(modelContext, resolvedType)); dependencies.addAll(resolvedSubclasses(resolvedType)); return dependencies; &#125; private List&lt;ResolvedType&gt; resolvedPropertiesAndFields(ModelContext modelContext, ResolvedType resolvedType) &#123; if (modelContext.hasSeenBefore(resolvedType) || enumTypeDeterminer.isEnum(resolvedType.getErasedType())) &#123; return newArrayList(); &#125; modelContext.seen(resolvedType); List&lt;ResolvedType&gt; properties = newArrayList(); for (ModelProperty property : nonTrivialProperties(modelContext, resolvedType)) &#123; LOG.debug(\"Adding type &#123;&#125; for parameter &#123;&#125;\", property.getType().getSignature(), property.getName()); if (!isMapType(property.getType())) &#123; properties.add(property.getType()); &#125; // 集合类的元素类型 properties.addAll(maybeFromCollectionElementType(modelContext, property)); // map的value类型 properties.addAll(maybeFromMapValueType(modelContext, property)); // 本身 properties.addAll(maybeFromRegularType(modelContext, property)); &#125; return properties; &#125; 生成model看调用栈 ApiDocumentationScanner ApiListingScanner OperationModelsProvider 123collectFromReturnType(context); // 出参collectParameters(context); // 入参collectGlobalModels(context); 这里解析每个属性的类型，jackson的实现 这里记录下，82个typeResolver（在哪里注入的？） 1234567// 集合类型 数组类型 --&gt; 容器类型public static boolean isContainerType(ResolvedType type) &#123; return List.class.isAssignableFrom(type.getErasedType()) || Set.class.isAssignableFrom(type.getErasedType()) || (Collection.class.isAssignableFrom(type.getErasedType()) &amp;&amp; !Maps.isMapType(type)) || type.isArray();&#125; 验证需指定泛型12345678910111213141516171819202122232425262728293031@ToString@Getter@Setterpublic class ApiResponse&lt;T&gt; &#123; private Integer code; private String msg; private Boolean success; private T data; public static &lt;T&gt; ApiResponse&lt;T&gt; success(T data) &#123; ApiResponse&lt;T&gt; response = new ApiResponse&lt;T&gt;(); response.setData(data); response.setCode(1); response.setSuccess(Boolean.TRUE); return response; &#125;&#125;@RestControllerpublic class UserController &#123; @GetMapping(\"/list\") public ApiResponse&lt;List&lt;User&gt;&gt; list(@RequestBody User user) &#123; return ApiResponse.success(Lists.newArrayList()); &#125; @GetMapping(\"/list/no/type\") public ApiResponse listNoType(@RequestBody User user) &#123; return ApiResponse.success(Lists.newArrayList()); &#125;&#125; 最后和自己之前实验的一样，不指定泛型，是得不到想要的泛型数据的。 之后看下jackson的序列化实现，与Gson对比。 ps 总结是相对有目的性的结论，其他部分是看代码时候的一些记录，没有逻辑性。 带着疑问看源码吧，还要带着目的性，比如看源码是为了得到哪些结论，学习哪些技巧，等。","tags":[{"name":"swagger","slug":"swagger","permalink":"https://northernw.github.io/tags/swagger/"}]},{"title":"COLA学习","date":"2019-08-15T15:21:40.000Z","path":"2019/08/15/COLA学习/","text":"关于作者文章：复杂度应对之道 - COLA应用架构 GitHub：COLA ps maven环境变量配置1vim ~/.bash_profile 123export M2_HOME=$HOME/maven/apache-maven-3.3.9export PATH=$PATH:$M2_HOME/bin 执行mvn -v验证","tags":[{"name":"COLA","slug":"COLA","permalink":"https://northernw.github.io/tags/COLA/"}]},{"title":"InnoDB存储引擎-索引与算法","date":"2019-08-14T17:30:34.000Z","path":"2019/08/15/InnoDB存储引擎-索引与算法/","text":"索引与算法B+树索引聚集索引 clustered index 聚集索引是按照表主键构造的一颗B+树，叶子节点(也称为数据节点)中存放完整的数据行 非叶子节点，存放的是键值及指向数据叶的偏移量(可以理解为指向叶子节点的指针) 聚集索引逻辑上连续 页通过双向链表连接，按照主键排序，页中的记录也是双向链表连接 辅助索引 secondary index也称非聚集索引 辅助索引的叶子节点包含了该索引列的值和主键的值 锁锁机制用于管理对共享资源的并发访问。提供数据的完整性和一致性。 lock和latch lock 锁 latch 闩 对象 事务 线程 保护 数据库内容 内存数据结构 持续时间 整个事务过程 临界资源 模式 行锁、表锁、意向锁 读写锁、互斥量 死锁 通过waits-for graph、time out等机制进行死锁检测与处理 无死锁检测与处理。仅通过应用程序加锁的顺序来保证无死锁的情况发生 存在于 lock manager的哈希表中 每个数据结构的对象中 innodb中的锁一致性非锁定读是指innodb通过多版本控制的方式来读取当前时间数据库内行的数据。 读取的行正在执行update或delete，这时读取操作不会等待锁释放，而是去读取快照数据。通过undo段来实现。 提交读隔离级别下，读取最新的快照。 可重复读，读取事务开始前的快照。 一致性锁定读select … for update，加X锁，其他事务不能再枷锁。(别的事务可以进行读取，和一致性非锁定读情况一样) select … lock in share mode，加S锁。 锁的算法行锁的三种算法 record lock 单个记录上的锁 gap lock 间隙锁，锁定一个范围，但不包含记录本身 next-key lock 记录锁+间隙锁，锁定记录和范围，为了解决幻读，phantom problem（第二次查询会得到第一次查询中不存在的行） eg. 10，11，13，20 (-无穷，10] (10,11] (11,13] (13,20] (20,+无穷) 查询的列是唯一索引的情况下，next-key lock 会退化成record lock，以提高性能。 Eg. a 为主键，唯一。 P267 ~ 268 a. select * from t where a = 5 for update; b. insert into t select 4; 此时b会话不会阻塞。 锁问题 脏读 读到未提交数据 不可重复读 本文里包括幻读 丢失更新 阻塞一个事务中的锁等待另一个事务的锁释放它所占用的资源 阻塞确保事务可以正常地运行 死锁指两个或两个以上的事务在执行过程中，因争夺锁资源而造成的一种互相等待的现象 wait-for graph，等待图，一种较为主动的死锁检测，图中存在回路则存在死锁。在每个事务请求锁发生等待时都会判断是否存在回路，一般来说回滚undo量最小的事务。采用深度优先的算法实现，1.2版本后由递归版本优化为非递归版本。 锁升级指将当前锁的粒度将低。 eg. 把一个表的1000个行锁升级为页锁，或将页锁升级为表锁。","tags":[]},{"title":"JVM-3.垃圾收集器与内存分配策略","date":"2019-07-21T16:46:15.000Z","path":"2019/07/22/JVM-3-垃圾收集器与内存分配策略/","text":"","tags":[]},{"title":"JVM-2.Java内存区域与内存溢出异常","date":"2019-07-21T15:43:00.000Z","path":"2019/07/21/JVM-2-Java内存区域与内存溢出异常/","text":"运行时数据区域 程序计数器(线程私有) Java虚拟机栈(线程私有) 本地方法栈(线程私有，执行本地方法) Java堆：所有的对象实例及数组都在堆上分配。基本采用分代回收，新生代（Eden/From Survivor/To Survivor），老年代。 方法区：存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。Non-Heap非堆。HotSpot称之为永久代(也有回收，主要针对常量和对类的卸载。但效果一般，因为回收条件苛刻。) 运行时常量池：本是方法区的一部分，已经独立移出。用于存放编译期生成的各种字面量和符号引用。 直接内存：不是虚拟机运行时数据区的一部分，也不是虚拟机规范中定义的内存区域。JDK1.4 NIO 使用Native函数库直接分配堆外内存。 对象探秘 对象创建 为新生对象分配内存，有两种方式可用。受到不同收集器的影响。 指针碰撞(Bump the pointer)。堆中内存规整，用指针划分使用过的和未使用的两块空间，分配内存就是指针向未使用空间移动。 空闲列表(Free List)。虚拟机需要维护一个列表，记录哪些内存块是可用的，分配空间时从列表中找到一块足够大的空间划分给新对象，同时更新列表。 Serial、ParNew带等Compact过程：指针碰撞；CMS等基于Mark-Sweep：空闲列表。 并发情况下争用同一地址创建对象，如何解决？ 虚拟机采用CAS+失败重试来保证更新操作的原子性。 方案二：本地线程分配缓冲(Thread local allocation buffer, TLAB). 每个线程预先在堆中拿出一小块私有，缓冲区分配满了，再同步锁定，分配新TLAB. 通过-XX:+/-UseTLAB设定。 对象的内存布局 对象头(Header，Mark Word) 存储对象自身的运行时数据，如哈希码，GC份代年龄，锁状态标志，线程持有的锁，偏向线程ID，偏向时间戳等。 有一部分虚拟机实现，会在对象头中存储类型指针。与对象的访问定位有关。 实例数据(Instance data) 父类+子类的各种类型的字段内容。存储顺序受到VM分配策略和字段在源码中定义顺序影响。 对其填充(Padding) HotSpot虚拟机要求对象起始地址必须是8字节的整数倍。 对象的访问定位 句柄访问：包含了对象实例数据和类型数据各自的具体地址信息。 直接指针访问：实例对象中存储类型数据指针。 异常 OutOfMemoryError 堆溢出 虚拟机栈和本地方法栈溢出 方法区和运行时常量池溢出 本机直接内存溢出","tags":[{"name":"深入理解Java虚拟机","slug":"深入理解Java虚拟机","permalink":"https://northernw.github.io/tags/深入理解Java虚拟机/"}]},{"title":"Gson源码笔记","date":"2019-07-18T20:00:15.000Z","path":"2019/07/19/Gson源码笔记/","text":"Gson源码版本12345&lt;dependency&gt; &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt; &lt;artifactId&gt;gson&lt;/artifactId&gt; &lt;version&gt;2.8.6&lt;/version&gt;&lt;/dependency&gt; summary 抽象类TypeAdapter，委托模式，处理json字符串和特定类型对象之间的相互转换。抽象方法read和write在具体的TypeAdapter有各自的实现，比较复杂的包括集合类型的CollectionTypeAdapterFactory.Adapter（比如read时，要实例化集合，再循环处理集合内元素）、自定义对象类型的ReflectiveTypeAdapterFactory.Adapter（比如需要循环处理类内field的read和write）。 接口TypeAdapterFactory，抽象工厂模式，创建给定类型的TypeAdapter。基础类型已经预先创建typeAdapter（调用factory.create时，如果是给定的类型，返回预定义的adapter），Collection、自定义类型等包含泛型的，在运行时create（因为会有不同的要素，比如constructor、elementType、boundFields等）。 序列化时，泛型类型通过TypeAdapterRuntimeTypeWrapper进行处理。会判断使用rawType带过来的类型还是运行时真实value的类型进行后续处理。 责任链模式（？待确认）。在对顶层类型getAdapter过程中，会递归对下层类型进行getAdapter，并保存在上层adapter中；在顶层adapter.write过程中，也递归调用到子类型的adapter.write。ps在基本类型adapter中，才调用JsonReader/Writer读写。 设计模式工厂模式TypeAdapterFactory.create提供给定TypeToken的TypeAdapter. 12345678public interface TypeAdapterFactory &#123; /** * Returns a type adapter for &#123;@code type&#125;, or null if this factory doesn't * support &#123;@code type&#125;. */ &lt;T&gt; TypeAdapter&lt;T&gt; create(Gson gson, TypeToken&lt;T&gt; type);&#125; factory和adapterGson内置factory12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455 List&lt;TypeAdapterFactory&gt; factories = new ArrayList&lt;TypeAdapterFactory&gt;();// Gson的类型 // built-in type adapters that cannot be overridden factories.add(TypeAdapters.JSON_ELEMENT_FACTORY); factories.add(ObjectTypeAdapter.FACTORY); // the excluder must precede all adapters that handle user-defined types factories.add(excluder);// 用户定义的TypeAdapters // user's type adapters factories.addAll(typeAdapterFactories);// 基础类型 // type adapters for basic platform types factories.add(TypeAdapters.STRING_FACTORY); factories.add(TypeAdapters.INTEGER_FACTORY); factories.add(TypeAdapters.BOOLEAN_FACTORY); factories.add(TypeAdapters.BYTE_FACTORY); factories.add(TypeAdapters.SHORT_FACTORY); factories.add(TypeAdapters.newFactory(long.class, Long.class, longAdapter(longSerializationPolicy))); factories.add(TypeAdapters.newFactory(double.class, Double.class, doubleAdapter(serializeSpecialFloatingPointValues))); factories.add(TypeAdapters.newFactory(float.class, Float.class, floatAdapter(serializeSpecialFloatingPointValues))); factories.add(TypeAdapters.NUMBER_FACTORY); factories.add(TypeAdapters.CHARACTER_FACTORY); factories.add(TypeAdapters.STRING_BUILDER_FACTORY); factories.add(TypeAdapters.STRING_BUFFER_FACTORY); factories.add(TypeAdapters.newFactory(BigDecimal.class, TypeAdapters.BIG_DECIMAL)); factories.add(TypeAdapters.newFactory(BigInteger.class, TypeAdapters.BIG_INTEGER)); factories.add(TypeAdapters.URL_FACTORY); factories.add(TypeAdapters.URI_FACTORY); factories.add(TypeAdapters.UUID_FACTORY); factories.add(TypeAdapters.LOCALE_FACTORY); factories.add(TypeAdapters.INET_ADDRESS_FACTORY); factories.add(TypeAdapters.BIT_SET_FACTORY); factories.add(DateTypeAdapter.FACTORY); factories.add(TypeAdapters.CALENDAR_FACTORY); factories.add(TimeTypeAdapter.FACTORY); factories.add(SqlDateTypeAdapter.FACTORY); factories.add(TypeAdapters.TIMESTAMP_FACTORY); factories.add(ArrayTypeAdapter.FACTORY); factories.add(TypeAdapters.CLASS_FACTORY);// 组合类型和自定义类型 // type adapters for composite and user-defined types factories.add(new CollectionTypeAdapterFactory(constructorConstructor)); factories.add(new MapTypeAdapterFactory(constructorConstructor, complexMapKeySerialization)); factories.add(new JsonAdapterAnnotationTypeAdapterFactory(constructorConstructor)); factories.add(TypeAdapters.ENUM_FACTORY); factories.add(new ReflectiveTypeAdapterFactory( constructorConstructor, fieldNamingPolicy, excluder)); newFactorytypeToken.getRawType() 12345678910111213141516171819202122232425262728 public static &lt;TT&gt; TypeAdapterFactory newFactory( final Class&lt;TT&gt; type, final TypeAdapter&lt;TT&gt; typeAdapter) &#123; return new TypeAdapterFactory() &#123; @SuppressWarnings(\"unchecked\") // we use a runtime check to make sure the 'T's equal @Override public &lt;T&gt; TypeAdapter&lt;T&gt; create(Gson gson, TypeToken&lt;T&gt; typeToken) &#123; return typeToken.getRawType() == type ? (TypeAdapter&lt;T&gt;) typeAdapter : null; &#125; @Override public String toString() &#123; return \"Factory[type=\" + type.getName() + \",adapter=\" + typeAdapter + \"]\"; &#125; &#125;; &#125;// 装箱+非装箱 public static &lt;TT&gt; TypeAdapterFactory newFactory( final Class&lt;TT&gt; unboxed, final Class&lt;TT&gt; boxed, final TypeAdapter&lt;? super TT&gt; typeAdapter) &#123; return new TypeAdapterFactory() &#123; @SuppressWarnings(\"unchecked\") // we use a runtime check to make sure the 'T's equal @Override public &lt;T&gt; TypeAdapter&lt;T&gt; create(Gson gson, TypeToken&lt;T&gt; typeToken) &#123; Class&lt;? super T&gt; rawType = typeToken.getRawType(); return (rawType == unboxed || rawType == boxed) ? (TypeAdapter&lt;T&gt;) typeAdapter : null; &#125; @Override public String toString() &#123; return \"Factory[type=\" + boxed.getName() + \"+\" + unboxed.getName() + \",adapter=\" + typeAdapter + \"]\"; &#125; &#125;; &#125; 1234567891011121314151617181920 public static final TypeAdapterFactory TIMESTAMP_FACTORY = new TypeAdapterFactory() &#123; @SuppressWarnings(\"unchecked\") // we use a runtime check to make sure the 'T's equal @Override public &lt;T&gt; TypeAdapter&lt;T&gt; create(Gson gson, TypeToken&lt;T&gt; typeToken) &#123; if (typeToken.getRawType() != Timestamp.class) &#123; return null; &#125;// gson.getAdapter(Date.class)的逻辑？ final TypeAdapter&lt;Date&gt; dateTypeAdapter = gson.getAdapter(Date.class); return (TypeAdapter&lt;T&gt;) new TypeAdapter&lt;Timestamp&gt;() &#123; @Override public Timestamp read(JsonReader in) throws IOException &#123; Date date = dateTypeAdapter.read(in); return date != null ? new Timestamp(date.getTime()) : null; &#125; @Override public void write(JsonWriter out, Timestamp value) throws IOException &#123; dateTypeAdapter.write(out, value); &#125; &#125;; &#125; &#125;; CALENDAR和LOCALE的factory、adapter可以学习 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104public static final TypeAdapter&lt;Calendar&gt; CALENDAR = new TypeAdapter&lt;Calendar&gt;() &#123; private static final String YEAR = \"year\"; private static final String MONTH = \"month\"; private static final String DAY_OF_MONTH = \"dayOfMonth\"; private static final String HOUR_OF_DAY = \"hourOfDay\"; private static final String MINUTE = \"minute\"; private static final String SECOND = \"second\"; @Override public Calendar read(JsonReader in) throws IOException &#123; if (in.peek() == JsonToken.NULL) &#123; in.nextNull(); return null; &#125; in.beginObject(); int year = 0; int month = 0; int dayOfMonth = 0; int hourOfDay = 0; int minute = 0; int second = 0; while (in.peek() != JsonToken.END_OBJECT) &#123; String name = in.nextName(); int value = in.nextInt(); if (YEAR.equals(name)) &#123; year = value; &#125; else if (MONTH.equals(name)) &#123; month = value; &#125; else if (DAY_OF_MONTH.equals(name)) &#123; dayOfMonth = value; &#125; else if (HOUR_OF_DAY.equals(name)) &#123; hourOfDay = value; &#125; else if (MINUTE.equals(name)) &#123; minute = value; &#125; else if (SECOND.equals(name)) &#123; second = value; &#125; &#125; in.endObject(); return new GregorianCalendar(year, month, dayOfMonth, hourOfDay, minute, second); &#125; @Override public void write(JsonWriter out, Calendar value) throws IOException &#123; if (value == null) &#123; out.nullValue(); return; &#125; out.beginObject(); out.name(YEAR); out.value(value.get(Calendar.YEAR)); out.name(MONTH); out.value(value.get(Calendar.MONTH)); out.name(DAY_OF_MONTH); out.value(value.get(Calendar.DAY_OF_MONTH)); out.name(HOUR_OF_DAY); out.value(value.get(Calendar.HOUR_OF_DAY)); out.name(MINUTE); out.value(value.get(Calendar.MINUTE)); out.name(SECOND); out.value(value.get(Calendar.SECOND)); out.endObject(); &#125;&#125;;public static final TypeAdapterFactory CALENDAR_FACTORY = newFactoryForMultipleTypes(Calendar.class, GregorianCalendar.class, CALENDAR);public static final TypeAdapter&lt;Locale&gt; LOCALE = new TypeAdapter&lt;Locale&gt;() &#123; @Override public Locale read(JsonReader in) throws IOException &#123; if (in.peek() == JsonToken.NULL) &#123; in.nextNull(); return null; &#125; String locale = in.nextString(); StringTokenizer tokenizer = new StringTokenizer(locale, \"_\"); String language = null; String country = null; String variant = null; if (tokenizer.hasMoreElements()) &#123; language = tokenizer.nextToken(); &#125; if (tokenizer.hasMoreElements()) &#123; country = tokenizer.nextToken(); &#125; if (tokenizer.hasMoreElements()) &#123; variant = tokenizer.nextToken(); &#125; if (country == null &amp;&amp; variant == null) &#123; return new Locale(language); &#125; else if (variant == null) &#123; return new Locale(language, country); &#125; else &#123; return new Locale(language, country, variant); &#125; &#125; @Override public void write(JsonWriter out, Locale value) throws IOException &#123; out.value(value == null ? null : value.toString()); &#125;&#125;;public static final TypeAdapterFactory LOCALE_FACTORY = newFactory(Locale.class, LOCALE); Enum的泛型实现可以看下jackson的实现，以后枚举字段可以直接用枚举值，注解使用code来序列化（不过还要注意vo和po的转换，DAO层和数据库是怎么支持枚举的？） class1.isAssignableFrom(class2) 表示 class1是class2的超类或本身 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152private static final class EnumTypeAdapter&lt;T extends Enum&lt;T&gt;&gt; extends TypeAdapter&lt;T&gt; &#123; // 序列化名称到枚举的映射 private final Map&lt;String, T&gt; nameToConstant = new HashMap&lt;String, T&gt;(); // 枚举到名称的映射，如果有SerializedName的注解，用注解的名称，否则用枚举自身的名称 private final Map&lt;T, String&gt; constantToName = new HashMap&lt;T, String&gt;(); public EnumTypeAdapter(Class&lt;T&gt; classOfT) &#123; try &#123; // classOfT.getEnumConstants() 获取枚举 for (T constant : classOfT.getEnumConstants()) &#123; String name = constant.name(); SerializedName annotation = classOfT.getField(name).getAnnotation(SerializedName.class); if (annotation != null) &#123; name = annotation.value(); for (String alternate : annotation.alternate()) &#123; nameToConstant.put(alternate, constant); &#125; &#125; nameToConstant.put(name, constant); constantToName.put(constant, name); &#125; &#125; catch (NoSuchFieldException e) &#123; throw new AssertionError(e); &#125; &#125; @Override public T read(JsonReader in) throws IOException &#123; if (in.peek() == JsonToken.NULL) &#123; in.nextNull(); return null; &#125; return nameToConstant.get(in.nextString()); &#125; @Override public void write(JsonWriter out, T value) throws IOException &#123; out.value(value == null ? null : constantToName.get(value)); &#125;&#125;public static final TypeAdapterFactory ENUM_FACTORY = new TypeAdapterFactory() &#123; @SuppressWarnings(&#123;\"rawtypes\", \"unchecked\"&#125;) @Override public &lt;T&gt; TypeAdapter&lt;T&gt; create(Gson gson, TypeToken&lt;T&gt; typeToken) &#123; Class&lt;? super T&gt; rawType = typeToken.getRawType(); // 判断Enum是rawType的超类 if (!Enum.class.isAssignableFrom(rawType) || rawType == Enum.class) &#123; return null; &#125; if (!rawType.isEnum()) &#123; rawType = rawType.getSuperclass(); // handle anonymous subclasses &#125; return (TypeAdapter&lt;T&gt;) new EnumTypeAdapter(rawType); &#125;&#125;; InetAdress和JsonElement是用超类Adapter工作的 基本类型的factory和adapterLong/Float/Double的adapter根据序列化自定义而不同 复杂类型的factory和adapter CollectionTypeAdapterFactory 集合类型ReflectiveTypeAdapterFactory 自定义对象TypeAdapterRuntimeTypeWrapper是对TypeAdapter的封装，用于write运行时判断更准确的类型。 在array/collection/map/自定义类型等adapter中使用。 比如Object a = new String(“aaa”)，实际上应使用StringTypeAdapter来write。 read时，以什么样的类型赋值给a？ TypeToken匿名内部类有两种语法格式 new 接口(){} new 父类构造器(参数列表){}TypeToken为第二种new TypeToken&lt;List&lt;TwoGeneric&lt;Integer,User&gt;&gt;&gt;(){};得到的是TypeToken&lt;List&lt;TwoGeneric&lt;Integer,User&gt;&gt;&gt;的匿名子类。 $Gson$Types123456789101112131415161718192021222324252627282930313233public static Class&lt;?&gt; getRawType(Type type) &#123; if (type instanceof Class&lt;?&gt;) &#123; // type is a normal class. return (Class&lt;?&gt;) type; &#125; else if (type instanceof ParameterizedType) &#123; ParameterizedType parameterizedType = (ParameterizedType) type; // I'm not exactly sure why getRawType() returns Type instead of Class. // Neal isn't either but suspects some pathological case related // to nested classes exists. Type rawType = parameterizedType.getRawType(); checkArgument(rawType instanceof Class); return (Class&lt;?&gt;) rawType; &#125; else if (type instanceof GenericArrayType) &#123; Type componentType = ((GenericArrayType)type).getGenericComponentType(); return Array.newInstance(getRawType(componentType), 0).getClass(); &#125; else if (type instanceof TypeVariable) &#123; // we could use the variable's bounds, but that won't work if there are multiple. // having a raw type that's more general than necessary is okay return Object.class; &#125; else if (type instanceof WildcardType) &#123; return getRawType(((WildcardType) type).getUpperBounds()[0]); &#125; else &#123; String className = type == null ? \"null\" : type.getClass().getName(); throw new IllegalArgumentException(\"Expected a Class, ParameterizedType, or \" + \"GenericArrayType, but &lt;\" + type + \"&gt; is of type \" + className); &#125;&#125; 序列化与反序列化结果类型对比 wildcard类型，根据上界反序列化。upperBoundList的上界是TestInnerObject，?是TestInnerSonObject，反序列化丢失了innerSonId的值。lowerBoundList的上界是Object，反序列化为map 声明的类型为Object，write正常，read为map 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758@Testpublic void testGson() &#123; Gson gson = new Gson(); TestInnerObject testInnerObject = new TestInnerObject(); testInnerObject.setInnerId(\"inner\"); TestInnerParentObject testInnerParentObject = new TestInnerParentObject(); testInnerParentObject.setInnerParentId(\"parent\"); TestInnerSonObject testInnerSonObject = new TestInnerSonObject(); testInnerSonObject.setInnerSonId(\"son\"); TestObject testObject = new TestObject(); testObject.setString(\"mock\"); testObject.setLongList(Lists.newArrayList(1L, 2L)); Map&lt;String, TestInnerObject&gt; map = Maps.newHashMap(); map.put(\"mockKey\", testInnerObject); testObject.setMap(map); testObject.setUpperBoundList(Lists.newArrayList(testInnerSonObject)); testObject.setLowerBoundList(Lists.newArrayList(testInnerParentObject)); testObject.setObject(testInnerSonObject); testObject.setLists(new List[]&#123;Lists.newArrayList(testInnerObject)&#125;); String json = gson.toJson(testObject); log.info(\"json = &#123;&#125;\", json); testObject = gson.fromJson(json, TestObject.class); log.info(\"testObject = &#123;&#125;\", testObject);&#125;@Datapublic class TestObject &#123; private String string; private List&lt;Long&gt; longList; private List&lt;TestInnerObject&gt; testInnerObjectList; private Map&lt;String, TestInnerObject&gt; map; private List&lt;? extends TestInnerObject&gt; upperBoundList; private List&lt;? super TestInnerObject&gt; lowerBoundList; private TestObject[] testObjectArray; private List[] lists; private Object object;&#125;@Datapublic class TestInnerObject extends TestInnerParentObject &#123; private String innerId;&#125;@Datapublic class TestInnerParentObject &#123; private String innerParentId;&#125;@Datapublic class TestInnerSonObject extends TestInnerObject &#123; private String innerSonId;&#125; todo map 复杂key的序列化 map peek == JsonToken.BEGIN_ARRAY reflectiveTypeAdapter 多个泛型 [done. read时根据name和TypeToken，write时根据TypeAdapterRuntimeTypeWrapper] toJson时候，type里带实例的runtime type? [done. TypeAdapterRuntimeTypeWrapper处理] JsonAdapterAnnotationTypeAdapterFactory [almost done. 获取filed上注解的TypeAdapter进行后续处理] $Gson$Types.resolve [done] JsonWriter和JsonReader的读写 [almost done. 写比较简单，不同类型输出；读在fillBuffer时先读到缓存中，不同类型在缓存中操作。] read之后怎么赋值？ 装箱和非装箱，都是用的装箱adapter，返回装箱类型。—-那装箱类型赋值给非装箱类型吗？用set还是set方法？ TypeToken的rawType是怎么取到的？ gson.getAdapter(Date.class)的逻辑？","tags":[{"name":"Gson","slug":"Gson","permalink":"https://northernw.github.io/tags/Gson/"}]},{"title":"OKHttp源码解析和设计模式","date":"2019-07-16T20:00:47.000Z","path":"2019/07/17/OKHttp源码解析和设计模式/","text":"","tags":[{"name":"OKHttp","slug":"OKHttp","permalink":"https://northernw.github.io/tags/OKHttp/"}]},{"title":"剑指offer-面试需要的基本知识","date":"2019-07-15T15:35:14.000Z","path":"2019/07/15/剑指offer-面试需要的基本知识/","text":"编程语言单例模式java版好的解法 懒汉模式，field直接创建实例 volatile+双重检查，加volatile是防止其他线程在本线程刚分配内存、还未初始化时就得到instance 静态内部类，内部类中在filed创建实例，实现在使用时才创建 数据结构数组 找出数组中重复的数字。 最简单的是哈希表，时间O(1)，空间O(n). 如果空间优先，允许修改原数组。遍历数组的过程中，判断值m与下标m的位置的值是否相同，相同，找到；不相同，替换值m与下标m位置的值。时间O(n)，时间O(1). 空间优先，不允许修改原数组。利用二分查找的思想。将数组与n的中值比较，判断出重复的数是小于还是大于中值，继续二分中值。 二维数组中的查找，行从左到右升序，列从上到下升序，判断是否存在给定的数。clue：从右上或左下开始判断。 字符串注意某些情形下，从后往前处理，可以减少移动的次数。 链表链表创建、插入、删除。 树 遍历 （根节点所在位置）前序，中序，后序 宽度优先：利用队列，存储当前深度的节点 重建二叉树 二叉树的下一个节点 栈和队列 相互实现 算法和数据操作递归和循环递归的实现方式代码简洁，但性能不如循环的实现方式 查找和排序重点掌握二分查找、归并排序和快速排序 回溯法在二维数组上搜索路径 动态规划与贪婪算法动态规划：求某个问题的最优解，且问题可以分为多个子问题。（自上而下：递归，自下而上：循环）贪婪算法：存在特殊的选择，一定能得到最优解。 位运算与、或、异或、左移、右移。","tags":[{"name":"剑指offer","slug":"剑指offer","permalink":"https://northernw.github.io/tags/剑指offer/"}]},{"title":"剑指offer-基础知识","date":"2019-07-14T16:06:22.000Z","path":"2019/07/15/剑指offer-基础知识/","text":"编程语言pass 实现Singleton模式 java volatile+双重检查 内部类 数据结构二维数组中的查找算法和数据操作 递归和循环 斐波那契数列及实际问题的数学建模 查找和排序 查找：顺序查找、二分查找、哈希表查找和二叉排序树查找 排序：插入排序、冒泡排序、归并排序、快速排序 比较优劣：空间消耗、平均时间复杂度、最差时间复杂度 回溯法 动态规划与贪婪算法 位运算","tags":[{"name":"剑指offer","slug":"剑指offer","permalink":"https://northernw.github.io/tags/剑指offer/"}]},{"title":"Spring Framework Reference Documentation","date":"2019-07-11T16:18:27.000Z","path":"2019/07/12/Spring-Framework-Reference-Documentation/","text":"Overview","tags":[{"name":"spring","slug":"spring","permalink":"https://northernw.github.io/tags/spring/"}]},{"title":"关于ExceptionHandler与ResponseBody的一次排查","date":"2019-07-11T14:41:49.000Z","path":"2019/07/11/关于ExceptionHandler与ResponseBody的一次排查/","text":"问题工作中，在使用@ExceptionHandler作为controller全局异常处理的过程中，发现@ResponseBody注解并不能像往常一样返回json格式的数据，而是返回了xml格式的数据。 结论错误认识 @ResponseBody不意味着json格式响应，而是指controller的返回结果直接写回response，无需经过ModelAndView。见spring文档 The @ResponseBody annotation is similar to @RequestBody. This annotation can be placed on a method and indicates that the return type should be written straight to the HTTP response body (and not placed in a Model, or interpreted as a view name) 新认识 HttpMessageConverter才是进行数据格式转换的关键 经过试错发现，在request header中加入Accept application/json可以使原问题得到正确的json响应 遗留问题 工程中其他请求的request header中没有Accept application/json也能得到json响应，是为什么？ 记录下排查过程 怀疑@ResponseBody注解未生效 查阅后得知spring3.1版本已修复 （知道自己对@ResponseBody有误解后，就明白这并不是个问题，xml响应也是注解的结果） 百思不得其解，在方法中使用HttpServletResponse response直接输出json响应暴力解决。仔细想想，本质上是一致的。 12response.setContentType(&quot;application/json;charset=UTF-8&quot;);response.getWriter().write(gson.toJson(re)); 排除spring版本影响 工程依赖过多，更改版本错误重重，临时用springboot搭建了web，使用4.3.9.RELEASE（原工程版本）和5.1.8.RELEASE，配合@RestController，无需Accept application/json就能得到json响应 关于遗留问题工程中其他请求的request header中没有Accept application/json也能得到json响应，是为什么？ 差别在于，异常处理与正常返回用到的RequestResponseBodyMethodProcessor不是同一个实例，异常的是在ExceptionHandlerExceptionResolver中的，正常的是在RequestMappingHandlerAdapter.前者比之后者，缺少springmvc-config.xml中自定义的MessageConverter，例如MappingJackson2HttpMessageConverter。 123456789101112&lt;bean class=\"org.springframework.http.converter.json.MappingJackson2HttpMessageConverter\"&gt; &lt;property name=\"supportedMediaTypes\"&gt; &lt;list&gt; &lt;value&gt;application/json;charset=UTF-8&lt;/value&gt; &lt;value&gt;text/html;charset=UTF-8&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name=\"objectMapper\"&gt; &lt;bean class=\"com.jd.common.xss.CustomObjectMapper\" /&gt; &lt;/property&gt;&lt;/bean&gt; 在writeWithMessageConverters方法中，异常resolver返回的b有7个，排序后最前的是application/xml，正常resolver返回9个，排序最前是上面自定义的application/json;charset=UTF-8，因此，最终返回结果有格式上的差异。AbstractMessageConverterMethodProcessor#writeWithMessageConverters123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081HttpServletRequest request = inputMessage.getServletRequest();// 根据request的accept得到请求支持的返回格式 aList&lt;MediaType&gt; requestedMediaTypes = getAcceptableMediaTypes(request);// 得到controller方法支持的返回格式 b &lt;-- 差别在这里List&lt;MediaType&gt; producibleMediaTypes = getProducibleMediaTypes(request, valueType, declaredType);if (outputValue != null &amp;&amp; producibleMediaTypes.isEmpty()) &#123; throw new IllegalArgumentException(\"No converter found for return value of type: \" + valueType);&#125;// 对比a和b，得到所有兼容的返回格式 cSet&lt;MediaType&gt; compatibleMediaTypes = new LinkedHashSet&lt;MediaType&gt;();for (MediaType requestedType : requestedMediaTypes) &#123; for (MediaType producibleType : producibleMediaTypes) &#123; if (requestedType.isCompatibleWith(producibleType)) &#123; compatibleMediaTypes.add(getMostSpecificMediaType(requestedType, producibleType)); &#125; &#125;&#125;if (compatibleMediaTypes.isEmpty()) &#123; if (outputValue != null) &#123; throw new HttpMediaTypeNotAcceptableException(producibleMediaTypes); &#125; return;&#125;// 将c排序List&lt;MediaType&gt; mediaTypes = new ArrayList&lt;MediaType&gt;(compatibleMediaTypes);MediaType.sortBySpecificityAndQuality(mediaTypes);// 从c中得到排序较靠前的某个Concrete返回格式 dMediaType selectedMediaType = null;for (MediaType mediaType : mediaTypes) &#123; if (mediaType.isConcrete()) &#123; selectedMediaType = mediaType; break; &#125; else if (mediaType.equals(MediaType.ALL) || mediaType.equals(MEDIA_TYPE_APPLICATION)) &#123; selectedMediaType = MediaType.APPLICATION_OCTET_STREAM; break; &#125;&#125;// 以下根据d得到对应的messageConverter，输出相应格式的结果if (selectedMediaType != null) &#123; selectedMediaType = selectedMediaType.removeQualityValue(); for (HttpMessageConverter&lt;?&gt; messageConverter : this.messageConverters) &#123; if (messageConverter instanceof GenericHttpMessageConverter) &#123; if (((GenericHttpMessageConverter) messageConverter).canWrite( declaredType, valueType, selectedMediaType)) &#123; outputValue = (T) getAdvice().beforeBodyWrite(outputValue, returnType, selectedMediaType, (Class&lt;? extends HttpMessageConverter&lt;?&gt;&gt;) messageConverter.getClass(), inputMessage, outputMessage); if (outputValue != null) &#123; addContentDispositionHeader(inputMessage, outputMessage); ((GenericHttpMessageConverter) messageConverter).write( outputValue, declaredType, selectedMediaType, outputMessage); if (logger.isDebugEnabled()) &#123; logger.debug(\"Written [\" + outputValue + \"] as \\\"\" + selectedMediaType + \"\\\" using [\" + messageConverter + \"]\"); &#125; &#125; return; &#125; &#125; else if (messageConverter.canWrite(valueType, selectedMediaType)) &#123; outputValue = (T) getAdvice().beforeBodyWrite(outputValue, returnType, selectedMediaType, (Class&lt;? extends HttpMessageConverter&lt;?&gt;&gt;) messageConverter.getClass(), inputMessage, outputMessage); if (outputValue != null) &#123; addContentDispositionHeader(inputMessage, outputMessage); ((HttpMessageConverter) messageConverter).write(outputValue, selectedMediaType, outputMessage); if (logger.isDebugEnabled()) &#123; logger.debug(\"Written [\" + outputValue + \"] as \\\"\" + selectedMediaType + \"\\\" using [\" + messageConverter + \"]\"); &#125; &#125; return; &#125; &#125;&#125; 再来看看ExceptionHandlerExceptionResolver和RequestMappingHandlerAdapter中的RequestResponseBodyMethodProcessor为什么有不一样的messageConverters呢？ 跟踪不到setMessageConverters的直接调用，两个类的setMessageConverters都分别调用了2次，一次7个，一次9个，AnnotationDrivenBeanDefinitionParser#parse也分别执行了两次，一次7，一次9，可以推测是这个类触发了eher和rmhq的实例化. 123public void setMessageConverters(List&lt;HttpMessageConverter&lt;?&gt;&gt; messageConverters) &#123; this.messageConverters = messageConverters;&#125; 那么，问题又回到，为什么异常处理的时候，用到的eher实例是7个的那个呢？ 看不动了，缓缓。需要从上至下了解spring的启动。","tags":[{"name":"springmvc","slug":"springmvc","permalink":"https://northernw.github.io/tags/springmvc/"},{"name":"springboot","slug":"springboot","permalink":"https://northernw.github.io/tags/springboot/"},{"name":"ExceptionHandler","slug":"ExceptionHandler","permalink":"https://northernw.github.io/tags/ExceptionHandler/"},{"name":"ResponseBody","slug":"ResponseBody","permalink":"https://northernw.github.io/tags/ResponseBody/"}]},{"title":"剑指offer-关于面试","date":"2019-07-10T11:08:04.000Z","path":"2019/07/10/剑指offer-关于面试/","text":"面试的形式 电话面试 环境安静 用形象化的语言把细节说清楚 共享桌面远程面试：考察编程习惯和调试能力 思考清楚再编码 良好的代码命名和缩进对齐习惯 能够进行单元测试：断点，单步跟踪，查看内存，分析调用栈 现场面试 不迟到 注意面试流程 准备几个问题 面试的环节 行为面试：5~10分钟，性格特点，项目经历，暖场 30s~1min 简单自我介绍：主要学习、工作经历 若详细问项目，STAR模型 Situation：项目背景 Task：自己完成的任务 Action：为完成任务自己做了哪些工作，是怎么做的 Result：自己的贡献 其他常见问题 该项目中碰到的最大问题是什么，怎么解决的？ 从这个项目中学到了什么？ 什么时候会和其他团队成员（RD、QA、UED、PM）有什么样的冲突，你们是怎么解决冲突的？ 掌握的技能 了解：只上过课或看过书，没有做过实际项目，不建议列在简历上，除非是应聘职位需要 熟悉：实际项目中使用某项技术很长时间，通过查阅文档可以独立解决大部分问题 项目开发过程中用到的技能，可以用“熟悉” 毕业设计用到的技能 为什么跳槽？ no: 老板太苛刻，同事太难相处，加班太频繁，工资太低 ok: 现在的工作做了一段时间，已经没有太多的激情了，因此希望寻找一份更有挑战的工作。再论述为什么有些厌倦现在的职位，以及面试的职位我为什么会有兴趣。 技术面试：40~50分钟 基础知识扎实全面，包括编程语言、数据结构、算法等 能写出正确的、完整的、鲁棒的高质量代码 能思路清晰地分析、解决复杂问题 能从时间、空间复杂度两方面优化算法效率 具备优秀的沟通能力、学习能力、发散思维能力等 应聘者提问 问与职位或项目相关的问题 不要问薪水 不要打听面试结果","tags":[{"name":"剑指offer","slug":"剑指offer","permalink":"https://northernw.github.io/tags/剑指offer/"},{"name":"面试","slug":"面试","permalink":"https://northernw.github.io/tags/面试/"}]},{"title":"循序渐进Linux（5）-软件安装与管理","date":"2019-07-05T15:47:01.000Z","path":"2019/07/05/循序渐进Linux（5）-软件安装与管理/","text":"源码安装 下载解压 wget tar.. ./configure, Makefile make make install rpm安装 rpm -i[vh] file1.rpm file2.rpm 安装 rpm -q[ag] package1…packageN 查询 rpm -v[..] package1…验证 -K: –checksig rpm -U[] file1.rpm .. fileN.rpm 更新 rpm -e[] package1…N 删除 e=erase yum安装yellowdog updater modified 使用rpm安装yum 配置yum：包括资源镜像列表等 yum维护软件间的依赖性，可同时配置多个资源库，配置简单明了，保持与RPM数据库的一致性 yum的基本用法 安装 yum install xx 更新 yum check-update yum update yum update kernel kernel-source yum upgrade … 查询 yum info yum info vsftpd yum info perl* 可使用通配符 yum info updates yum info installed yum list updates yum list yum list gcc* … 不错的yum源 EPEL(Extra Packages for Enterprise Linux，企业版linux附加包)，epel-release RPMForge, rpmforge-release 二进制软件安装一般直接解压，或执行setup install install.sh等命令或脚本","tags":[{"name":"linux","slug":"linux","permalink":"https://northernw.github.io/tags/linux/"},{"name":"读书笔记","slug":"读书笔记","permalink":"https://northernw.github.io/tags/读书笔记/"},{"name":"循序渐进linux","slug":"循序渐进linux","permalink":"https://northernw.github.io/tags/循序渐进linux/"}]},{"title":"循序渐进Linux（4）-常用命令及使用技巧","date":"2019-07-04T20:21:39.000Z","path":"2019/07/05/循序渐进Linux（4）-常用命令及使用技巧/","text":"shell简介系统管理与维护 ls pwd cd date passwd su clear man who uname uptime last dmesg free ps: ps -ef, ps aux top 文件管理与编辑 mkdir more cat diff grep rm touch ln file cp find split mv 压缩与解压4.4 暂停 磁盘管理与维护网络设置与维护文本编辑vi","tags":[{"name":"linux","slug":"linux","permalink":"https://northernw.github.io/tags/linux/"},{"name":"读书笔记","slug":"读书笔记","permalink":"https://northernw.github.io/tags/读书笔记/"},{"name":"循序渐进linux","slug":"循序渐进linux","permalink":"https://northernw.github.io/tags/循序渐进linux/"}]},{"title":"循序渐进Linux（2）-系统基本结构","date":"2019-07-04T12:00:28.000Z","path":"2019/07/04/循序渐进Linux（2）-系统基本结构/","text":"控制台的使用控制台 桌面控制台 字符控制台 默认有6个字符控制台，独立作业，互不影响。 图形界面切换到字符界面：ctrl+alt+F1~F6 字符界面切换到图形界面：在字符界面输入命令“startx”或者ctrl+alt+F7组合键 系统与硬件硬件资源管理 查看系统PCI设备lspci 查看CPU信息more /proc/cpuinfo查看系统物理CPU个数cat /proc/cpuinfo | grep &quot;physical id&quot; | sort | uniq | wc -l 查看系统内存信息more /proc/meminfo 查看磁盘分区信息fdisk -l 外在设备的使用挂载mount -t 文件系统类型 设备名 挂载目录临时挂载点/mnt，手动挂载/media，自动挂载/run，CentOS 7.x版本自动挂载目录，所有移动设备自动挂载到该目录下例如：挂载U盘，假如设备名为/dev/sda1，建立/mnt/usb进行挂载mount -t vfat /dev/sda1 /mnt/usb 卸载umount 挂载目录例如：umount /mnt/usb 文件系统结构目录结构经典树形目录 常见目录结构： /boot 存放启动linux的核心文件 /bin和/sbin 存放可执行的二进制文件 /sbin s: super user，存放只有超级用户才能执行的命令 /home 系统中每个用户的工作目录 /lib 存放共享程序库和映像文件，供很多程序使用 /root 超级用户root的默认主目录。一般用户没有进入这个目录的权限 /run 外设的自动挂载点目录 /lost+found 保存丢失的文件。不恰当的关机操作和磁盘操作均会导致文件丢失，这些会丢失的文件会临时放在/lost+found下，系统重启后，fsck程序能发现这些文件。 /tmp 临时文件目录 系统核心组成 内存管理：合理有效地管理整个系统的物理内存，同时快速响应内核各个子系统对内存分配的请求。 进程管理：控制系统进程对CPU的访问。进程调度。 进程间通信：控制不同进程之间在用户空间的同步、数据共享和交换。例如，一个进程在等待硬件操作时是挂起的，等硬件操作完成，进程被恢复执行。协调这个过程的就是进程间的通信机制。 虚拟文件系统：用一个通用的文件模型表示各种不同的文件系统，屏蔽了具体文件系统的差异。分为逻辑文件系统和设备驱动程序。前者指linux支持的文件系统，如ext2、ext3、xfs等，后者指为每一种硬件控制器所编写的设备驱动程序模块。 网络接口：提供了对各种网络标准的实现和各种网络硬件的支持。 运行机制 init系统，初始化 runlevel与target 系统关机过程 系统服务管理工具systemd 启动、停止、重启服务 查看、禁止、启用服务 systemd与sysvinit命令对比 SecureCRT使用","tags":[{"name":"linux","slug":"linux","permalink":"https://northernw.github.io/tags/linux/"},{"name":"读书笔记","slug":"读书笔记","permalink":"https://northernw.github.io/tags/读书笔记/"},{"name":"循序渐进linux","slug":"循序渐进linux","permalink":"https://northernw.github.io/tags/循序渐进linux/"}]},{"title":"循序渐进Linux（1）","date":"2019-07-03T15:56:10.000Z","path":"2019/07/03/循序渐进Linux-1/","text":"key words单靠人工无法满足技术、业务、管理方面的要求重视标准化、自动化、稳定性、可靠性等需求 分区硬盘命名方案基于文件，一般命名如下：/dev/hda2/dev/sdb3 /dev：所有设备文件的存放目录 hd和sd: 代表分区所在设备类型，hd表示IDE硬盘，sd表示SCSI硬盘 a：表示分区在哪个设备上。/dev/hda表示第一块IDE硬盘，/dev/sdb表示第二块SCSI硬盘，/dev/sdd表示第四块SCSI硬盘，以此类推。 2：可以理解为第几个分区。linux前4个分区为主分区或扩展分区，逻辑分区从5开始。例如，/dev/hda2表示第一块IDE硬盘的第二个主分区或扩展分区，/dev/sdc6表示第三块SCSI硬盘的第二个逻辑分区。 其他常见分区 /boot: 存储系统的引导信息和内核信息等 /usr: 存储系统应用软件的安装信息 /var: 存储系统的日志信息","tags":[{"name":"linux","slug":"linux","permalink":"https://northernw.github.io/tags/linux/"},{"name":"读书笔记","slug":"读书笔记","permalink":"https://northernw.github.io/tags/读书笔记/"},{"name":"循序渐进linux","slug":"循序渐进linux","permalink":"https://northernw.github.io/tags/循序渐进linux/"}]},{"title":"ConcurrentHashMap源码笔记","date":"2019-06-27T11:09:41.000Z","path":"2019/06/27/ConcurrentHashMap源码笔记/","text":"spread假设table的长度为n=2^k，取模操作hash%n等价于hash&amp;(n-1)，n-1为mask(二进制的k-1个1)即，hash的低k位决定了桶的位置，k位以上的高位不起作用，如果不同hash的低k位相同，就会产生碰撞 12345static final int spread(int h) &#123; // 将高16位与低16位异或，增加hash的分散度，降低碰撞概率 // 与上HASH_BITS，将最高位置为0，使spread结果为正数 return (h ^ (h &gt;&gt;&gt; 16)) &amp; HASH_BITS;&#125; put循环做4件事情： 如果表为空，初始化表，继续 如果对应下标节点为空，cas插入，跳出 如果发现表正在迁移，帮助迁移，继续 执行put操作。如果链表长度超过阈值，转为树。如果为更新，直接返回，否则跳出 循环退出后，计数+112345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394public V put(K key, V value) &#123; return putVal(key, value, false);&#125;/** * Implementation for put and putIfAbsent * onlyIfAbsent: true 只替换为null值的node, false: 都替换 */final V putVal(K key, V value, boolean onlyIfAbsent) &#123; // key value 不能为null if (key == null || value == null) throw new NullPointerException(); // 计算hash值 int hash = spread(key.hashCode()); int binCount = 0; // 循环执行 for (Node&lt;K, V&gt;[] tab = table; ; ) &#123; Node&lt;K, V&gt; f; int n, i, fh; // 如果table为空，初始化 if (tab == null || (n = tab.length) == 0) tab = initTable(); // 如果对应下标节点为空，直接插入 else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; // cas替换 if (casTabAt(tab, i, null, new Node&lt;K, V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin // 如果节点不为空，且节点的hash为-1，-1表示在迁移，则帮助迁移 &#125; else if ((fh = f.hash) == MOVED) // 此处，helpTransfer返回的tab为f的nextTable，或者为已完整迁移的新table tab = helpTransfer(tab, f); // 执行put操作 else &#123; V oldVal = null; // 请求同步锁，避免并发写操作 synchronized (f) &#123; // double check if (tabAt(tab, i) == f) &#123; // 链表结构 if (fh &gt;= 0) &#123; // 统计节点个数（非精确）-- 统计的是原来链表的长度 // for中break之后，不会再执行++binCount // f的binCount为1，第二个节点binCount为2 // 假设第k节点key相同，替换新值后break，binCount为k，链表长度未知 // 或者第k节点的next为空，链接上新节点后break，binCount为k，链表长度为k+1 binCount = 1; for (Node&lt;K, V&gt; e = f; ; ++binCount) &#123; K ek; // 找到相同的key，更新value if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; // 否则添加到链表尾部 Node&lt;K, V&gt; pred = e; if ((e = e.next) == null) &#123; pred.next = new Node&lt;K, V&gt;(hash, key, value, null); break; &#125; &#125; &#125; else if (f instanceof TreeBin) &#123; // 红黑树结构，TreeBin的hash为-2 Node&lt;K, V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K, V&gt;) f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; if (binCount != 0) &#123; // 如果链表节点个数大于阈值，将链表转化为红黑树 // 如果原结构为红黑树，binCount=2 if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); // oldVal不为null，说明是更新操作，节点个数无变化，直接返回 if (oldVal != null) return oldVal; break; &#125; &#125; &#125; // 到这里说明是添加操作，计数+1 addCount(1L, binCount); return null;&#125; initTable只有一个线程能进行初始化，其他线程让出CPU1234567891011121314151617181920212223242526272829303132333435/** * Initializes table, using the size recorded in sizeCtl. */private final Node&lt;K, V&gt;[] initTable() &#123; Node&lt;K, V&gt;[] tab; int sc; while ((tab = table) == null || tab.length == 0) &#123; // sc小于0，说明正在扩容或者迁移，让出cpu if ((sc = sizeCtl) &lt; 0) Thread.yield(); // lost initialization race; just spin else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; // 当前线程获得初始化机会，cas sizeCtl为-1 try &#123; if ((tab = table) == null || tab.length == 0) &#123; // 如果sizeCtl有正值，用正值，否则采用默认容量 // sizeCtl什么时候有正值？有什么样的正值？ int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings(\"unchecked\") Node&lt;K, V&gt;[] nt = (Node&lt;K, V&gt;[]) new Node&lt;?, ?&gt;[n]; // 此时tab=table!=null，其他线程从本方法的循环中跳出 table = tab = nt; // 相当于n - n*1/4，即sc = 0.75*n // &gt;&gt;&gt; 无符号右移 sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; // 初始化完成后，sizeCtl=0.75*n sizeCtl = sc; &#125; // 初始化完成跳出循环 break; &#125; &#125; return tab;&#125; addCount对于check，从putVal过来的几种情形（这里check=binCount） table中槽为空，直接放入新节点，check=0 table中槽里的节点（称为first节点）key和新值一样，被替换，check=1 – 这种情况在putVal直接return了，不会进到addCount 非first节点的key和新值一样，或者加入了新节点，check&gt;1 – 替换的也return了，只有新节点才进入addCount，这是check=binCount=原链表长度 槽中是红黑树，check=2 总结下，这里check的值有0、k（&gt;1）、2 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778/** * Adds to count, and if table is too small and not already * resizing, initiates transfer. If already resizing, helps * perform transfer if work is available. Rechecks occupancy * after a transfer to see if another resize is already needed * because resizings are lagging additions. * * @param x the count to add * @param check if &lt;0, don't check resize, if &lt;= 1 only check if uncontended */ private final void addCount(long x, int check) &#123; CounterCell[] as; long b, s; // 如果计数表不为空 // 或者cas baseCount失败，说明存在并发竞争 if ((as = counterCells) != null || !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) &#123; CounterCell a; long v; int m; // 假设无竞争 boolean uncontended = true; // 如果计数表为空 // 如果计数表长度小于1 // 如果计数表随机位为null // 如果随机位不为空，且cas替换计数失败，说明有竞争 if (as == null || (m = as.length - 1) &lt; 0 || (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null || !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) &#123; // 大概作用是加入了一个新的计数cell // 上述if第4个cas失败了，在fullAddCount会重新生成一个随机数，再把统计放入对应计数位 fullAddCount(x, uncontended); return; &#125; // if &lt;0, don't check resize, if &lt;= 1 only check if uncontended if (check &lt;= 1) return; // 累加baseCount和计数表里的值 s = sumCount(); &#125; // 判断是否要扩容 if (check &gt;= 0) &#123; Node&lt;K, V&gt;[] tab, nt; int n, sc; // 如果节点个数大于阈值，0.75n，类似于加载因子 // 并且table不为空 &amp;&amp; table长度未超过上限 while (s &gt;= (long) (sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123; // 获取表长度的一个标识值 int rs = resizeStamp(n); // 如果在扩容，判断是否需要帮助迁移 if (sc &lt; 0) &#123; // 如果sc的高16位与标识符不等 // bug report: https://bugs.java.com/bugdatabase/view_bug.do?bug_id=JDK-8214427 // sc == rs + 1，存在bug，正确判断为 sc == (rs &lt;&lt; RESIZE_STAMP_SHIFT + 1)，判断已无扩容线程 // sc == rs + MAX_RESIZERS，正确判断为 sc == (rs &lt;&lt; RESIZE_STAMP_SHIFT + MAX_RESIZERS)，判断扩容线程数已达最大 // 如果临时表nextTable为空，或者迁移下标transferIndex小于0，说明扩容结束 if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) // 无需帮助扩容 break; // 否则，将sc cas为sc+1，表示sc低16位加1，即扩容线程数增加了一个 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) // 扩容 transfer(tab, nt); &#125; // 不处于扩容状态，cas sizeCtl为(rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)，标识有1个线程在扩容 // sizeCtl的高16位存储待扩容表长n的标识符，低16位存储[扩容线程数+1] else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) // 扩容 transfer(tab, null); s = sumCount(); &#125; &#125;&#125; sizeCtl的注释说明，当sizeCtl为负数时，-1标识表初始化，-(sizeCtl-1)标识活动的扩容线程数为什么在具体实现里，是sizeCtl的低16位，且需减去1的值，标识扩容线程数呢？((rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)这里有个疑问，为什么是+2？不能是+1么？ – 无责任猜测是版本变更了，最初也许没有RESIZE_STAMP_SHIFT、没有高低16位这么复杂 如果是+1，假如表长为0，第一个扩容线程加入，sizeCtl=[1.....0](16位)+[0....1](16位)=很负的一个负数，除非sizeCtl溢出了，不然也没发现其他情况下有重复的情况 +2的话，sizeCtl=[1.....0](16位)+[0....10](16位)=还是很负的一个负数，和+1只有最低位有区别。 123456789/** * Table initialization and resizing control. When negative, the * table is being initialized or resized: -1 for initialization, * else -(1 + the number of active resizing threads). Otherwise, * when table is null, holds the initial table size to use upon * creation, or 0 for default. After initialization, holds the * next element count value upon which to resize the table. */private transient volatile int sizeCtl; resizeStampresizeStamp的结果作为扩容&amp;迁移时sizeCtl的高16位信息 sizeCtl为负数 标识着此次扩容&amp;迁移对应的表长n123456static final int resizeStamp(int n) &#123; // n的二进制前导0个数。因为表长n为2的次幂，每次扩容*2，意味着每次扩容前导0个数少1，用于判断是否为同一次扩容 // 将第16位或为1，是为了左移RESIZE_STAMP_SHIFT后为负数 // 1 &lt;&lt; (RESIZE_STAMP_BITS - 1) = (0b)1000 0000 0000 0000 return Integer.numberOfLeadingZeros(n) | (1 &lt;&lt; (RESIZE_STAMP_BITS - 1));&#125; helpTransfer1234567891011121314151617181920212223242526272829/** * Helps transfer if a resize is in progress. */final Node&lt;K, V&gt;[] helpTransfer(Node&lt;K, V&gt;[] tab, Node&lt;K, V&gt; f) &#123; Node&lt;K, V&gt;[] nextTab; int sc; // 重新判断，旧表不为空，f为迁移节点，f关联的新表不为空 if (tab != null &amp;&amp; (f instanceof ForwardingNode) &amp;&amp; (nextTab = ((ForwardingNode&lt;K, V&gt;) f).nextTable) != null) &#123; // 同addCount，循环判断是否需要帮助扩容 int rs = resizeStamp(tab.length); while (nextTab == nextTable &amp;&amp; table == tab &amp;&amp; (sc = sizeCtl) &lt; 0) &#123; // 不满足帮助扩容条件，跳出 if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || transferIndex &lt;= 0) break; // 否则，将sc低位cas+1（标识多一个线程扩容）成功后，帮助扩容 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) &#123; transfer(tab, nextTab); break; &#125; &#125; // 返回新表 return nextTab; &#125; // 返回，此时table已是完成扩容的表 return table;&#125; transfer123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164private final void transfer(Node&lt;K, V&gt;[] tab, Node&lt;K, V&gt;[] nextTab) &#123; int n = tab.length, stride; // 计算步长，最短为16，最长为n（单CPU） if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range // 如果新表为空，先建新表，容量为2n if (nextTab == null) &#123; // initiating try &#123; @SuppressWarnings(\"unchecked\") Node&lt;K, V&gt;[] nt = (Node&lt;K, V&gt;[]) new Node&lt;?, ?&gt;[n &lt;&lt; 1]; nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; &#125; nextTable = nextTab; // 从后向前迁移 transferIndex = n; &#125; int nextn = nextTab.length; ForwardingNode&lt;K, V&gt; fwd = new ForwardingNode&lt;K, V&gt;(nextTab); // 迁移推进标识，为false时跳出循环 // 标识是否进行迁移范围分配 boolean advance = true; // 全表迁移状态标识，true标识全部迁移完成 boolean finishing = false; // to ensure sweep before committing nextTab // 下标i,bound赋予初值，循环中会计算本次迁移的范围 for (int i = 0, bound = 0; ; ) &#123; Node&lt;K, V&gt; f; int fh; // 分配循环 while (advance) &#123; int nextIndex, nextBound; // i未到达本次迁移下界bound，或者全表迁移完成，标识停止推进，不会走到else if的范围分配 if (--i &gt;= bound || finishing) advance = false; // transferIndex小于0，没有可分配的迁移了，标识停止推进 else if ((nextIndex = transferIndex) &lt;= 0) &#123; i = -1; advance = false; // 否则，若cas transferIndex成功（减去步长），分配迁移范围 &#125; else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; // 下界bound为更新后的transferIndex bound = nextBound; // 上界i为之前的transferIndex减1 i = nextIndex - 1; // 标识停止分配范围 advance = false; &#125; &#125; // i的临界判断 if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; int sc; // 如果全表已迁移完成，赋值table和sizeCtl if (finishing) &#123; nextTable = null; table = nextTab; sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); return; &#125; // i已到临界条件，本线程迁移工作完成，cas将sizeCtl-1 if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; // 如果本线程是迁移工作中的最后一个活动线程，直接返回（sc为sizeCtl cas前的值） if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return; // 如果还有其他线程在迁移，仅标识迁移完成，且推进继续（为了double check？check什么？） finishing = advance = true; i = n; // recheck before commit &#125; &#125; // 以下为迁移处理 // 如果节点为null，直接cas替换为fwd，成功则advance为true，重新分配 else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); // 如果节点本身是fwd，说明本段步长已处理过，在while中重新分配范围 else if ((fh = f.hash) == MOVED) advance = true; // already processed // 桶的头节点加锁，迁移到新表 else &#123; synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; Node&lt;K, V&gt; ln, hn; // 链表结构 if (fh &gt;= 0) &#123; int runBit = fh &amp; n; // 假设n=2^k，按照hash第k位为0或1分为2组，0组放低位，1组放高位 // lastRun之后的节点直接用原节点，不新new // lastRun之前的节点在新表中逆序，之后的节点保持原序 Node&lt;K, V&gt; lastRun = f; for (Node&lt;K, V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; // 这里主要是判断lastRun的一串节点，要放高位还是低位 if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; else &#123; hn = lastRun; ln = null; &#125; // 重新遍历链表，链接出0组和1组节点，lastRun已链接上某个组，无需再遍历 for (Node&lt;K, V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K, V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K, V&gt;(ph, pk, pv, hn); &#125; // 替换 setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); // 继续推进 advance = true; &#125; // 树结构 else if (f instanceof TreeBin) &#123; TreeBin&lt;K, V&gt; t = (TreeBin&lt;K, V&gt;) f; TreeNode&lt;K, V&gt; lo = null, loTail = null; TreeNode&lt;K, V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K, V&gt; e = t.first; e != null; e = e.next) &#123; int h = e.hash; TreeNode&lt;K, V&gt; p = new TreeNode&lt;K, V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) &#123; if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; &#125; else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K, V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K, V&gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; &#125; &#125; &#125; &#125;&#125; get123456789101112131415161718192021222324252627public V get(Object key) &#123; Node&lt;K, V&gt;[] tab; Node&lt;K, V&gt; e, p; int n, eh; K ek; int h = spread(key.hashCode()); // 表不为空，且hash取模所在桶不为空 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; // 桶的头结点为要找的节点 if ((eh = e.hash) == h) &#123; if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; // 头结点hash&lt;0，说明为树或者迁移节点，调用find查找 else if (eh &lt; 0) // ForwardingNode -1; treeBin -2 return (p = e.find(h, key)) != null ? p.val : null; // 否则为链表格式，遍历查找 while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125; 作者说123456789101112131415....We do not want to waste* the space required to associate a distinct lock object with* each bin, so instead use the first node of a bin list itself as* a lock. Locking support for these locks relies on builtin* &quot;synchronized&quot; monitors.* * Using the first node of a list as a lock does not by itself* suffice though: When a node is locked, any update must first* validate that it is still the first node after locking it, and* retry if not. ....* * ....The transfer operation must also ensure that all* accessible bins in both the old and new table are usable by any* traversal. This is arranged in part by proceeding from the* last bin (table.length - 1) up towards the first. 引用一段并发分析Java 8 中 ConcurrentHashMap工作原理的要点分析 6.1初化的同步问题 表长度的分配并不是在构造函数中进行的，而是在put方法中进行的，也就是说这实际上是个懒汉模式。但是如果多个线程同时进行表长度的空间分配，显然是非线程安全的。所以只能有一个线程来进行创建表，其它线程会等待创建完成。ConcurrentHashMap类中设定一个volatile变量sizeCtl private transient volatile int sizeCtl; 然后通过CAS方法去修改它，如果有其它线程发现sieCtl为-1 U.compareAndSwapInt(this, SIZECTL, sc, -1) 就表示已经有线程正在创建表了，那么当前线程就会放弃CPU使用权（调用Thread.yield()方法），等待分初始化完成后继续进行put操作。否则当前线程尝试将siezeCtl修改为-1,若成功，就由当前线程来创建表。 6.2 put方法和remove方法之间的同步问题 在表的同一个槽上，一个线程调用put方法和另一个线程调用put方法是互斥的；在表的同一个槽上，一个线程调用remove方法和另一个线程调用remove方法也是互斥的；在表的同一个槽上，一个线程调用remove方法和另一个线程调用put方法也是互斥的。这些互斥操作在代码中都是通过锁来保证的。 6.3 put(或remove)方法和get方法的同步问题 实际上是不需要同步，先到先得。这主要由于Node定义中value和next都定义成了volatile类型。一个线程能否get到另一个线程刚刚put（或remove）的值，这主要由两个线程当前访问的结点所处的位置决定的。 6.4 get方法和扩容操作的同步问题 可以分成两种情况讨论 1）该位置的头结点是Node类型对象，直接get，即使这个桶正在进行迁移，在get方法未完成前，迁移已完成（槽被设置成了ForwordingNode对象），也没关系，并不影响get的结果，因为get线程仍然持有旧链表的引用，可以从当前结点位置访问到所有的后续结点，原因是新表中的节点是通过复制旧表中的结点得到的，所以新表的结点的next不会影响旧表中对应结点的next值。当get方法结束后，旧链表就不可达了，会被垃圾回收线程回收。 2）该位置的头结点是ForwordingNode类型对象（头结点的hash值 == -1），头结点是ForwordingNode类型的对象，调用该对象的find方法，在新表中查找。 所以无论哪种情况，都能get到正确的值。 6.5 put(或remove)方法和扩容操作的同步问题 同样可以分为两种情况讨论： 1）该位置的头结点是Node类型对象，put操作就走正常路线，先将Node对象放入到旧表中，然后调用addCount方法，判断是否需要帮助扩容。 2）该位置的头结点是ForwordingNode类型对象，那就会先帮助扩容，然后在新表中进行put操作。","tags":[{"name":"ConcurrentHashMap","slug":"ConcurrentHashMap","permalink":"https://northernw.github.io/tags/ConcurrentHashMap/"}]},{"title":"foreach中不宜进行remove/add等操作","date":"2019-06-19T17:26:18.000Z","path":"2019/06/20/foreach中不宜进行remove-add等操作/","text":"原因对比源码和反编译可以看出，都使用iterator进行迭代，差别在于foreach用list.remove(i)，iterator用iterator.remove()。执行在(Integer)iterator.next()抛出并发修改异常（见反编译代码），原因在于，next()中校验了Itr的expectedModCount和ArrayList的modCount需相等。list.remove(i)使list的modCount++，而iterator中的expectedModCount不变，由此产生差异。iterator.remove()会将list的modCount赋值给expectedModCount，无差异。 源码123456789101112131415161718public static void main(String[] args) &#123; List&lt;Integer&gt; list = Lists.newArrayList(1, 2, 3, 4); for (Integer i : list) &#123; if (i.equals(1)) &#123; list.remove(i); &#125; &#125; Iterator&lt;Integer&gt; iterator = list.iterator(); while (iterator.hasNext()) &#123; Integer i = iterator.next(); if (i.equals(3)) &#123; iterator.remove(); &#125; &#125;&#125; 执行异常信息1234Exception in thread &quot;main&quot; java.util.ConcurrentModificationException at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:907) at java.util.ArrayList$Itr.next(ArrayList.java:857) at com.jzt.service.joint.agent.ForEachTest.main(ForEachTest.java:18) IDE反编译12345678910111213141516171819202122public static void main(String[] args) &#123; List&lt;Integer&gt; list = Lists.newArrayList(new Integer[]&#123;1, 2, 3, 4&#125;); Iterator iterator = list.iterator(); Integer i; while(iterator.hasNext()) &#123; i = (Integer)iterator.next(); if (i.equals(1)) &#123; list.remove(i); &#125; &#125; iterator = list.iterator(); while(iterator.hasNext()) &#123; i = (Integer)iterator.next(); if (i.equals(3)) &#123; iterator.remove(); &#125; &#125;&#125; 附ArrayList中Itr源码节选1234567891011121314151617181920212223242526public E next() &#123; checkForComodification(); int i = cursor; if (i &gt;= size) throw new NoSuchElementException(); Object[] elementData = ArrayList.this.elementData; if (i &gt;= elementData.length) throw new ConcurrentModificationException(); cursor = i + 1; return (E) elementData[lastRet = i];&#125; public void remove() &#123; if (lastRet &lt; 0) throw new IllegalStateException(); checkForComodification(); try &#123; ArrayList.this.remove(lastRet); cursor = lastRet; lastRet = -1; expectedModCount = modCount; &#125; catch (IndexOutOfBoundsException ex) &#123; throw new ConcurrentModificationException(); &#125;&#125; 删除倒数第二个元素不抛异常修改代码为remove 3，在迭代中输出元素，结果如下，foreach不会输出最后一个元素。list.remove(i)之后的hasNext()返回false，说明iterator的cursor与list的size相等。iterator.next()后，it.cursor = i + 1 = 3，it.lastRet = 2，指向元素3. list.remove((Integer)3)后，list.size = 3。 iterator.remove()执行的是remove(lastRet)，list.size = 3，修改cursor = lastRet = 2，指向元素4. 输出结果123456781231234 hasNext源码123456int cursor; // index of next element to return 下一个next返回int lastRet = -1; // index of last element returned; -1 if no such 上一个next返回 public boolean hasNext() &#123; return cursor != size;&#125;","tags":[]},{"title":"Java foreach原理","date":"2019-06-19T16:34:16.000Z","path":"2019/06/20/Java-foreach原理/","text":"从字节码可以看出，foreach使用了Iterator迭代器，循环判断hashNext()，用next()取操作对象。Collection接口继承了Iterable接口，可以获取iterator对象。【Iterator iterator();】集合对象有对应的xxIterator，实现具体的hashNext()、next()、remove()等操作。反编译源码更直观。 测试源码1234567891011public class ForEachTest &#123; public static void main(String[] args) &#123; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); list.add(1); list.add(2); for (Integer i : list) &#123; System.out.println(i); &#125; &#125;&#125; IDE反编译源码1234567891011121314151617public class ForEachTest &#123; public ForEachTest() &#123; &#125; public static void main(String[] args) &#123; List&lt;Integer&gt; list = new ArrayList(); list.add(1); list.add(2); Iterator var2 = list.iterator(); while(var2.hasNext()) &#123; Integer i = (Integer)var2.next(); System.out.println(i); &#125; &#125;&#125; 字节码foreach部分节选 1234567891011121314151617181920212223242526 for (Integer i : list) &#123; System.out.println(i); &#125;L3 LINENUMBER 17 L3 ALOAD 1 INVOKEINTERFACE java/util/List.iterator ()Ljava/util/Iterator; (itf) ASTORE 2L4FRAME APPEND [java/util/List java/util/Iterator] ALOAD 2 INVOKEINTERFACE java/util/Iterator.hasNext ()Z (itf) IFEQ L5 ALOAD 2 INVOKEINTERFACE java/util/Iterator.next ()Ljava/lang/Object; (itf) CHECKCAST java/lang/Integer ASTORE 3L6 LINENUMBER 18 L6 GETSTATIC java/lang/System.out : Ljava/io/PrintStream; ALOAD 3 INVOKEVIRTUAL java/io/PrintStream.println (Ljava/lang/Object;)VL7 LINENUMBER 19 L7 GOTO L4","tags":[]},{"title":"Java查看字节码","date":"2019-06-19T14:47:07.000Z","path":"2019/06/19/Java查看字节码/","text":"命令行方式编译成.class文件后，执行javap命令1javap -c xx.class idea安装bytecode viewer插件 切到.java文件所在tab，选择view-&gt;show bytecode","tags":[]},{"title":"ArrayList笔记","date":"2019-06-18T15:05:38.000Z","path":"2019/06/18/ArrayList笔记/","text":"Q&amp;A 为什么for中不能remove，而iterator时可以？见foreach中不宜进行remove/add等操作 迭代是怎么实现的？见Java foreach原理 关联知识 本质是数组 Object[] forEach(Consumer&lt;? super E&gt; action) action.accept(elementData[i]) 了解下Consumer，lambda removeIf(Predicate&lt;? super E&gt; filter) if (filter.test(element)) 要点扩容1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162 /** * Default initial capacity. 默认容量 */ private static final int DEFAULT_CAPACITY = 10; /** * Shared empty array instance used for empty instances. 指定容量为0时用到 */ private static final Object[] EMPTY_ELEMENTDATA = &#123;&#125;; /** * Shared empty array instance used for default sized empty instances. We * distinguish this from EMPTY_ELEMENTDATA to know how much to inflate when * first element is added. 默认构造函数用到，容量为10 */ private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;;/** * The maximum size of array to allocate. * Some VMs reserve some header words in an array. * Attempts to allocate larger arrays may result in * OutOfMemoryError: Requested array size exceeds VM limit 受VM影响 */ private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; /** * Increases the capacity to ensure that it can hold at least the * number of elements specified by the minimum capacity argument. * * @param minCapacity the desired minimum capacity */ private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); // 右移，增加原容量两倍 if (newCapacity - minCapacity &lt; 0) // 还是比所需小，选所需 newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) // 如果超过array最大值，尝试Intager最大值，不一定成功 newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); &#125; private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) grow(minCapacity); &#125; private static int calculateCapacity(Object[] elementData, int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; return Math.max(DEFAULT_CAPACITY, minCapacity); &#125; return minCapacity; &#125; // minCapacity为size + 1 private void ensureCapacityInternal(int minCapacity) &#123; ensureExplicitCapacity(calculateCapacity(elementData, minCapacity)); &#125; 增还有add(int,E),addAll(collection&lt;&gt;)等重点是检查容量和扩容ensureCapacityInternal，在指定位置上赋值新元素 1234567891011/** * Appends the specified element to the end of this list. * * @param e element to be appended to this list * @return &lt;tt&gt;true&lt;/tt&gt; (as specified by &#123;@link Collection#add&#125;) */public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125; 删 public E remove(int index) rangeCheck(index); 移动后续元素 最后元素赋值释放引用 public boolean remove(Object o) null和非null分开处理 fastRemove（与remove不同的是，无需rangeCheck） 改 public E set(int index, E element) rangeCheck;赋值；return old data 查 public E get(int index) rangeCheck;return elementData(index);123E elementData(int index) &#123; return (E) elementData[index];&#125; 一些源码的解惑batchRemove123456789101112131415161718192021222324252627282930private boolean batchRemove(Collection&lt;?&gt; c, boolean complement) &#123; final Object[] elementData = this.elementData; int r = 0, w = 0; boolean modified = false; try &#123; for (; r &lt; size; r++) // 这里complement也很巧妙，removeAll把contains false的元素保留，retainAll把contains true的元素保留 if (c.contains(elementData[r]) == complement) elementData[w++] = elementData[r]; &#125; finally &#123; // Preserve behavioral compatibility with AbstractCollection, // even if c.contains() throws. // 此处是指contains抛出异常，r != size，需要把size-r个（没有被处理）的元素拷贝到w位置后 if (r != size) &#123; System.arraycopy(elementData, r, elementData, w, size - r); w += size - r; &#125; if (w != size) &#123; // clear to let GC do its work for (int i = w; i &lt; size; i++) elementData[i] = null; modCount += size - w; size = w; modified = true; &#125; &#125; return modified; &#125; removeIf用了BitSet，filter test为真时，将该位BitSet设置为true 1234567891011121314151617181920212223242526272829303132333435363738394041424344public boolean removeIf(Predicate&lt;? super E&gt; filter) &#123; Objects.requireNonNull(filter); // figure out which elements are to be removed // any exception thrown from the filter predicate at this stage // will leave the collection unmodified int removeCount = 0; final BitSet removeSet = new BitSet(size); final int expectedModCount = modCount; final int size = this.size; for (int i=0; modCount == expectedModCount &amp;&amp; i &lt; size; i++) &#123; @SuppressWarnings(&quot;unchecked&quot;) final E element = (E) elementData[i]; // filter test为真时，将该位BitSet设置为true if (filter.test(element)) &#123; removeSet.set(i); removeCount++; &#125; &#125; if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125; // shift surviving elements left over the spaces left by removed elements final boolean anyToRemove = removeCount &gt; 0; if (anyToRemove) &#123; final int newSize = size - removeCount; for (int i=0, j=0; (i &lt; size) &amp;&amp; (j &lt; newSize); i++, j++) &#123; // 将值为false的元素保留（前移） i = removeSet.nextClearBit(i); elementData[j] = elementData[i]; &#125; // 清除多余的空间，赋值null，去掉对对象的引用 for (int k=newSize; k &lt; size; k++) &#123; elementData[k] = null; // Let gc do its work &#125; this.size = newSize; if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125; modCount++; &#125; return anyToRemove; &#125; replaceAllUnaryOperator 一元操作 12345List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;();for (int i = 0; i &lt; 10; i++) &#123; list.add(i);&#125;list.replaceAll(x -&gt; x + 10); 123456789101112131415161718192021222324252627/** * Replaces each element of this list with the result of applying the * operator to that element. Errors or runtime exceptions thrown by * the operator are relayed to the caller. * @implSpec * The default implementation is equivalent to, for this &#123;@code list&#125;: * &lt;pre&gt;&#123;@code * final ListIterator&lt;E&gt; li = list.listIterator(); * while (li.hasNext()) &#123; * li.set(operator.apply(li.next())); * &#125; * &#125;&lt;/pre&gt; */@Override@SuppressWarnings(&quot;unchecked&quot;)public void replaceAll(UnaryOperator&lt;E&gt; operator) &#123; Objects.requireNonNull(operator); final int expectedModCount = modCount; final int size = this.size; for (int i=0; modCount == expectedModCount &amp;&amp; i &lt; size; i++) &#123; elementData[i] = operator.apply((E) elementData[i]); &#125; if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125; modCount++;&#125; 经常用到 System.arraycopy(original, 0, copy, 0, Math.min(original.length, newLength));","tags":[]},{"title":"java8 | lambda","date":"2019-05-17T18:08:53.000Z","path":"2019/05/18/java8-lambda/","text":"《Java8函数式编程》 [英] Richard Warburton 著，王群锋 译 人民邮电出版社 目录 简介 lambda表达式 流 类库 高级集合类和收集器 数据并行化 测试、调试和重构 设计和架构的原则 使用lambda编写并发程序 下一步 设计和架构的原则软件开发最重要的设计工具不是什么技术，而是一颗在设计原则方面训练有素的头脑。—— Craig Larman SOLID原则 Single responsibility 单一功能原则 Open/Closed 开闭原则 Liskov subsitution Interface segregation Dependency inversion 依赖反转","tags":[{"name":"lambda","slug":"lambda","permalink":"https://northernw.github.io/tags/lambda/"}]},{"title":"9. Palindrome Number | Easy","date":"2019-01-23T17:11:28.000Z","path":"2019/01/24/9-Palindrome-Number-Easy/","text":"leetCode: 9. Palindrome Number Description12345678910111213141516171819Determine whether an integer is a palindrome. An integer is a palindrome when it reads the same backward as forward.Example 1:Input: 121Output: trueExample 2:Input: -121Output: falseExplanation: From left to right, it reads -121. From right to left, it becomes 121-. Therefore it is not a palindrome.Example 3:Input: 10Output: falseExplanation: Reads 01 from right to left. Therefore it is not a palindrome.Follow up:Coud you solve it without converting the integer to a string? SolutionFirst笨法子Runtime: 103 ms, faster than 68.27% of Java online submissions for Palindrome Number.1234567891011121314151617181920class Solution &#123; public boolean isPalindrome(int x) &#123; if(x &lt; 0)&#123; return false; &#125; StringBuilder sb = new StringBuilder(); while(x &gt; 0)&#123; sb.append(x%10); x/=10; &#125; String s = sb.toString(); int n = s.length(); for(int i=0;i&lt;n/2;i++)&#123; if(s.charAt(i)!=s.charAt(n-i-1))&#123; return false; &#125; &#125; return true; &#125;&#125; Recomended12345678910111213class Solution &#123; public boolean isPalindrome(int x) &#123; if(x &lt; 0 || (x!=0 &amp;&amp; x%10==0))&#123; return false; &#125; int reverse = 0; while(x&gt;reverse)&#123; reverse = reverse * 10 + x % 10; x/=10; &#125; return (x == reverse || x==reverse/10); &#125;&#125; Note","tags":[{"name":"leetCode","slug":"leetCode","permalink":"https://northernw.github.io/tags/leetCode/"}]},{"title":"String to Integer (atoi) | Medium","date":"2019-01-23T16:43:49.000Z","path":"2019/01/24/String-to-Integer-atoi-Medium/","text":"leetCode: 8. String to Integer (atoi) Description1234567891011121314151617181920212223242526272829303132333435363738394041Implement atoi which converts a string to an integer.The function first discards as many whitespace characters as necessary until the first non-whitespace character is found. Then, starting from this character, takes an optional initial plus or minus sign followed by as many numerical digits as possible, and interprets them as a numerical value.The string can contain additional characters after those that form the integral number, which are ignored and have no effect on the behavior of this function.If the first sequence of non-whitespace characters in str is not a valid integral number, or if no such sequence exists because either str is empty or it contains only whitespace characters, no conversion is performed.If no valid conversion could be performed, a zero value is returned.Note:Only the space character &apos; &apos; is considered as whitespace character.Assume we are dealing with an environment which could only store integers within the 32-bit signed integer range: [−231, 231 − 1]. If the numerical value is out of the range of representable values, INT_MAX (231 − 1) or INT_MIN (−231) is returned.Example 1:Input: &quot;42&quot;Output: 42Example 2:Input: &quot; -42&quot;Output: -42Explanation: The first non-whitespace character is &apos;-&apos;, which is the minus sign. Then take as many numerical digits as possible, which gets 42.Example 3:Input: &quot;4193 with words&quot;Output: 4193Explanation: Conversion stops at digit &apos;3&apos; as the next character is not a numerical digit.Example 4:Input: &quot;words and 987&quot;Output: 0Explanation: The first non-whitespace character is &apos;w&apos;, which is not a numerical digit or a +/- sign. Therefore no valid conversion could be performed.Example 5:Input: &quot;-91283472332&quot;Output: -2147483648Explanation: The number &quot;-91283472332&quot; is out of the range of a 32-bit signed integer. Thefore INT_MIN (−231) is returned. SolutionFirst不是很喜欢这类题但是巧妙的解法还是很多啊 ref: leetcode discuss 12345678910111213141516171819202122232425262728class Solution &#123; public int myAtoi(String str) &#123; int sign = 1, i = 0, ans = 0, n=str.length(); if(n==0)&#123; return 0; &#125; // 这里的while和if都很巧妙 while(i&lt;n &amp;&amp; str.charAt(i)==&apos; &apos;)&#123; i++; &#125; if(i&lt;n &amp;&amp; (str.charAt(i)==&apos;-&apos; || str.charAt(i)==&apos;+&apos;))&#123; sign = 1 - 2*(str.charAt(i)==&apos;-&apos;?1:0); i++; &#125; int max = Integer.MAX_VALUE/10; // 这部分while直接忽略掉后续的不合法字符，也很赞 // 边界判断尾数&gt;&apos;7&apos;，对正负数都有效 // &quot;-2147483648&quot; &apos;8&apos;&gt;&apos;7&apos;，正好返回最小值 while(i&lt;n &amp;&amp; str.charAt(i)&gt;=&apos;0&apos; &amp;&amp; str.charAt(i)&lt;=&apos;9&apos;)&#123; if(ans&gt;max || (ans == max &amp;&amp; str.charAt(i)&gt;&apos;7&apos;))&#123; return sign==1?Integer.MAX_VALUE:Integer.MIN_VALUE; &#125; ans = ans * 10 + (str.charAt(i)-&apos;0&apos;); i++; &#125; return sign*ans; &#125;&#125; Note","tags":[{"name":"leetCode","slug":"leetCode","permalink":"https://northernw.github.io/tags/leetCode/"}]},{"title":"ZigZag Conversion | Medium","date":"2019-01-23T14:51:08.000Z","path":"2019/01/23/ZigZag-Conversion-Medium/","text":"leetCode: 6. ZigZag Conversion Description123456789101112131415161718192021222324The string &quot;PAYPALISHIRING&quot; is written in a zigzag pattern on a given number of rows like this: (you may want to display this pattern in a fixed font for better legibility)P A H NA P L S I I GY I RAnd then read line by line: &quot;PAHNAPLSIIGYIR&quot;Write the code that will take a string and make this conversion given a number of rows:string convert(string s, int numRows);Example 1:Input: s = &quot;PAYPALISHIRING&quot;, numRows = 3Output: &quot;PAHNAPLSIIGYIR&quot;Example 2:Input: s = &quot;PAYPALISHIRING&quot;, numRows = 4Output: &quot;PINALSIGYAHRPI&quot;Explanation:P I NA L S I GY A H RP I SolutionFirst心里想着是有数学规律的解答方案的，奈何数学太渣 Runtime: 56 ms, faster than 26.43% of Java online submissions for ZigZag Conversion. 1234567891011121314151617181920212223242526272829303132333435363738394041class Solution &#123; public String convert(String s, int numRows) &#123; if (numRows == 1) &#123; return s; &#125; int n = s.length(); int cnt = numRows + numRows - 2; int numCols = (n / cnt + 1) * (numRows - 1); char[][] flag = new char[numRows][numCols]; int i = 0, j = 0, k = 0; while (k &lt; n) &#123; flag[i][j] = s.charAt(k); k++; if (j % (numRows - 1) == 0) &#123; if (i == numRows - 1) &#123; i--; j++; &#125; else &#123; i++; &#125; &#125; else &#123; i--; j++; &#125; &#125; StringBuilder ans = new StringBuilder(); for (i = 0; i &lt; flag.length; i++) &#123; for (j = 0; j &lt; flag[i].length; j++) &#123; if (flag[i][j] != &apos;\\0&apos;) &#123; ans.append(flag[i][j]); &#125; &#125; &#125; return ans.toString(); &#125;&#125; RecommendedSort By Row[走Z方案]思路相同，写法可比自己的巧妙多了 34 ms goingDown的布尔比较巧妙 StringBuilder+List，节省空间，也省掉遍历1234567891011121314151617181920212223class Solution &#123; public String convert(String s, int numRows) &#123; if (numRows == 1) return s; List&lt;StringBuilder&gt; rows = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; Math.min(numRows, s.length()); i++) rows.add(new StringBuilder()); int curRow = 0; boolean goingDown = false; for (char c : s.toCharArray()) &#123; rows.get(curRow).append(c); if (curRow == 0 || curRow == numRows - 1) goingDown = !goingDown; curRow += goingDown ? 1 : -1; &#125; StringBuilder ret = new StringBuilder(); for (StringBuilder row : rows) ret.append(row); return ret.toString(); &#125;&#125; Visit By Row [找规律]传说中的数学方案每个循环的字符个数cycLen = 2*(numRows)-2，k为循环个数 0行的字符索引为 cycLen * k, k = 0,1,2… numRows-1行的字符索引为 cycLen * k + (numRows - 1), 比0行的索引加（numRows-1）个 其他每行，在每个循环中都有2个字符，索引分别为cycLenk+i和cycLen(k+1)-i，即cycLen*k+(cycLen-i) 总结起来 每行中都有一个索引cycLen * k + i 的字符 非0行和numRows-1行，还有一个 cycLen * k + (cycLen - i)的字符见下方实现 19 msj表示那一行内，位于竖着的那些列的字符所在索引即，[A] [S] [G]1234 P I N[A] L [S] I [G] Y A H R P I 12345678910111213141516171819202122class Solution &#123; public String convert(String s, int numRows) &#123; if (numRows == 1) return s; int n = s.length(); int cyclen = numRows * 2 - 2; StringBuilder ans = new StringBuilder(); // i表示每行 // j表示每个分块 for(int i = 0; i &lt; numRows; i++)&#123; for(int j = 0; j+i &lt;n; j += cyclen )&#123; ans.append(s.charAt(j+i)); if(i!=0 &amp;&amp; i!= numRows-1 &amp;&amp; j+cyclen-i&lt;n)&#123; ans.append(s.charAt(j+cyclen-i)); &#125; &#125; &#125; return ans.toString(); &#125;&#125; Note","tags":[{"name":"leetCode","slug":"leetCode","permalink":"https://northernw.github.io/tags/leetCode/"}]},{"title":"Hello World","date":"2019-01-19T16:34:16.000Z","path":"2019/01/20/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[]},{"title":"Longest Palindromic Substring | Medium","date":"2019-01-17T11:15:12.000Z","path":"2019/01/17/Longest-Palindromic-Substring-Medium/","text":"leetCode: 5. Longest Palindromic Substring DescriptionGiven a string s, find the longest palindromic substring in s. You may assume that the maximum length of s is 1000. Example 1: Input: “babad”Output: “bab”Note: “aba” is also a valid answer.Example 2: Input: “cbbd”Output: “bb” SolutionFirst原想暴力破解，想了下时间复杂度还是太高，看了hints决定提笔算。做的题都忘光光了，加油！ 145ms, faster than 20.90% of Java online submissions.仍需努力呀 Time complexity: O(n^2)Space complexity: O(n^2)12345678910111213141516171819202122232425262728293031class Solution &#123; public String longestPalindrome(String s) &#123; // Dynamic Programming int n = s.length(); boolean[][] flag = new boolean[n][n]; for(int len = 0; len &lt; n; len++)&#123; for(int i = 0; i&lt; n-len; i++)&#123; int start = i, end = i + len; if((start == end) || (start+1 == end))&#123; flag[start][end] = s.charAt(start) == s.charAt(end); &#125;else&#123; flag[start][end] = (s.charAt(start) == s.charAt(end)) &amp;&amp; flag[start+1][end-1]; &#125; &#125; &#125; int len = 0; String ans = &quot;&quot;; for(int i = 0; i &lt; n; i++)&#123; for(int j = 0; j &lt; n; j++)&#123; if(flag[i][j] &amp;&amp; (j-i+1 &gt; len))&#123; len = j-i+1; ans = s.substring(i,j+1); &#125; &#125; &#125; return ans; &#125;&#125; 优化了下，ans不在flag循环中取，还是145ms，大头仍是O(n^2)1234567891011121314int max = 0;int left = 0, right = 0;for(int i = 0; i &lt; n; i++)&#123; for(int j = 0; j &lt; n; j++)&#123; if(flag[i][j] &amp;&amp; (j-i+1 &gt; max))&#123; max = j-i+1; left = i; right = j; &#125; &#125;&#125;return s.substring(left,right+1); Recomended最早想的方案也是这个，怎么就没想到有2n-1个中心呢没留意到偶数处理方案 16 ms, faster than 93.37% of Java online submissions.赞~时间复杂度还是O(n^2) Expand Around Center12345678910111213141516171819202122232425262728293031class Solution &#123; private int left = 0; private int len = 0; private int n = 0; public String longestPalindrome(String s) &#123; n = s.length(); if(n &lt; 2)&#123; return s; &#125; for(int i = 0; i &lt; n; i++)&#123; aroundCenter(s, i, i); aroundCenter(s, i, i+1); &#125; return s.substring(left, left+len); &#125; private void aroundCenter(String s, int start, int end)&#123; while(start&gt;=0 &amp;&amp; end &lt;n &amp;&amp; s.charAt(start)==s.charAt(end))&#123; start--; end++; &#125; // 此时start+1和end-1为正确下标，回文子串长度为(end-1)-(start+1)+1 if(end - start -1 &gt; len)&#123; left = start + 1; len = end - start -1; &#125; &#125;&#125; Compare做了个简单的对比输入字符串长约900字符平均 中心点法1ms，DP 20ms+ 原想对比dp两次for循环会增加多少时间，貌似不太准，作罢1234567891011@Testpublic void longestPalindrome() throws Exception &#123; LongestPalindromicSubstring longestPalindromicSubstring = new LongestPalindromicSubstring(); long start = System.currentTimeMillis(); System.out.println(longestPalindromicSubstring.longestPalindromeWithAroundCenter(s)); System.out.println(System.currentTimeMillis() - start); start = System.currentTimeMillis(); System.out.println(longestPalindromicSubstring.longestPalindromeWithDP(s)); System.out.println(System.currentTimeMillis() - start);&#125; Note dp找到状态转移方程，简单理解为递推公式 数组下标的计算，对数字不太敏感，笨 s[i,j], i与j中有j-i+1个元素 s[i]走j-i步到s[j]，因为s[i]本身占一个元素，再过j-i个元素","tags":[{"name":"leetCode","slug":"leetCode","permalink":"https://northernw.github.io/tags/leetCode/"}]},{"title":"Longest Substring Without Repeating Characters | Medium","date":"2019-01-15T20:59:49.000Z","path":"2019/01/16/Longest-Substring-Without-Repeating-Characters-Medium/","text":"leetCode: 3. Longest Substring Without Repeating Characters DescriptionGiven a string, find the length of the longest substring without repeating characters. Example 1: Input: “abcabcbb”Output: 3Explanation: The answer is “abc”, with the length of 3.Example 2: Input: “bbbbb”Output: 1Explanation: The answer is “b”, with the length of 1.Example 3: Input: “pwwkew”Output: 3Explanation: The answer is “wke”, with the length of 3. Note that the answer must be a substring, “pwke” is a subsequence and not a substring. SolutionFirstTime Limit Exceeded 123456789101112131415161718192021class Solution &#123; public int lengthOfLongestSubstring(String s) &#123; int ans = 1, len = s.length(); if(len == 0)&#123; return 0; &#125; Set&lt;Character&gt; set = new HashSet(); for(int left = 0; left &lt; len-1; left++)&#123; for(int right = left+1; right &lt; len; right++)&#123; set.clear(); for(int i = left; i &lt; right+1; i++)&#123; set.add(s.charAt(i)); &#125; if(right-left+1 == set.size() &amp;&amp; set.size() &gt; ans)&#123; ans = set.size(); &#125; &#125; &#125; return ans; &#125;&#125; RecommendedSliding Window123456789101112131415161718class Solution &#123; public int lengthOfLongestSubstring(String s) &#123; // sliding window int n = s.length(); int ans = 0; Set&lt;Character&gt; set = new HashSet&lt;&gt;(); for(int i = 0, j = 0; i &lt; n &amp;&amp; j &lt; n; )&#123; // try to extend the range of [i, j] if(!set.contains(s.charAt(j)))&#123; ans = Math.max(ans, j - i +1); set.add(s.charAt(j++)); &#125;else&#123; set.remove(s.charAt(i++)); &#125; &#125; return ans; &#125;&#125; Sliding Window Optimized123456789101112131415161718class Solution &#123; public int lengthOfLongestSubstring(String s) &#123; // sliding window optimized int n = s.length(); int ans = 0; // current index of character Map&lt;Character,Integer&gt; map = new HashMap&lt;&gt;(); for(int i = 0, j = 0; j &lt; n; j++)&#123; // try to extend the range of [i, j] if(map.containsKey(s.charAt(j)))&#123; i = Math.max(i, map.get(s.charAt(j))); &#125; ans = Math.max(ans, j - i + 1); map.put(s.charAt(j), j + 1); &#125; return ans; &#125;&#125; Note String substring(int,int) 左开右闭截取 charAt(int) 取char Set set = new HashSet&lt;&gt;(); 滑动窗解法，s[i,j)视为满足条件的子串，比较s[j]与s[i,j)，若满足条件，j++，若不满足条件，i++","tags":[{"name":"leetCode","slug":"leetCode","permalink":"https://northernw.github.io/tags/leetCode/"}]},{"title":"Add Two Numbers | Medium","date":"2019-01-15T17:48:27.000Z","path":"2019/01/16/Add-Two-Numbers-Medium/","text":"leetCode: 2. Add Two Numbers DescriptionYou are given two non-empty linked lists representing two non-negative integers. The digits are stored in reverse order and each of their nodes contain a single digit. Add the two numbers and return it as a linked list. You may assume the two numbers do not contain any leading zero, except the number 0 itself. Example: Input: (2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)Output: 7 -&gt; 0 -&gt; 8Explanation: 342 + 465 = 807. SolutionFirst嗯，裹脚布，又臭又长12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode addTwoNumbers(ListNode l1, ListNode l2) &#123; int carry = 0; ListNode head = null; ListNode ans = new ListNode(0); while (l1 != null &amp;&amp; l2 != null) &#123; int sum = l1.val + l2.val + carry; if (sum &gt;= 10) &#123; carry = 1; sum %= 10; &#125; else &#123; carry = 0; &#125; ans.next = new ListNode(sum); ans = ans.next; if (head == null) &#123; head = ans; &#125; l1 = l1.next; l2 = l2.next; &#125; while (l1 != null) &#123; int sum = carry + l1.val; if (sum &gt;= 10) &#123; carry = 1; sum %= 10; &#125; else &#123; carry = 0; &#125; ans.next = new ListNode(sum); ans = ans.next; l1 = l1.next; &#125; while (l2 != null) &#123; int sum = carry + l2.val; if (sum &gt;= 10) &#123; carry = 1; sum %= 10; &#125; else &#123; carry = 0; &#125; ans.next = new ListNode(sum); ans = ans.next; l2 = l2.next; &#125; if (carry &gt; 0) &#123; ans.next = new ListNode(carry); &#125; return head; &#125;&#125; Betterref: (a 11-line cpp solution)[https://leetcode.com/problems/add-two-numbers/discuss/997/c%2B%2B-Sharing-my-11-line-c%2B%2B-solution-can-someone-make-it-even-more-concise] 123456789101112131415161718192021222324252627282930313233/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode addTwoNumbers(ListNode l1, ListNode l2) &#123; int carry = 0; ListNode ans = new ListNode(0); ListNode head = ans; ListNode zero = new ListNode(0); while (l1 != null || l2 != null || carry &gt; 0) &#123; if (l1 == null) &#123; l1 = zero; &#125; if (l2 == null) &#123; l2 = zero; &#125; int sum = l1.val + l2.val + carry; carry = sum / 10; ans.next = new ListNode(sum % 10); ans = ans.next; l1 = l1.next; l2 = l2.next; &#125; return head.next; &#125;&#125; Given Solution12345678910111213141516171819public ListNode addTwoNumbers(ListNode l1, ListNode l2) &#123; ListNode dummyHead = new ListNode(0); ListNode p = l1, q = l2, curr = dummyHead; int carry = 0; while (p != null || q != null) &#123; int x = (p != null) ? p.val : 0; int y = (q != null) ? q.val : 0; int sum = carry + x + y; carry = sum / 10; curr.next = new ListNode(sum % 10); curr = curr.next; if (p != null) p = p.next; if (q != null) q = q.next; &#125; if (carry &gt; 0) &#123; curr.next = new ListNode(carry); &#125; return dummyHead.next;&#125; Note","tags":[{"name":"leetCode","slug":"leetCode","permalink":"https://northernw.github.io/tags/leetCode/"}]},{"title":"Reverse Integer | Easy | leetCode","date":"2019-01-14T14:58:20.000Z","path":"2019/01/14/Reverse-Integer-Easy-leetCode/","text":"leetCode: 7. Reverse Integer DescriptionGiven a 32-bit signed integer, reverse digits of an integer. Example 1: Input: 123Output: 321Example 2: Input: -123Output: -321Example 3: Input: 120Output: 21Note:Assume we are dealing with an environment which could only store integers within the 32-bit signed integer range: [-2^31, 2^31 - 1]. For the purpose of this problem, assume that your function returns 0 when the reversed integer overflows. SolutionFirst30ms12345678910111213141516171819class Solution &#123; public int reverse(int x) &#123; int res = 0; int tmp = x; boolean negtive = x &lt; 0; if(negtive)&#123; tmp = -tmp; &#125; while(tmp != 0)&#123; int carry = tmp % 10; if(res &gt; (Integer.MAX_VALUE - carry)/10)&#123; return 0; &#125; res = res *10 + carry; tmp /= 10; &#125; return negtive ? -res : res; &#125;&#125; Noteoverflow溢出Integer的最大值MAX_VALUE，最小值MIN_VALUE","tags":[{"name":"leetCode","slug":"leetCode","permalink":"https://northernw.github.io/tags/leetCode/"}]},{"title":"Two Sum | Easy | leetCode","date":"2019-01-14T11:23:30.000Z","path":"2019/01/14/Two-Sum-Easy-leetCode/","text":"leetCode: 1.Two Sum DescriptionGiven an array of integers, return indices of the two numbers such that they add up to a specific target. You may assume that each input would have exactly one solution, and you may not use the same element twice. Example: 1234Given nums = [2, 7, 11, 15], target = 9,Because nums[0] + nums[1] = 2 + 7 = 9,return [0, 1]. SolutionFirst45ms1234567891011121314class Solution &#123; public int[] twoSum(int[] nums, int target) &#123; int len = nums.length; int i=0,j=0; for(;i&lt;len;i++)&#123; for(j=i+1; j&lt;len;j++)&#123; if(nums[i]+nums[j] == target)&#123; return new int[]&#123;i,j&#125;; &#125; &#125; &#125; return null; &#125;&#125; Better7ms123456789101112class Solution &#123; public int[] twoSum(int[] nums, int target) &#123; Map&lt;Integer,Integer&gt; map = new HashMap&lt;Integer,Integer&gt;(); for(int i = 0; i &lt; nums.length; i++)&#123; if(map.containsKey(target - nums[i]))&#123; return new int[]&#123;map.get(target - nums[i]),i&#125;; &#125; map.put(nums[i],i); &#125; return new int[]&#123;0,0&#125;; &#125;&#125; Faster说是6ms.. 记录下123456789101112class Solution &#123; public int[] twoSum(int[] nums, int target) &#123; Map&lt;Integer,Integer&gt; map = new HashMap&lt;Integer,Integer&gt;(); for(int i = 0; i &lt; nums.length; i++)&#123; if(map.get(target - nums[i])!=null)&#123; return new int[]&#123;map.get(target - nums[i]),i&#125;; &#125; map.put(nums[i],i); &#125; return new int[]&#123;0,0&#125;; &#125;&#125; Note擅于利用map搜索","tags":[{"name":"leetCode","slug":"leetCode","permalink":"https://northernw.github.io/tags/leetCode/"}]}]